question,answer,contexts,ground_truth,context_precision,faithfulness,answer_relevancy,context_recall,answer_correctness,answer_similarity
What are the two main tasks BERT is pre-trained on?,"BERT is pre-trained on two main tasks: masked language modeling (MLM) and next sentence prediction (NSP). The MLM task involves predicting randomly masked tokens in the input sequence, while the NSP task is a binary classification to determine if two segments follow each other in the original text.","['BERT BERT \nE[CLS] E1 E[SEP] ... ENE1’... EM’\nC\nT1\nT[SEP] ...\n TN\nT1’...\n TM’\n[CLS] Tok 1 [SEP] ... Tok NTok 1 ... TokM \nQuestion Paragraph Start/End Span \nBERT \nE[CLS] E1 E[SEP] ... ENE1’... EM’\nC\nT1\nT[SEP] ...\n TN\nT1’...\n TM’\n[CLS] Tok 1 [SEP] ... Tok NTok 1 ... TokM \nMasked Sentence A Masked Sentence B \nPre-training Fine-Tuning NSP Mask LM Mask LM \nUnlabeled Sentence A and B Pair SQuAD \nQuestion Answer Pair NER MNLI Figure 1: Overall pre-training and ﬁne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and ﬁne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During ﬁne-tuning, all parameters are ﬁne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n2.3 Transfer Learning from Supervised Data\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to ﬁne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n3 BERT\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and ﬁne-tuning . Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For ﬁne-\ntuning, the BERT model is ﬁrst initialized with\nthe pre-trained parameters, and all of the param-\neters are ﬁne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate ﬁne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\nA distinctive feature of BERT is its uniﬁed ar-\nchitecture across different tasks. There is mini-mal difference between the pre-trained architec-\nture and the ﬁnal downstream architecture.\nModel Architecture BERT’s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthetensor2tensor library.1Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as “The Annotated Transformer.”2\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERT BASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERT LARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\nBERT BASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n1https://github.com/tensorﬂow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/ﬁlter size to be 4H,\ni.e., 3072 for the H= 768 and 4096 for the H= 1024 .\n4We note that in the literature the bidirectional Trans-'
 'alternatives that lead to better downstream task\nperformance; (2) We use a novel dataset, CC-\nNEWS, and conﬁrm that using more data for pre-\ntraining further improves performance on down-\nstream tasks; (3) Our training improvements show\nthat masked language model pretraining, under\nthe right design choices, is competitive with all\nother recently published methods. We release our\nmodel, pretraining and ﬁne-tuning code imple-\nmented in PyTorch ( Paszke et al. ,2017 ).\n2 Background\nIn this section, we give a brief overview of the\nBERT ( Devlin et al. ,2019 ) pretraining approach\nand some of the training choices that we will ex-\namine experimentally in the following section.\n2.1 Setup\nBERT takes as input a concatenation of two\nsegments (sequences of tokens), x1,...,x N\nandy1,...,yM. Segments usually consist of\nmore than one natural sentence. The two seg-\nments are presented as a single input sequence\nto BERT with special tokens delimiting them:\n[CLS],x1,...,x N,[SEP],y1,...,yM,[EOS].\nMandNare constrained such that M+N < T ,\nwhereTis a parameter that controls the maximum\nsequence length during training.\nThe model is ﬁrst pretrained on a large unla-\nbeled text corpus and subsequently ﬁnetuned us-\ning end-task labeled data.\n2.2 Architecture\nBERT uses the now ubiquitous transformer archi-\ntecture ( Vaswani et al. ,2017 ), which we will not\nreview in detail. We use a transformer architecture\nwithLlayers. Each block uses Aself-attention\nheads and hidden dimension H.\n2.3 Training Objectives\nDuring pretraining, BERT uses two objectives:\nmasked language modeling and next sentence pre-\ndiction.\nMasked Language Model (MLM) A random\nsample of the tokens in the input sequence is\nselected and replaced with the special token\n[MASK]. The MLM objective is a cross-entropy\nloss on predicting the masked tokens. BERT uni-\nformly selects 15% of the input tokens for possi-\nble replacement. Of the selected tokens, 80% are\nreplaced with [MASK], 10% are left unchanged,and 10% are replaced by a randomly selected vo-\ncabulary token.\nIn the original implementation, random mask-\ning and replacement is performed once in the be-\nginning and saved for the duration of training, al-\nthough in practice, data is duplicated so the mask\nis not always the same for every training sentence\n(see Section 4.1).\nNext Sentence Prediction (NSP) NSP is a bi-\nnary classiﬁcation loss for predicting whether two\nsegments follow each other in the original text.\nPositive examples are created by taking consecu-\ntive sentences from the text corpus. Negative ex-\namples are created by pairing segments from dif-\nferent documents. Positive and negative examples\nare sampled with equal probability.\nThe NSP objective was designed to improve\nperformance on downstream tasks, such as Natural\nLanguage Inference ( Bowman et al. ,2015 ), which\nrequire reasoning about the relationships between\npairs of sentences.\n2.4 Optimization\nBERT is optimized with Adam ( Kingma and Ba ,\n2015 ) using the following parameters: β1= 0.9,\nβ2= 0.999,ǫ=1e-6 and L2weight de-\ncay of0.01. The learning rate is warmed up\nover the ﬁrst 10,000 steps to a peak value of\n1e-4, and then linearly decayed. BERT trains\nwith a dropout of 0.1 on all layers and at-\ntention weights, and a GELU activation func-\ntion ( Hendrycks and Gimpel ,2016 ). Models are\npretrained for S=1,000,000 updates, with mini-\nbatches containing B=256 sequences of maxi-\nmum length T=512 tokens.\n2.5 Data\nBERT is trained on a combination of B OOK COR-\nPUS (Zhu et al. ,2015 ) plus English W IKIPEDIA ,\nwhich totals 16GB of uncompressed text.3\n3 Experimental Setup\nIn this section, we describe the experimental setup\nfor our replication study of BERT.\n3.1 Implementation\nWe reimplement BERT in FAIRSEQ (Ott et al. ,\n2019 ). We primarily follow the original BERT\n3Yang et al. (2019 ) use the same dataset but report having\nonly 13GB of text after data cleaning. This is most likely due\nto subtle differences in cleaning of the Wikipedia data.'
 'BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout }@google.com\nAbstract\nWe introduce a new language representa-\ntion model called BERT , which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be ﬁne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeciﬁc architecture modiﬁcations.\nBERT is conceptually simple and empirically\npowerful. It obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n1 Introduction\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce ﬁne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).There are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based andﬁne-tuning . The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speciﬁc architectures that\ninclude the pre-trained representations as addi-\ntional features. The ﬁne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speciﬁc parameters, and is trained on the\ndownstream tasks by simply ﬁne-tuning allpre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the ﬁne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying ﬁne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\nIn this paper, we improve the ﬁne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations from Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a “masked lan-\nguage model” (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the maskedarXiv:1810.04805v2  [cs.CL]  24 May 2019'
 '(Zellers et al., 2018), this new task of common-\nsense natural language inference seemed trivial\nfor humans (88%) and yet challenging for then-\nstate-of-the-art models ( ă60%), including ELMo\n(Peters et al., 2018). However, BERT (Devlin\net al., 2018) soon reached over 86%, almost\nhuman-level performance. One news article on\nthis development was headlined “ ﬁnally, a ma-\nchine that can ﬁnish your sentence. ”1\nIn this paper, we investigate the following ques-\ntion: How well do deep pretrained models, like\n1A New York Times article at https: //nyti.ms /2DycutY.\n1arXiv:1905.07830v1  [cs.CL]  19 May 2019']",Masked LM (MLM) and Next Sentence Prediction (NSP).,0.9999999999666667,1.0,1.0000000000000002,1.0,0.9787201029065133,0.9148804116260533
"What model sizes are reported for BERT, and what are their specifications?","BERT has two main model sizes: BERT BASE with 110M parameters and BERT LARGE with 340M parameters. BERT BASE typically has L=12 layers, H=768 hidden units, and A=12 attention heads, while BERT LARGE has L=24 layers, H=1024 hidden units, and A=16 attention heads.","['is (L=64, H=512, A=2) with 235M parameters\n(Al-Rfou et al., 2018). By contrast, BERT BASE\ncontains 110M parameters and BERT LARGE con-\ntains 340M parameters.\nIt has long been known that increasing the\nmodel size will lead to continual improvements\non large-scale tasks such as machine translation\nand language modeling, which is demonstrated\nby the LM perplexity of held-out training data\nshown in Table 6. However, we believe that\nthis is the ﬁrst work to demonstrate convinc-\ningly that scaling to extreme model sizes also\nleads to large improvements on very small scale\ntasks, provided that the model has been sufﬁ-\nciently pre-trained. Peters et al. (2018b) presented'
 'Figure 5 shows scaling values for both model and\nmodel+data parallelism. We observe excellent scaling num-\nbers in both settings. For example, the 8.3 billion parame-\nters case with 8-way (8 GPU) model parallelism achieves\n77% of linear scaling. Model+data parallelism requires fur-\nther communication of gradients and as a result the scaling\nnumbers drop slightly. However, even for the largest conﬁg-\nuration (8.3 billion parameters) running on 512 GPUs, we\nachieve 74% scaling relative to linear scaling of the strong\nsingle GPU baseline conﬁguration (1.2 billion parameters).\nFurther scaling analysis is provided in Appendix D\n5.2. Language Modeling Results Using GPT-2\nTo demonstrate that large language models can further ad-\nvance the state of the art, we consider training GPT-2 models\nof the sizes and conﬁgurations listed in Table 2. The 355M\nmodel is equivalent in size and conﬁguration of BERT-Large\nmodel (Devlin et al., 2018). The 2.5B model is bigger than\nthe previous largest GPT-2 model, and the 8.3B model is\nlarger than any left-to-right transformer language model\never trained, to the best of our knowledge. To train and eval-'
 'and (2) the number of training passes through the\ndata. For example, the recently proposed XLNet\narchitecture ( Yang et al. ,2019 ) is pretrained us-\ning nearly 10 times more data than the original\nBERT ( Devlin et al. ,2019 ). It is also trained with\na batch size eight times larger for half as many op-\ntimization steps, thus seeing four times as many\nsequences in pretraining compared to BERT.\nTo help disentangle the importance of these fac-\ntors from other modeling choices (e.g., the pre-\ntraining objective), we begin by training RoBERTa\nfollowing the BERT LARGE architecture ( L= 24 ,\nH= 1024 ,A= 16 , 355M parameters). We\npretrain for 100K steps over a comparable B OOK -\nCORPUS plus W IKIPEDIA dataset as was used in'
 '(Zellers et al., 2018), this new task of common-\nsense natural language inference seemed trivial\nfor humans (88%) and yet challenging for then-\nstate-of-the-art models ( ă60%), including ELMo\n(Peters et al., 2018). However, BERT (Devlin\net al., 2018) soon reached over 86%, almost\nhuman-level performance. One news article on\nthis development was headlined “ ﬁnally, a ma-\nchine that can ﬁnish your sentence. ”1\nIn this paper, we investigate the following ques-\ntion: How well do deep pretrained models, like\n1A New York Times article at https: //nyti.ms /2DycutY.\n1arXiv:1905.07830v1  [cs.CL]  19 May 2019']","BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M).",0.8333333332916666,0.3333333333333333,0.9545174553144801,1.0,0.9827217063787537,0.9308868255150147
How does BERT's architecture facilitate the use of a unified model across diverse NLP tasks?,"BERT's architecture utilizes a multi-layer bidirectional Transformer encoder, which allows it to jointly consider both left and right context for all tokens during pre-training. This bidirectional self-attention mechanism enables the model to generate rich representations that are applicable to various tasks without substantial task-specific modifications. As a result, the same pre-trained model can be fine-tuned for different NLP tasks with minimal changes to the architecture.","['part of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to beneﬁt from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these ﬁndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.'
 '(Zellers et al., 2018), this new task of common-\nsense natural language inference seemed trivial\nfor humans (88%) and yet challenging for then-\nstate-of-the-art models ( ă60%), including ELMo\n(Peters et al., 2018). However, BERT (Devlin\net al., 2018) soon reached over 86%, almost\nhuman-level performance. One news article on\nthis development was headlined “ ﬁnally, a ma-\nchine that can ﬁnish your sentence. ”1\nIn this paper, we investigate the following ques-\ntion: How well do deep pretrained models, like\n1A New York Times article at https: //nyti.ms /2DycutY.\n1arXiv:1905.07830v1  [cs.CL]  19 May 2019'
 'BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout }@google.com\nAbstract\nWe introduce a new language representa-\ntion model called BERT , which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be ﬁne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeciﬁc architecture modiﬁcations.\nBERT is conceptually simple and empirically\npowerful. It obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n1 Introduction\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce ﬁne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).There are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based andﬁne-tuning . The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speciﬁc architectures that\ninclude the pre-trained representations as addi-\ntional features. The ﬁne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speciﬁc parameters, and is trained on the\ndownstream tasks by simply ﬁne-tuning allpre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the ﬁne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying ﬁne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\nIn this paper, we improve the ﬁne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations from Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a “masked lan-\nguage model” (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the maskedarXiv:1810.04805v2  [cs.CL]  24 May 2019'
 'BERT BERT \nE[CLS] E1 E[SEP] ... ENE1’... EM’\nC\nT1\nT[SEP] ...\n TN\nT1’...\n TM’\n[CLS] Tok 1 [SEP] ... Tok NTok 1 ... TokM \nQuestion Paragraph Start/End Span \nBERT \nE[CLS] E1 E[SEP] ... ENE1’... EM’\nC\nT1\nT[SEP] ...\n TN\nT1’...\n TM’\n[CLS] Tok 1 [SEP] ... Tok NTok 1 ... TokM \nMasked Sentence A Masked Sentence B \nPre-training Fine-Tuning NSP Mask LM Mask LM \nUnlabeled Sentence A and B Pair SQuAD \nQuestion Answer Pair NER MNLI Figure 1: Overall pre-training and ﬁne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and ﬁne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During ﬁne-tuning, all parameters are ﬁne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n2.3 Transfer Learning from Supervised Data\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to ﬁne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n3 BERT\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and ﬁne-tuning . Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For ﬁne-\ntuning, the BERT model is ﬁrst initialized with\nthe pre-trained parameters, and all of the param-\neters are ﬁne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate ﬁne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\nA distinctive feature of BERT is its uniﬁed ar-\nchitecture across different tasks. There is mini-mal difference between the pre-trained architec-\nture and the ﬁnal downstream architecture.\nModel Architecture BERT’s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthetensor2tensor library.1Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as “The Annotated Transformer.”2\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERT BASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERT LARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\nBERT BASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n1https://github.com/tensorﬂow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/ﬁlter size to be 4H,\ni.e., 3072 for the H= 768 and 4096 for the H= 1024 .\n4We note that in the literature the bidirectional Trans-']","BERT uses a multi-layer bidirectional Transformer encoder architecture, allowing for minimal task-specific architecture modifications in fine-tuning.",0.4166666666458333,1.0,0.9080497631240698,,0.6081019603696035,0.932407841478414
Can you describe the modifications LLaMA makes to the transformer architecture for improved performance?,"The specific modifications that LLaMA makes to the transformer architecture are not detailed in the provided context. However, it mentions that the paper will present an overview of these modifications along with their training method. For more precise information, one would need to refer to the full paper.","['LLaMA: Open and Efﬁcient Foundation Language Models\nHugo Touvron∗, Thibaut Lavril∗, Gautier Izacard∗, Xavier Martinet\nMarie-Anne Lachaux, Timothee Lacroix, Baptiste Rozière, Naman Goyal\nEric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin\nEdouard Grave∗, Guillaume Lample∗\nMeta AI\nAbstract\nWe introduce LLaMA, a collection of founda-\ntion language models ranging from 7B to 65B\nparameters. We train our models on trillions\nof tokens, and show that it is possible to train\nstate-of-the-art models using publicly avail-\nable datasets exclusively, without resorting\nto proprietary and inaccessible datasets. In\nparticular, LLaMA-13B outperforms GPT-3\n(175B) on most benchmarks, and LLaMA-\n65B is competitive with the best models,\nChinchilla-70B and PaLM-540B. We release\nall our models to the research community1.\n1 Introduction\nLarge Languages Models (LLMs) trained on mas-\nsive corpora of texts have shown their ability to per-\nform new tasks from textual instructions or from a\nfew examples (Brown et al., 2020). These few-shot\nproperties ﬁrst appeared when scaling models to a\nsufﬁcient size (Kaplan et al., 2020), resulting in a\nline of work that focuses on further scaling these\nmodels (Chowdhery et al., 2022; Rae et al., 2021).\nThese efforts are based on the assumption that\nmore parameters will lead to better performance.\nHowever, recent work from Hoffmann et al. (2022)\nshows that, for a given compute budget, the best\nperformances are not achieved by the largest mod-\nels, but by smaller models trained on more data.\nThe objective of the scaling laws from Hoff-\nmann et al. (2022) is to determine how to best\nscale the dataset and model sizes for a particular\ntraining compute budget. However, this objective\ndisregards the inference budget, which becomes\ncritical when serving a language model at scale.\nIn this context, given a target level of performance,\nthe preferred model is not the fastest to train but the\nfastest at inference, and although it may be cheaper\nto train a large model to reach a certain level of\n∗Equal contribution. Correspondence: {htouvron,\nthibautlav,gizacard,egrave,glample}@meta.com\n1https://github.com/facebookresearch/llamaperformance, a smaller one trained longer will\nultimately be cheaper at inference. For instance,\nalthough Hoffmann et al. (2022) recommends\ntraining a 10B model on 200B tokens, we ﬁnd\nthat the performance of a 7B model continues to\nimprove even after 1T tokens.\nThe focus of this work is to train a series of\nlanguage models that achieve the best possible per-\nformance at various inference budgets, by training\non more tokens than what is typically used. The\nresulting models, called LLaMA , ranges from 7B\nto 65B parameters with competitive performance\ncompared to the best existing LLMs. For instance,\nLLaMA-13B outperforms GPT-3 on most bench-\nmarks, despite being 10 ×smaller. We believe that\nthis model will help democratize the access and\nstudy of LLMs, since it can be run on a single GPU.\nAt the higher-end of the scale, our 65B-parameter\nmodel is also competitive with the best large lan-\nguage models such as Chinchilla or PaLM-540B.\nUnlike Chinchilla, PaLM, or GPT-3, we only\nuse publicly available data, making our work com-\npatible with open-sourcing, while most existing\nmodels rely on data which is either not publicly\navailable or undocumented (e.g. “Books – 2TB” or\n“Social media conversations”). There exist some\nexceptions, notably OPT (Zhang et al., 2022),\nGPT-NeoX (Black et al., 2022), BLOOM (Scao\net al., 2022) and GLM (Zeng et al., 2022), but none\nthat are competitive with PaLM-62B or Chinchilla.\nIn the rest of this paper, we present an overview\nof the modiﬁcations we made to the transformer\narchitecture (Vaswani et al., 2017), as well as our\ntraining method. We then report the performance of\nour models and compare with others LLMs on a set\nof standard benchmarks. Finally, we expose some\nof the biases and toxicity encoded in our models,\nusing some of the most recent benchmarks from'
 'GPU TypeGPU PowerGPU-hoursTotal power Carbon emitted\nconsumption consumption (tCO 2eq)\nOPT-175B A100-80GB 400W 809,472 356 MWh 137\nBLOOM-175B A100-80GB 400W 1,082,880 475 MWh 183\nLLaMA-7B A100-80GB 400W 82,432 36 MWh 14\nLLaMA-13B A100-80GB 400W 135,168 59 MWh 23\nLLaMA-33B A100-80GB 400W 530,432 233 MWh 90\nLLaMA-65B A100-80GB 400W 1,022,362 449 MWh 173\nTable 15: Carbon footprint of training different models in the same data center. We follow Wu et al. (2022)\nto compute carbon emission of training OPT, BLOOM and our models in the same data center. For the power\nconsumption of a A100-80GB, we take the thermal design power for NVLink systems, that is 400W. We take a\nPUE of 1.1 and a carbon intensity factor set at the national US average of 0.385 kg CO 2e per KWh.\nstarting from feed forward models (Bengio et al.,\n2000), recurrent neural networks (Elman, 1990;\nMikolov et al., 2010) and LSTMs (Hochreiter and\nSchmidhuber, 1997; Graves, 2013). More recently,\ntransformer networks, based on self-attention, have\nled to important improvements, especially for cap-\nturing long range dependencies (Vaswani et al.,\n2017; Radford et al., 2018; Dai et al., 2019).\nScaling. There is a long history of scaling for\nlanguage models, for both the model and dataset\nsizes. Brants et al. (2007) showed the beneﬁts of\nusing language models trained on 2 trillion tokens,\nresulting in 300 billion n-grams, on the quality of\nmachine translation. While this work relied on a\nsimple smoothing technique, called Stupid Backoff ,\nHeaﬁeld et al. (2013) later showed how to scale\nKneser-Ney smoothing to Web-scale data. This\nallowed to train a 5-gram model on 975 billions to-\nkens from CommonCrawl, resulting in a model\nwith 500 billions n-grams (Buck et al., 2014).\nChelba et al. (2013) introduced the One Billion\nWord benchmark, a large scale training dataset to\nmeasure the progress of language models.\nIn the context of neural language models, Joze-\nfowicz et al. (2016) obtained state-of-the-art re-\nsults on the Billion Word benchmark by scaling\nLSTMs to 1 billion parameters. Later, scaling\ntransformers lead to improvement on many NLP\ntasks. Notable models include BERT (Devlin et al.,\n2018), GPT-2 (Radford et al., 2019), Megatron-\nLM (Shoeybi et al., 2019), and T5 (Raffel et al.,\n2020). A signiﬁcant breakthrough was obtained\nwith GPT-3 (Brown et al., 2020), a model with\n175 billion parameters. This lead to a series of\nLarge Language Models , such as Jurassic-1 (Lieber\net al., 2021), Megatron-Turing NLG (Smith et al.,2022), Gopher (Rae et al., 2021), Chinchilla (Hoff-\nmann et al., 2022), PaLM (Chowdhery et al., 2022),\nOPT (Zhang et al., 2022), and GLM (Zeng et al.,\n2022). Hestness et al. (2017) and Rosenfeld et al.\n(2019) studied the impact of scaling on the perfor-\nmance of deep learning models, showing the exis-\ntence of power laws between the model and dataset\nsizes and the performance of the system. Kaplan\net al. (2020) derived power laws speciﬁcally for\ntransformer based language models, which were\nlater reﬁned by Hoffmann et al. (2022), by adapting\nthe learning rate schedule when scaling datasets.\nFinally, Wei et al. (2022) studied the effect of scal-\ning on the abilities of large language models.\n8 Conclusion\nIn this paper, we presented a series of language\nmodels that are released openly, and competitive\nwith state-of-the-art foundation models. Most\nnotably, LLaMA-13B outperforms GPT-3 while\nbeing more than 10 ×smaller, and LLaMA-65B is\ncompetitive with Chinchilla-70B and PaLM-540B.\nUnlike previous studies, we show that it is possible\nto achieve state-of-the-art performance by training\nexclusively on publicly available data, without\nresorting to proprietary datasets. We hope that\nreleasing these models to the research community\nwill accelerate the development of large language\nmodels, and help efforts to improve their robust-\nness and mitigate known issues such as toxicity and\nbias. Additionally, we observed like Chung et al.'
 'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nFigure 2. Transformer Architecture. Purple blocks correspond to\nfully connected layers. Each blue block represents a single trans-\nformer layer that is replicated N times.\nand compute efﬁciency. The original transformer formula-\ntion was designed as a machine translation architecture that\ntransforms an input sequence into another output sequence\nusing two parts, an Encoder andDecoder . However, recent\nwork leveraging transformers for language modeling such as\nBERT (Devlin et al., 2018) and GPT-2 (Radford et al., 2019)\nuse only the Encoder orDecoder depending on their needs.\nThis work explores both a decoder architecture, GPT-2, and\nan encoder architecture, BERT.\nFigure 2 shows a schematic diagram of the model we used.\nWe refer the reader to prior work for a detailed descrip-\ntion of the model architecture (Vaswani et al., 2017; Devlin\net al., 2018; Radford et al., 2019). It is worthwhile to men-\ntion that both GPT-2 and BERT use GeLU (Hendrycks &\nGimpel, 2016) nonlinearities and layer normalization (Ba\net al., 2016) to the input of the multi-head attention and feed\nforward layers, whereas the original transformer (Vaswani\net al., 2017) uses ReLU nonlinearities and applies layer\nnormalization to outputs.\n2.3. Data and Model Parallelism in Deep Learning\nThere are two central paradigms for scaling out deep neu-\nral network training to numerous hardware accelerators:\ndata parallelism (Valiant, 1990) where a training minibatch\nis split across multiple workers, and model parallelism in\nwhich the memory usage and computation of a model is\ndistributed across multiple workers. By increasing the mini-\nbatch size proportionally to the number of available work-\ners (i.e. weak scaling ), one observes near linear scaling\nin training data throughput. However, large batch train-\ning introduces complications into the optimization process\nthat can result in reduced accuracy or longer time to conver-\ngence, offsetting the beneﬁt of increased training throughput\n(Keskar et al., 2017). Further research (Goyal et al., 2017;\nYou et al., 2017; 2019) has developed techniques to miti-gate these effects and drive down the training time of large\nneural networks. To scale out training even further, parallel\nwork (Chen et al., 2016) has combined data parallelism with\nactivation checkpointing: recomputing activations in the\nbackward pass without storing them in the forward pass to\nreduce memory requirements.\nHowever, these techniques have one fundamental limitation\nin the problem size they can tackle: the model must ﬁt\nentirely on one worker. With language models of increasing\nsize and complexity like BERT and GPT-2, neural networks\nhave approached the memory capacity of modern hardware\naccelerators. One solution to this problem is to employ\nparameter sharing to reduce the memory footprint of the\nmodel (Lan et al., 2019), but this limits the overall capacity\nof the model. Our approach is to utilize model parallelism\nto split the model across multiple accelerators. This not\nonly alleviates the memory pressure, but also increases the\namount of parallelism independently of the microbatch size.\nWithin model parallelism, there are two further paradigms:\nlayer-wise pipeline parallelism, and more general distributed\ntensor computation. In pipeline model parallelism, groups\nof operations are performed on one device before the outputs\nare passed to the next device in the pipeline where a differ-\nent group of operations are performed. Some approaches\n(Harlap et al., 2018; Chen et al., 2018) use a parameter\nserver (Li et al., 2014) in conjunction with pipeline par-\nallelism. However these suffer from inconsistency issues.\nThe GPipe framework for TensorFlow (Huang et al., 2018)\novercomes this inconsistency issue by using synchronous\ngradient decent. This approach requires additional logic to\nhandle the efﬁcient pipelining of these communication and'
 'by traditional single-GPU-per-model training by implement-\ning model parallelism with only a few modiﬁcations to\nthe existing PyTorch transformer implementations. We ef-\nﬁciently trained transformer based models up to 8.3 bil-\nlion parameter on 512 NVIDIA V100 GPUs with 8-way\nmodel parallelism and achieved up to 15.1 PetaFLOPs sus-\ntained over the entire application. We also showed that for\nBERT models, careful attention to the placement of layer\nnormalization in BERT-like models is critical to achieving\nincreased accuracies as the model size increases. We study\nthe effect of model size on down-stream task accuracy and\nachieve far superior results on downstream tasks and estab-\nlish new SOTA for WikiText103, LAMBADA, and RACE\ndatasets. Finally, we open sourced our code to enable future\nwork leveraging model parallel transformers.\nThere are several directions for future work. Continuing\nto increase the scale of pretraining is a promising line of']"," LLaMA incorporates pre-normalization (using RMSNorm), the SwiGLU activation function, and rotary embeddings (RoPE) into its transformer architecture for enhanced training stability and performance.",0.9999999999,1.0,0.0,1.0,0.201113647286567,0.804454589146268
"How does LLaMA's approach to embedding layer optimization differ from traditional transformer models, and what are the specific benefits of these modifications?","LLaMA's approach to embedding layer optimization includes techniques like shrinking the embedding layer's gradient to 0.1, which addresses numerical instability issues. This modification aims to enhance training stability and overall model performance, particularly in large language models. The specific benefits include improved training reliability and the ability to effectively leverage mixed precision training on diverse platforms.","['LLaMA: Open and Efﬁcient Foundation Language Models\nHugo Touvron∗, Thibaut Lavril∗, Gautier Izacard∗, Xavier Martinet\nMarie-Anne Lachaux, Timothee Lacroix, Baptiste Rozière, Naman Goyal\nEric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin\nEdouard Grave∗, Guillaume Lample∗\nMeta AI\nAbstract\nWe introduce LLaMA, a collection of founda-\ntion language models ranging from 7B to 65B\nparameters. We train our models on trillions\nof tokens, and show that it is possible to train\nstate-of-the-art models using publicly avail-\nable datasets exclusively, without resorting\nto proprietary and inaccessible datasets. In\nparticular, LLaMA-13B outperforms GPT-3\n(175B) on most benchmarks, and LLaMA-\n65B is competitive with the best models,\nChinchilla-70B and PaLM-540B. We release\nall our models to the research community1.\n1 Introduction\nLarge Languages Models (LLMs) trained on mas-\nsive corpora of texts have shown their ability to per-\nform new tasks from textual instructions or from a\nfew examples (Brown et al., 2020). These few-shot\nproperties ﬁrst appeared when scaling models to a\nsufﬁcient size (Kaplan et al., 2020), resulting in a\nline of work that focuses on further scaling these\nmodels (Chowdhery et al., 2022; Rae et al., 2021).\nThese efforts are based on the assumption that\nmore parameters will lead to better performance.\nHowever, recent work from Hoffmann et al. (2022)\nshows that, for a given compute budget, the best\nperformances are not achieved by the largest mod-\nels, but by smaller models trained on more data.\nThe objective of the scaling laws from Hoff-\nmann et al. (2022) is to determine how to best\nscale the dataset and model sizes for a particular\ntraining compute budget. However, this objective\ndisregards the inference budget, which becomes\ncritical when serving a language model at scale.\nIn this context, given a target level of performance,\nthe preferred model is not the fastest to train but the\nfastest at inference, and although it may be cheaper\nto train a large model to reach a certain level of\n∗Equal contribution. Correspondence: {htouvron,\nthibautlav,gizacard,egrave,glample}@meta.com\n1https://github.com/facebookresearch/llamaperformance, a smaller one trained longer will\nultimately be cheaper at inference. For instance,\nalthough Hoffmann et al. (2022) recommends\ntraining a 10B model on 200B tokens, we ﬁnd\nthat the performance of a 7B model continues to\nimprove even after 1T tokens.\nThe focus of this work is to train a series of\nlanguage models that achieve the best possible per-\nformance at various inference budgets, by training\non more tokens than what is typically used. The\nresulting models, called LLaMA , ranges from 7B\nto 65B parameters with competitive performance\ncompared to the best existing LLMs. For instance,\nLLaMA-13B outperforms GPT-3 on most bench-\nmarks, despite being 10 ×smaller. We believe that\nthis model will help democratize the access and\nstudy of LLMs, since it can be run on a single GPU.\nAt the higher-end of the scale, our 65B-parameter\nmodel is also competitive with the best large lan-\nguage models such as Chinchilla or PaLM-540B.\nUnlike Chinchilla, PaLM, or GPT-3, we only\nuse publicly available data, making our work com-\npatible with open-sourcing, while most existing\nmodels rely on data which is either not publicly\navailable or undocumented (e.g. “Books – 2TB” or\n“Social media conversations”). There exist some\nexceptions, notably OPT (Zhang et al., 2022),\nGPT-NeoX (Black et al., 2022), BLOOM (Scao\net al., 2022) and GLM (Zeng et al., 2022), but none\nthat are competitive with PaLM-62B or Chinchilla.\nIn the rest of this paper, we present an overview\nof the modiﬁcations we made to the transformer\narchitecture (Vaswani et al., 2017), as well as our\ntraining method. We then report the performance of\nour models and compare with others LLMs on a set\nof standard benchmarks. Finally, we expose some\nof the biases and toxicity encoded in our models,\nusing some of the most recent benchmarks from'
 'ideas by not just transferring the language model to extract\ncontextual word representations, but by also ﬁnetuning the\nlanguage model in an end to end fashion on downstream\ntasks. Through these works, the state of the art has advanced\nfrom transferring just word embedding tables to transferring\nentire multi-billion parameter language models. This pro-\ngression of methods has necessitated the need for hardware,\nsystems techniques, and frameworks that are able to oper-\nate efﬁciently at scale and satisfy increasing computational\nneeds. Our work aims to provide the tools necessary to take\nanother step forward in this trend.\n2.2. Transformer Language Models and Multi-Head\nAttention\nCurrent work in NLP trends towards using transformer mod-\nels (Vaswani et al., 2017) due to their superior accuracy'
 'Published as a conference paper at ICLR 2023\nexact dataset setting in T0 (Sanh et al., 2022) and the information extraction datasets in GLM-130B\nto allow the correct evaluation on some types of tasks (e.g., NLI).\nFigure 14 shows the ablation results. On the 8 datasets we test, we find that the GLM objective\nis a major contributor to the improvement (from GLM (uni) to GLM + MIP (bi)). For exam-\nple, it accounts for 73% improvement in LAMBADA and 90% improvement in MMLU, which\nare very widely adopted challenging benchmarks for LLMs. As for MIP, on some datasets (e.g.,\nWiC, ReCoRD, Hellaswag), MIP may even harm the performance. While for datasets related to text\nsimilarity and coreference (e.g., WSC, BoolQ, ANLI R1), MIP is the main contributor. It is likely\nbecause the text similarity and coreference challenges, which people usually construct intentionally\nto test language models’ ability, are seldom seen in the self-supervised corpus that makes up peo-\nple’s daily written texts. Thus, MIP training mainly helps to bridge the gap between self-supervised\npre-training and these tasks.\nB.10 L ESSONS LEARNED\nLesson 1 (Bidirectional Architecture). The bidirectional-attention GLM is a strong architec-\nture alternative, in addition to GPTs.\nLesson 2 (Platform-aware Configuration). Configure LLMs based on the cluster and parallel\nstrategy used to squeeze hardware potential.\nLesson 3 (Improved Post-LN). Counter-stereotypically, DeepNorm, a type of Post-LN, is the\noption to stabilize GLM-130B.\nLesson 4 (Training Stability Categorization). Unexpected training instability that LLMs\nsuffer from arouses systematically and numerically.\nLesson 5 (Systematical Instability: FP16). Though FP16 induces more instability, it enables\ntraining and inference on diverse platforms.\nLesson 6 (Numerical Instability: Embedding Gradient Shrink). Shrinking embedding\nlayer’s gradient to its 0.1 can solve most numerical instability problems.\nLesson 7 (GLM’s INT4 Quantization Scaling Law). GLM has a unique INT4 weight quan-\ntization scaling law unobserved in GPT-style BLOOM.\nLesson 8 (Future Direction). To create powerful LLMs, the main focus can be on 1) more and\nbetter data, 2) better architectures and pre-training objectives, and 3) more sufficient training.\n30'
 'multiple times in the aggregated datasets, we used locality-\nsensitive hashing (LSH) to deduplicate content with a jac-\ncard similarity greater than 0.7. The resulting aggregate\ncorpus contains 174 GB of deduplicated text.\n4.2. Training Optimization and Hyperparameters\nTo train our models efﬁciently we utilize mixed precision\ntraining with dynamic loss scaling to take advantage of the\nV100’s Tensor Cores (Micikevicius et al., 2017; NVIDIA,\n2018). We start by initializing our weights Wwith a sim-\nple normal distribution W∼N (0,0.02). We then scale\nweights immediately before residual layers by1√\n2Nwhere\nN is the number of transformer layers comprised of self at-\ntention and MLP blocks. For our optimizer we utilize Adam\n(Kingma & Ba, 2014) with weight decay (Loshchilov &\nHutter, 2019) λ= 0.01. Additionally, we use global gradi-\nent norm clipping of 1.0 to improve the stability of training\nlarge models. In all cases, a dropout of 0.1 is used. Lastly,\nto better manage our memory footprint we utilize activation\ncheckpointing (Chen et al., 2016) after every transformer\nlayer.\nFor GPT-2 models, all training is performed with sequences\nof 1024 subword units at a batch size of 512 for 300k itera-']","LLaMA introduces optimizations in its embedding layer through techniques like factorized embeddings and the use of advanced activation functions, such as SwiGLU, for efficient computation and memory usage. These modifications benefit the model by enhancing learning efficiency and enabling the scaling of model parameters without a proportional increase in computational demand.",0.9999999999,0.42857142857142855,0.9197540681557639,0.0,0.6455466170352726,0.9155198014744237
"How were the questions for the multitask test sourced, and what was the criteria for their inclusion?","The questions for the multitask test were sourced to cover a wide range of subjects, including STEM, humanities, and social sciences, amounting to 57 subjects in total. The criteria for inclusion focused on assessing both world knowledge and problem-solving abilities, with questions ranging in difficulty from elementary to advanced professional levels. This diverse sourcing aimed to evaluate knowledge acquired during pretraining without relying on large training sets.","['F1. Qualitatively, we also found the task challenging for humans as there was little context for the\ntext snippets and the examples were drawn from plays using early English. Given this fairly high\nmachine performance and challenging nature for humans, we exclude this task from our benchmark.\nInstructions tables begin on the following page.\n10https://www.kaggle.com/c/quora-insincere-questions-classification/data\n18'
 'set, and a test set. The few-shot development set has 5questions per subject, the validation set may\nbe used for selecting hyperparameters and is made of 1540 questions, and the test set has 14079\nquestions. Each subject contains 100test examples at the minimum, which is longer than most exams\ndesigned to assess people.\nHuman-level accuracy on this test varies. Unspecialized humans from Amazon Mechanical Turk\nobtain 34.5%accuracy on this test. Meanwhile, expert-level performance can be far higher. For\nexample, real-world test-taker human accuracy at the 95th percentile is around 87% for US Medical\nLicensing Examinations, and these questions make up our “Professional Medicine” task. If we take\nthe 95th percentile human test-taker accuracy for exams that build up our test, and if we make an\neducated guess when such information is unavailable, we then estimate that expert-level accuracy is\napproximately 89.8%.\nSince our test aggregates different subjects and several levels of difﬁculty, we measure more than\nstraightforward commonsense or narrow linguistic understanding. Instead, we measure arbitrary\n3'
 'Published as a conference paper at ICLR 2021\nMEASURING MASSIVE MULTITASK\nLANGUAGE UNDERSTANDING\nDan Hendrycks\nUC BerkeleyCollin Burns\nColumbia UniversitySteven Basart\nUChicagoAndy Zou\nUC Berkeley\nMantas Mazeika\nUIUCDawn Song\nUC BerkeleyJacob Steinhardt\nUC Berkeley\nABSTRACT\nWe propose a new test to measure a text model’s multitask accuracy. The test\ncovers 57 tasks including elementary mathematics, US history, computer science,\nlaw, and more. To attain high accuracy on this test, models must possess extensive\nworld knowledge and problem solving ability. We ﬁnd that while most recent\nmodels have near random-chance accuracy, the very largest GPT-3 model improves\nover random chance by almost 20 percentage points on average. However, on every\none of the 57 tasks, the best models still need substantial improvements before\nthey can reach expert-level accuracy. Models also have lopsided performance\nand frequently do not know when they are wrong. Worse, they still have near-\nrandom accuracy on some socially important subjects such as morality and law.\nBy comprehensively evaluating the breadth and depth of a model’s academic and\nprofessional understanding, our test can be used to analyze models across many\ntasks and to identify important shortcomings.\n1 I NTRODUCTION\nNatural Language Processing (NLP) models have achieved superhuman performance on a number of\nrecently proposed benchmarks. However, these models are still well below human level performance\nfor language understanding as a whole, suggesting a disconnect between our benchmarks and the\nactual capabilities of these models. The General Language Understanding Evaluation benchmark\n(GLUE) (Wang et al., 2018) was introduced in 2018 to evaluate performance on a wide range of NLP\ntasks, and top models achieved superhuman performance within a year. To address the shortcomings\nof GLUE, researchers designed the SuperGLUE benchmark with more difﬁcult tasks (Wang et al.,\n2019). About a year since the release of SuperGLUE, performance is again essentially human-level\n(Raffel et al., 2019). While these benchmarks evaluate linguistic skills more than overall language\nunderstanding, an array of commonsense benchmarks have been proposed to measure basic reasoning\nand everyday knowledge (Zellers et al., 2019; Huang et al., 2019; Bisk et al., 2019). However, these\nrecent benchmarks have similarly seen rapid progress (Khashabi et al., 2020). Overall, the near\nhuman-level performance on these benchmarks suggests that they are not capturing important facets\nof language understanding.\nTransformer models have driven this recent progress by pretraining on massive text corpora, including\nall of Wikipedia, thousands of books, and numerous websites. These models consequently see\nextensive information about specialized topics, most of which is not assessed by existing NLP\nbenchmarks. It consequently remains an open question just how capable current language models are\nat learning and applying knowledge from many domains.\nTo bridge the gap between the wide-ranging knowledge that models see during pretraining and the\nexisting measures of success, we introduce a new benchmark for assessing models across a diverse\nset of subjects that humans learn. We design the benchmark to measure knowledge acquired during\npretraining by evaluating models exclusively in zero-shot and few-shot settings. This makes the\nbenchmark more challenging and more similar to how we evaluate humans. The benchmark covers\n57subjects across STEM, the humanities, the social sciences, and more. It ranges in difﬁculty from\nan elementary level to an advanced professional level, and it tests both world knowledge and problem\nsolving ability. Subjects range from traditional areas, such as mathematics and history, to more\n1arXiv:2009.03300v3  [cs.CY]  12 Jan 2021'
 'consisting of Amazon Mechanical Turk Human Intelligence Tasks. These are well-deﬁned tasks that\nrequire models to interact with ﬂexible formats and demonstrate multimodal understanding.\nThe Internet as a Training Set. A major distinction between our benchmark and previous multitask\nNLP benchmarks is that we do not require large training sets. Instead, we assume that models have\nacquired the requisite knowledge from reading vast quantities of diverse text from the Internet. This\n7']","Questions were manually collected by graduate and undergraduate students from freely available online sources, including practice questions for standardized tests and undergraduate courses, ensuring a wide representation of difficulty levels and subjects.",0.0,1.0,0.8943522169123735,0.0,0.6559012345779606,0.8588990559589011
How does BERT's performance on the GLUE benchmark compare to previous state-of-the-art models?,"BERT significantly outperformed previous state-of-the-art models on the GLUE benchmark, achieving scores that were much closer to human performance. For instance, BERT achieved an average score of 79.5, which was a notable improvement over models like ELMo that scored 68.7. This advancement showcased BERT's effectiveness in understanding and processing natural language tasks.","['BiLSTM+ELMo+Attn\nOpenAI GPT\nBERT + Single-task Adapters\nBERT (Large)\nBERT on STILTs\nBERT + BAM\nSemBERT\nSnorkel MeTaL\nALICE (Large)\nMT-DNN (ensemble)\nXLNet-Large (ensemble)0.50.60.70.80.91.01.11.2\nGLUE Score\nHuman Performance\nCoLA\nSST-2MRPC\nSTS-B\nQQP\nMNLIQNLI\nRTE\nWNLIFigure 1: GLUE benchmark performance for submitted systems, rescaled to set human performance\nto 1.0, shown as a single number score, and broken down into the nine constituent task performances.\nFor tasks with multiple metrics, we use an average of the metrics. More information on the tasks\nincluded in GLUE can be found in Wang et al. (2019a) and in Warstadt et al. (2019, CoLA), Socher\net al. (2013, SST-2), Dolan and Brockett (2005, MRPC), Cer et al. (2017, STS-B), and Williams et al.\n(2018, MNLI), and Rajpurkar et al. (2016, the original data source for QNLI).\nremains substantial scope for improvement towards GLUE’s high-level goals, the original version of\nthe benchmark is no longer a suitable metric for quantifying such progress.\nIn response, we introduce SuperGLUE, a new benchmark designed to pose a more rigorous test of\nlanguage understanding. SuperGLUE has the same high-level motivation as GLUE: to provide a\nsimple, hard-to-game measure of progress toward general-purpose language understanding technolo-\ngies for English. We anticipate that signiﬁcant progress on SuperGLUE should require substantive\ninnovations in a number of core areas of machine learning, including sample-efﬁcient, transfer,\nmultitask, and unsupervised or self-supervised learning.\nSuperGLUE follows the basic design of GLUE: It consists of a public leaderboard built around\neight language understanding tasks, drawing on existing data, accompanied by a single-number\nperformance metric, and an analysis toolkit. However, it improves upon GLUE in several ways:\nMore challenging tasks: SuperGLUE retains the two hardest tasks in GLUE. The remaining tasks\nwere identiﬁed from those submitted to an open call for task proposals and were selected based on\ndifﬁculty for current NLP approaches.\nMore diverse task formats: The task formats in GLUE are limited to sentence- and sentence-pair\nclassiﬁcation. We expand the set of task formats in SuperGLUE to include coreference resolution\nand question answering (QA).\nComprehensive human baselines: We include human performance estimates for all benchmark\ntasks, which verify that substantial headroom exists between a strong BERT-based baseline and\nhuman performance.\nImproved code support: SuperGLUE is distributed with a new, modular toolkit for work on\npretraining, multi-task learning, and transfer learning in NLP, built around standard tools including\nPyTorch (Paszke et al., 2017) and AllenNLP (Gardner et al., 2017).\nReﬁned usage rules: The conditions for inclusion on the SuperGLUE leaderboard have been\nrevamped to ensure fair competition, an informative leaderboard, and full credit assignment to data\nand task creators.\nThe SuperGLUE leaderboard, data, and software tools are available at super.gluebenchmark.com .\n2 Related Work\nMuch work prior to GLUE demonstrated that training neural models with large amounts of available\nsupervision can produce representations that effectively transfer to a broad range of NLP tasks\n2'
 'Table 4: Baseline performance on the SuperGLUE development.\nModel Avg BoolQ CB COPA MultiRC ReCoRD RTE WiC WSC\nMetrics Acc. Acc./F1 Acc. F1 a/EM F1/EM Acc. Acc. Acc.\nMost Frequent Class 47.7 62.2 50.0/22.2 55.0 59.9/ 0.8 32.4/31.5 52.7 50.0 63.5\nCBOW 47.7 62.4 71.4/49.6 63.0 20.3/ 0.3 14.4/13.8 54.2 55.3 61.5\nBERT 72.2 77.7 94.6/93.7 69.0 70.5/24.7 70.6/69.8 75.8 74.9 68.3\nBERT++ 74.6 80.1 96.4/95.0 78.0 70.5/24.7 70.6/69.8 82.3 74.9 68.3\nA Development Set Results\nIn Table 4, we present results of the baselines on the SuperGLUE tasks development sets.\nB Performance on GLUE Diagnostics\nFigure 2 shows the performance on the GLUE diagnostics dataset for systems submitted to the public\nleaderboard.\nDisjunction Downward Monotone Restrictivity Double Negation Prepositional Phrases60\n40\n20\n020406080\nChance\nBiLSTM+ELMo+Attn\nOpenAI GPT\nBERT + Single-task Adapters\nBERT (Large)\nBERT on STILTsBERT + BAM\nSemBERT\nSnorkel MeTaL\nALICE (Large)\nMT-DNN (ensemble)\nXLNet-Large (ensemble)\nFigure 2: Performance of GLUE submissions on selected diagnostic categories, reported using the\nR3metric scaled up by 100, as in Wang et al. (2019a, see paper for a description of the categories).\nSome initially difﬁcult categories, like double negation, saw gains from advances on GLUE, but\nothers remain hard (restrictivity) or even adversarial (disjunction, downward monotone).\nC Human Performance Estimation\nFor collecting data to establish human performance on the SuperGLUE tasks, we follow a two\nstep procedure where we ﬁrst provide some training to the crowd workers before they proceed to\nannotation. For both steps and all tasks, the average pay rate is $23.75/hr.7\nIn the training phase, workers are provided with instructions on the task, linked to an FAQ page, and\nare asked to annotate up to 30 examples from the development set. After answering each example,\nworkers are also asked to check their work against the provided ground truth label. After the training\nphase is complete, we provide the qualiﬁcation to work on the annotation phase to all workers\nwho annotated a minimum of ﬁve examples, i.e. completed ﬁve HITs during training and achieved\nperformance at, or above the median performance across all workers during training.\n7This estimate is taken from https://turkerview.com .\n16'
 'Table 1: DistilBERT retains 97% of BERT performance. Comparison on the dev sets of the\nGLUE benchmark. ELMo results as reported by the authors. BERT and DistilBERT results are the\nmedians of 5 runs with different seeds.\nModel Score CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B WNLI\nELMo 68.7 44.1 68.6 76.6 71.1 86.2 53.4 91.5 70.4 56.3\nBERT-base 79.5 56.3 86.7 88.6 91.8 89.6 69.3 92.7 89.0 53.5\nDistilBERT 77.0 51.3 82.2 87.5 89.2 88.5 59.9 91.3 86.9 56.3\nTable 2: DistilBERT yields to comparable\nperformance on downstream tasks. Com-\nparison on downstream tasks: IMDb (test ac-\ncuracy) and SQuAD 1.1 (EM/F1 on dev set).\nD: with a second step of distillation during\nﬁne-tuning.\nModel IMDb SQuAD\n(acc.) (EM/F1)\nBERT-base 93.46 81.2/88.5\nDistilBERT 92.82 77.7/85.8\nDistilBERT (D) - 79.1/86.9Table 3: DistilBERT is signiﬁcantly smaller\nwhile being constantly faster. Inference\ntime of a full pass of GLUE task STS-B (sen-\ntiment analysis) on CPU with a batch size of\n1.\nModel # param. Inf. time\n(Millions) (seconds)\nELMo 180 895\nBERT-base 110 668\nDistilBERT 66 410\nDistillation We applied best practices for training BERT model recently proposed in Liu et al. [2019].\nAs such, DistilBERT is distilled on very large batches leveraging gradient accumulation (up to 4K\nexamples per batch) using dynamic masking and without the next sentence prediction objective.\nData and compute power We train DistilBERT on the same corpus as the original BERT model:\na concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015]. DistilBERT\nwas trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the\nRoBERTa model [Liu et al., 2019] required 1 day of training on 1024 32GB V100.\n4 Experiments\nGeneral Language Understanding We assess the language understanding and generalization ca-\npabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark\n[Wang et al., 2018], a collection of 9 datasets for evaluating natural language understanding systems.\nWe report scores on the development sets for each task by ﬁne-tuning DistilBERT without the use\nof ensembling or multi-tasking scheme for ﬁne-tuning (which are mostly orthogonal to the present\nwork). We compare the results to the baseline provided by the authors of GLUE: an ELMo (Peters\net al. [2018]) encoder followed by two BiLSTMs.4\nThe results on each of the 9 tasks are showed on Table 1 along with the macro-score (average of\nindividual scores). Among the 9 tasks, DistilBERT is always on par or improving over the ELMo\nbaseline (up to 19 points of accuracy on STS-B). DistilBERT also compares surprisingly well to\nBERT, retaining 97% of the performance with 40% fewer parameters.\n4.1 Downstream task benchmark\nDownstream tasks We further study the performances of DistilBERT on several downstream tasks\nunder efﬁcient inference constraints: a classiﬁcation task (IMDb sentiment classiﬁcation - Maas et al.\n[2011]) and a question answering task (SQuAD v1.1 - Rajpurkar et al. [2016]).\nAs shown in Table 2, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb\nbenchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.9 points of the full BERT.\nWe also studied whether we could add another step of distillation during the adaptation phase by\nﬁne-tuning DistilBERT on SQuAD using a BERT model previously ﬁne-tuned on SQuAD as a\n4We use jiant [Wang et al., 2019] to compute the baseline.\n3'
 '(Zellers et al., 2018), this new task of common-\nsense natural language inference seemed trivial\nfor humans (88%) and yet challenging for then-\nstate-of-the-art models ( ă60%), including ELMo\n(Peters et al., 2018). However, BERT (Devlin\net al., 2018) soon reached over 86%, almost\nhuman-level performance. One news article on\nthis development was headlined “ ﬁnally, a ma-\nchine that can ﬁnish your sentence. ”1\nIn this paper, we investigate the following ques-\ntion: How well do deep pretrained models, like\n1A New York Times article at https: //nyti.ms /2DycutY.\n1arXiv:1905.07830v1  [cs.CL]  19 May 2019']","BERT achieved new state-of-the-art on the GLUE benchmark (80.5%), surpassing the previous best models.",0.999999999975,0.6666666666666666,0.9777424097407753,0.0,0.5605658034883583,0.9089298806201
"What significant improvements does BERT bring to the SQuAD v1.1,v2.0 and v13.5 tasks compared to prior models?","BERT brings significant improvements to the SQuAD v1.1 and v2.0 tasks by achieving state-of-the-art results, outperforming existing models by substantial margins. For SQuAD v1.1, BERT LARGE achieved an F1 score of 90.9, while for SQuAD v2.0, it improved the F1 score significantly over previous best systems. The introduction of BERT's architecture and its pretraining techniques have allowed it to excel in these tasks, demonstrating its effectiveness in understanding and generating language.","['ModelSQuAD 1.1 SQuAD 2.0\nEM F1 EM F1\nSingle models on dev, w/o data augmentation\nBERT LARGE 84.1 90.9 79.0 81.8\nXLNet LARGE 89.0 94.5 86.1 88.8\nRoBERTa 88.9 94.6 86.5 89.4\nSingle models on test (as of July 25, 2019)\nXLNet LARGE 86.3†89.1†\nRoBERTa 86.8 89.8\nXLNet + SG-Net Veriﬁer 87.0†89.9†\nTable 6: Results on SQuAD. †indicates results that de-\npend on additional external training data. RoBERTa\nuses only the provided SQuAD data in both dev and\ntest settings. BERT LARGE and XLNet LARGE results are\nfrom Devlin et al. (2019 ) and Yang et al. (2019 ), re-\nspectively.\nXLNet, while we use the same learning rate for\nall layers.\nFor SQuAD v1.1 we follow the same ﬁnetun-\ning procedure as Devlin et al. (2019 ). For SQuAD\nv2.0, we additionally classify whether a given\nquestion is answerable; we train this classiﬁer\njointly with the span predictor by summing the\nclassiﬁcation and span loss terms.\nResults We present our results in Table 6. On\nthe SQuAD v1.1 development set, RoBERTa\nmatches the state-of-the-art set by XLNet. On the\nSQuAD v2.0 development set, RoBERTa sets a\nnew state-of-the-art, improving over XLNet by 0.4\npoints (EM) and 0.6 points (F1).\nWe also submit RoBERTa to the public SQuAD\n2.0 leaderboard and evaluate its performance rel-\native to other systems. Most of the top systems\nbuild upon either BERT ( Devlin et al. ,2019 ) or\nXLNet ( Yang et al. ,2019 ), both of which rely on\nadditional external training data. In contrast, our\nsubmission does not use any additional data.\nOur single RoBERTa model outperforms all but\none of the single model submissions, and is the\ntop scoring system among those that do not rely\non data augmentation.\n5.3 RACE Results\nIn RACE, systems are provided with a passage of\ntext, an associated question, and four candidate an-\nswers. Systems are required to classify which of\nthe four candidate answers is correct.\nWe modify RoBERTa for this task by concate-Model Accuracy Middle High\nSingle models on test (as of July 25, 2019)\nBERT LARGE 72.0 76.6 70.1\nXLNet LARGE 81.7 85.4 80.2\nRoBERTa 83.2 86.5 81.3\nTable 7: Results on the RACE test set. BERT LARGE and\nXLNet LARGE results are from Yang et al. (2019 ).\nnating each candidate answer with the correspond-\ning question and passage. We then encode each of\nthese four sequences and pass the resulting [CLS]\nrepresentations through a fully-connected layer,\nwhich is used to predict the correct answer. We\ntruncate question-answer pairs that are longer than\n128 tokens and, if needed, the passage so that the\ntotal length is at most 512 tokens.\nResults on the RACE test sets are presented in\nTable 7. RoBERTa achieves state-of-the-art results\non both middle-school and high-school settings.\n6 Related Work\nPretraining methods have been designed\nwith different training objectives, includ-\ning language modeling ( Dai and Le ,2015 ;\nPeters et al. ,2018 ;Howard and Ruder ,2018 ),\nmachine translation ( McCann et al. ,2017 ), and\nmasked language modeling ( Devlin et al. ,2019 ;\nLample and Conneau ,2019 ). Many recent\npapers have used a basic recipe of ﬁnetuning\nmodels for each end task ( Howard and Ruder ,\n2018 ;Radford et al. ,2018 ), and pretraining\nwith some variant of a masked language model\nobjective. However, newer methods have\nimproved performance by multi-task ﬁne tun-\ning ( Dong et al. ,2019 ), incorporating entity\nembeddings ( Sun et al. ,2019 ), span predic-\ntion ( Joshi et al. ,2019 ), and multiple variants\nof autoregressive pretraining ( Song et al. ,2019 ;\nChan et al. ,2019 ;Yang et al. ,2019 ). Perfor-\nmance is also typically improved by training\nbigger models on more data ( Devlin et al. ,\n2019 ;Baevski et al. ,2019 ;Yang et al. ,2019 ;\nRadford et al. ,2019 ). Our goal was to replicate,\nsimplify, and better tune the training of BERT,\nas a reference point for better understanding the\nrelative performance of all of these methods.'
 '2018a; Hu et al., 2018). The top results from the\nSQuAD leaderboard do not have up-to-date public\nsystem descriptions available,11and are allowed to\nuse any public data when training their systems.\nWe therefore use modest data augmentation in\nour system by ﬁrst ﬁne-tuning on TriviaQA (Joshi\net al., 2017) befor ﬁne-tuning on SQuAD.\nOur best performing system outperforms the top\nleaderboard system by +1.5 F1 in ensembling and\n+1.3 F1 as a single system. In fact, our single\nBERT model outperforms the top ensemble sys-\ntem in terms of F1 score. Without TriviaQA ﬁne-\n11QANet is described in Yu et al. (2018), but the system\nhas improved substantially after publication.'
 'System Dev Test\nEM F1 EM F1\nTop Leaderboard Systems (Dec 10th, 2018)\nHuman - - 82.3 91.2\n#1 Ensemble - nlnet - - 86.0 91.7\n#2 Ensemble - QANet - - 84.5 90.5\nPublished\nBiDAF+ELMo (Single) - 85.6 - 85.8\nR.M. Reader (Ensemble) 81.2 87.9 82.3 88.5\nOurs\nBERT BASE (Single) 80.8 88.5 - -\nBERT LARGE (Single) 84.1 90.9 - -\nBERT LARGE (Ensemble) 85.8 91.8 - -\nBERT LARGE (Sgl.+TriviaQA) 84.2 91.1 85.1 91.8\nBERT LARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2\nTable 2: SQuAD 1.1 results. The BERT ensemble\nis 7x systems which use different pre-training check-\npoints and ﬁne-tuning seeds.\nSystem Dev Test\nEM F1 EM F1\nTop Leaderboard Systems (Dec 10th, 2018)\nHuman 86.3 89.0 86.9 89.5\n#1 Single - MIR-MRC (F-Net) - - 74.8 78.0\n#2 Single - nlnet - - 74.2 77.1\nPublished\nunet (Ensemble) - - 71.4 74.9\nSLQA+ (Single) - 71.4 74.4\nOurs\nBERT LARGE (Single) 78.7 81.9 80.0 83.1\nTable 3: SQuAD 2.0 results. We exclude entries that\nuse BERT as one of their components.\ntuning data, we only lose 0.1-0.4 F1, still outper-\nforming all existing systems by a wide margin.12\n4.3 SQuAD v2.0\nThe SQuAD 2.0 task extends the SQuAD 1.1\nproblem deﬁnition by allowing for the possibility\nthat no short answer exists in the provided para-\ngraph, making the problem more realistic.\nWe use a simple approach to extend the SQuAD\nv1.1 BERT model for this task. We treat ques-\ntions that do not have an answer as having an an-\nswer span with start and end at the [CLS] to-\nken. The probability space for the start and end\nanswer span positions is extended to include the\nposition of the [CLS] token. For prediction, we\ncompare the score of the no-answer span: snull=\nS·C+E·Cto the score of the best non-null span\n12The TriviaQA data we used consists of paragraphs from\nTriviaQA-Wiki formed of the ﬁrst 400 tokens in documents,\nthat contain at least one of the provided possible answers.System Dev Test\nESIM+GloVe 51.9 52.7\nESIM+ELMo 59.1 59.2\nOpenAI GPT - 78.0\nBERT BASE 81.6 -\nBERT LARGE 86.6 86.3\nHuman (expert)†- 85.0\nHuman (5 annotations)†- 88.0\nTable 4: SWAG Dev and Test accuracies.†Human per-\nformance is measured with 100 samples, as reported in\nthe SWAG paper.\nˆsi,j=maxj≥iS·Ti+E·Tj. We predict a non-null\nanswer when ˆsi,j> s null+τ, where the thresh-\noldτis selected on the dev set to maximize F1.\nWe did not use TriviaQA data for this model. We\nﬁne-tuned for 2 epochs with a learning rate of 5e-5\nand a batch size of 48.\nThe results compared to prior leaderboard en-\ntries and top published work (Sun et al., 2018;\nWang et al., 2018b) are shown in Table 3, exclud-\ning systems that use BERT as one of their com-\nponents. We observe a +5.1 F1 improvement over\nthe previous best system.\n4.4 SWAG\nThe Situations With Adversarial Generations\n(SWAG) dataset contains 113k sentence-pair com-\npletion examples that evaluate grounded common-\nsense inference (Zellers et al., 2018). Given a sen-\ntence, the task is to choose the most plausible con-\ntinuation among four choices.\nWhen ﬁne-tuning on the SWAG dataset, we\nconstruct four input sequences, each containing\nthe concatenation of the given sentence (sentence\nA) and a possible continuation (sentence B). The\nonly task-speciﬁc parameters introduced is a vec-\ntor whose dot product with the [CLS] token rep-\nresentation Cdenotes a score for each choice\nwhich is normalized with a softmax layer.\nWe ﬁne-tune the model for 3 epochs with a\nlearning rate of 2e-5 and a batch size of 16. Re-\nsults are presented in Table 4. BERT LARGE out-\nperforms the authors’ baseline ESIM+ELMo sys-\ntem by +27.1% and OpenAI GPT by 8.3%.\n5 Ablation Studies\nIn this section, we perform ablation experiments\nover a number of facets of BERT in order to better\nunderstand their relative importance. Additional'
 '(Zellers et al., 2018), this new task of common-\nsense natural language inference seemed trivial\nfor humans (88%) and yet challenging for then-\nstate-of-the-art models ( ă60%), including ELMo\n(Peters et al., 2018). However, BERT (Devlin\net al., 2018) soon reached over 86%, almost\nhuman-level performance. One news article on\nthis development was headlined “ ﬁnally, a ma-\nchine that can ﬁnish your sentence. ”1\nIn this paper, we investigate the following ques-\ntion: How well do deep pretrained models, like\n1A New York Times article at https: //nyti.ms /2DycutY.\n1arXiv:1905.07830v1  [cs.CL]  19 May 2019']","BERT set new records on SQuAD v1.1 and v2.0, significantly outperforming the top leaderboard systems at the time. Version 13.5 doesn't exist. ",0.8333333332916666,0.8888888888888888,0.9724994007002993,0.5,0.7430873544789747,0.8723494179158988
"What unique aspect of the LLaMA training dataset distinguishes it from datasets used by models like GPT-3, Chinchilla, and PaLM?","The unique aspect of the LLaMA training dataset is that it exclusively uses publicly available datasets, whereas models like GPT-3, Chinchilla, and PaLM often rely on proprietary or inaccessible datasets. This approach makes LLaMA compatible with open-sourcing and allows for greater transparency in its training data. Additionally, LLaMA focuses on using a larger quantity of training tokens compared to the typical datasets used by these other models.","['HellaSwag (Zellers et al., 2019), WinoGrande (Sak-\naguchi et al., 2021), ARC easy and challenge (Clark\net al., 2018) and OpenBookQA (Mihaylov et al.,\n2018). These datasets include Cloze and Winograd\nstyle tasks, as well as multiple choice question an-\nswering. We evaluate in the zero-shot setting as\ndone in the language modeling community.\nIn Table 3, we compare with existing models\nof various sizes and report numbers from the cor-\nresponding papers. First, LLaMA-65B outper-\nforms Chinchilla-70B on all reported benchmarks\nbut BoolQ. Similarly, this model surpasses PaLM-\n540B everywhere but on BoolQ and WinoGrande.\nLLaMA-13B model also outperforms GPT-3 on\nmost benchmarks despite being 10 ×smaller.\n3.2 Closed-book Question Answering\nWe compare LLaMA to existing large language\nmodels on two closed-book question answering\nbenchmarks: Natural Questions (Kwiatkowski\net al., 2019) and TriviaQA (Joshi et al., 2017). For\nboth benchmarks, we report exact match perfor-\nmance in a closed book setting, i.e., where the mod-\nels do not have access to documents that contain\nevidence to answer the question. In Table 4, we\nreport performance on NaturalQuestions, and in Ta-\nble 5, we report on TriviaQA. On both benchmarks,\nLLaMA-65B achieve state-of-the-arts performance\nin the zero-shot and few-shot settings. More im-\nportantly, the LLaMA-13B is also competitive on\nthese benchmarks with GPT-3 and Chinchilla, de-\nspite being 5-10 ×smaller. This model runs on a\nsingle V100 GPU during inference.\n0-shot 1-shot 5-shot 64-shot\nGopher 280B 43.5 - 57.0 57.2\nChinchilla 70B 55.4 - 64.1 64.6\nLLaMA7B 50.0 53.4 56.3 57.6\n13B 56.6 60.5 63.1 64.0\n33B 65.1 67.9 69.9 70.4\n65B 68.2 71.6 72.6 73.0\nTable 5: TriviaQA. Zero-shot and few-shot exact\nmatch performance on the ﬁltered dev set.\n3.3 Reading Comprehension\nWe evaluate our models on the RACE reading com-\nprehension benchmark (Lai et al., 2017). This\ndataset was collected from English reading com-\nprehension exams designed for middle and highRACE-middle RACE-high\nGPT-3 175B 58.4 45.5\nPaLM8B 57.9 42.3\n62B 64.3 47.5\n540B 68.1 49.1\nLLaMA7B 61.1 46.9\n13B 61.6 47.2\n33B 64.1 48.3\n65B 67.9 51.6\nTable 6: Reading Comprehension. Zero-shot accu-\nracy.\nschool Chinese students. We follow the evaluation\nsetup from Brown et al. (2020) and report results\nin Table 6. On these benchmarks, LLaMA-65B is\ncompetitive with PaLM-540B, and, LLaMA-13B\noutperforms GPT-3 by a few percents.\n3.4 Mathematical reasoning\nWe evaluate our models on two mathematical rea-\nsoning benchmarks: MATH (Hendrycks et al.,\n2021) and GSM8k (Cobbe et al., 2021). MATH\nis a dataset of 12K middle school and high school\nmathematics problems written in LaTeX. GSM8k\nis a set of middle school mathematical problems.\nIn Table 7, we compare with PaLM and Min-\nerva (Lewkowycz et al., 2022). Minerva is a series\nof PaLM models ﬁnetuned on 38.5B tokens ex-\ntracted from ArXiv and Math Web Pages, while\nneither PaLM or LLaMA are ﬁnetuned on mathe-\nmatical data. The numbers for PaLM and Minerva\nare taken from Lewkowycz et al. (2022), and we\ncompare with and without maj1@k .maj1@k de-\nnotes evaluations where we generate ksamples for\neach problem and perform a majority voting (Wang\net al., 2022). On GSM8k, we observe that LLaMA-\n65B outperforms Minerva-62B, although it has not\nbeen ﬁne-tuned on mathematical data.\n3.5 Code generation\nWe evaluate the ability of our models to write\ncode from a natural language description on two\nbenchmarks: HumanEval (Chen et al., 2021) and\nMBPP (Austin et al., 2021). For both tasks, the\nmodel receives a description of the program in a\nfew sentences, as well as a few input-output ex-\namples. In HumanEval, it also receives a function\nsignature, and the prompt is formatted as natural\ncode with the textual description and tests in a'
 'Humanities STEM Social Sciences Other Average\nGPT-NeoX 20B 29.8 34.9 33.7 37.7 33.6\nGPT-3 175B 40.8 36.7 50.4 48.8 43.9\nGopher 280B 56.2 47.4 71.9 66.1 60.0\nChinchilla 70B 63.6 54.9 79.3 73.9 67.5\nPaLM8B 25.6 23.8 24.1 27.8 25.4\n62B 59.5 41.9 62.7 55.8 53.7\n540B 77.0 55.6 81.0 69.6 69.3\nLLaMA7B 34.0 30.5 38.3 38.1 35.1\n13B 45.0 35.8 53.8 53.3 46.9\n33B 55.8 46.0 66.7 63.4 57.8\n65B 61.8 51.7 72.9 67.4 63.4\nTable 9: Massive Multitask Language Understanding (MMLU). Five-shot accuracy.\nthat may indicate that this benchmark is not\nreliable. On WinoGrande, the performance does\nnot correlate as well with training perplexity:\nthe LLaMA-33B and LLaMA-65B have similar\nperformance during the training.\n4 Instruction Finetuning\nIn this section, we show that brieﬂy ﬁnetuning on\ninstructions data rapidly leads to improvements\non MMLU. Although the non-ﬁnetuned version\nof LLaMA-65B is already able to follow basic in-\nstructions, we observe that a very small amount of\nﬁnetuning improves the performance on MMLU,\nand further improves the ability of the model to\nfollow instructions. Since this is not the focus of\nthis paper, we only conducted a single experiment\nfollowing the same protocol as Chung et al. (2022)\nto train an instruct model, LLaMA-I.\nOPT 30B 26.1\nGLM 120B 44.8\nPaLM 62B 55.1\nPaLM-cont 62B 62.8\nChinchilla 70B 67.5\nLLaMA 65B 63.4\nOPT-IML-Max 30B 43.2\nFlan-T5-XXL 11B 55.1\nFlan-PaLM 62B 59.6\nFlan-PaLM-cont 62B 66.1\nLLaMA-I 65B 68.9\nTable 10: Instruction ﬁnetuning – MMLU (5-shot).\nComparison of models of moderate size with and with-\nout instruction ﬁnetuning on MMLU.In Table 10, we report the results of our instruct\nmodel LLaMA-I on MMLU and compare with ex-\nisting instruction ﬁnetuned models of moderate\nsizes, namely, OPT-IML (Iyer et al., 2022) and the\nFlan-PaLM series (Chung et al., 2022). All the re-\nported numbers are from the corresponding papers.\nDespite the simplicity of the instruction ﬁnetuning\napproach used here, we reach 68.9% on MMLU.\nLLaMA-I (65B) outperforms on MMLU existing\ninstruction ﬁnetuned models of moderate sizes, but\nare still far from the state-of-the-art, that is 77.4\nfor GPT code-davinci-002 on MMLU (numbers\ntaken from Iyer et al. (2022)). The details of the\nperformance on MMLU on the 57 tasks can be\nfound in Table 16 of the appendix.\n5 Bias, Toxicity and Misinformation\nLarge language models have been showed to re-\nproduce and amplify biases that are existing in\nthe training data (Sheng et al., 2019; Kurita et al.,\n2019), and to generate toxic or offensive con-\ntent (Gehman et al., 2020). As our training dataset\ncontains a large proportion of data from the Web,\nwe believe that it is crucial to determine the po-\ntential for our models to generate such content.\nTo understand the potential harm of LLaMA-65B,\nwe evaluate on different benchmarks that measure\ntoxic content production and stereotypes detection.\nWhile we have selected some of the standard bench-\nmarks that are used by the language model com-\nmunity to indicate some of the issues with these\nmodels, these evaluations are not sufﬁcient to fully\nunderstand the risks associated with these models.'
 'LLaMA: Open and Efﬁcient Foundation Language Models\nHugo Touvron∗, Thibaut Lavril∗, Gautier Izacard∗, Xavier Martinet\nMarie-Anne Lachaux, Timothee Lacroix, Baptiste Rozière, Naman Goyal\nEric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin\nEdouard Grave∗, Guillaume Lample∗\nMeta AI\nAbstract\nWe introduce LLaMA, a collection of founda-\ntion language models ranging from 7B to 65B\nparameters. We train our models on trillions\nof tokens, and show that it is possible to train\nstate-of-the-art models using publicly avail-\nable datasets exclusively, without resorting\nto proprietary and inaccessible datasets. In\nparticular, LLaMA-13B outperforms GPT-3\n(175B) on most benchmarks, and LLaMA-\n65B is competitive with the best models,\nChinchilla-70B and PaLM-540B. We release\nall our models to the research community1.\n1 Introduction\nLarge Languages Models (LLMs) trained on mas-\nsive corpora of texts have shown their ability to per-\nform new tasks from textual instructions or from a\nfew examples (Brown et al., 2020). These few-shot\nproperties ﬁrst appeared when scaling models to a\nsufﬁcient size (Kaplan et al., 2020), resulting in a\nline of work that focuses on further scaling these\nmodels (Chowdhery et al., 2022; Rae et al., 2021).\nThese efforts are based on the assumption that\nmore parameters will lead to better performance.\nHowever, recent work from Hoffmann et al. (2022)\nshows that, for a given compute budget, the best\nperformances are not achieved by the largest mod-\nels, but by smaller models trained on more data.\nThe objective of the scaling laws from Hoff-\nmann et al. (2022) is to determine how to best\nscale the dataset and model sizes for a particular\ntraining compute budget. However, this objective\ndisregards the inference budget, which becomes\ncritical when serving a language model at scale.\nIn this context, given a target level of performance,\nthe preferred model is not the fastest to train but the\nfastest at inference, and although it may be cheaper\nto train a large model to reach a certain level of\n∗Equal contribution. Correspondence: {htouvron,\nthibautlav,gizacard,egrave,glample}@meta.com\n1https://github.com/facebookresearch/llamaperformance, a smaller one trained longer will\nultimately be cheaper at inference. For instance,\nalthough Hoffmann et al. (2022) recommends\ntraining a 10B model on 200B tokens, we ﬁnd\nthat the performance of a 7B model continues to\nimprove even after 1T tokens.\nThe focus of this work is to train a series of\nlanguage models that achieve the best possible per-\nformance at various inference budgets, by training\non more tokens than what is typically used. The\nresulting models, called LLaMA , ranges from 7B\nto 65B parameters with competitive performance\ncompared to the best existing LLMs. For instance,\nLLaMA-13B outperforms GPT-3 on most bench-\nmarks, despite being 10 ×smaller. We believe that\nthis model will help democratize the access and\nstudy of LLMs, since it can be run on a single GPU.\nAt the higher-end of the scale, our 65B-parameter\nmodel is also competitive with the best large lan-\nguage models such as Chinchilla or PaLM-540B.\nUnlike Chinchilla, PaLM, or GPT-3, we only\nuse publicly available data, making our work com-\npatible with open-sourcing, while most existing\nmodels rely on data which is either not publicly\navailable or undocumented (e.g. “Books – 2TB” or\n“Social media conversations”). There exist some\nexceptions, notably OPT (Zhang et al., 2022),\nGPT-NeoX (Black et al., 2022), BLOOM (Scao\net al., 2022) and GLM (Zeng et al., 2022), but none\nthat are competitive with PaLM-62B or Chinchilla.\nIn the rest of this paper, we present an overview\nof the modiﬁcations we made to the transformer\narchitecture (Vaswani et al., 2017), as well as our\ntraining method. We then report the performance of\nour models and compare with others LLMs on a set\nof standard benchmarks. Finally, we expose some\nof the biases and toxicity encoded in our models,\nusing some of the most recent benchmarks from'
 'MATH +maj1@k GSM8k +maj1@k\nPaLM8B 1.5 - 4.1 -\n62B 4.4 - 33.0 -\n540B 8.8 - 56.5 -\nMinerva8B 14.1 25.4 16.2 28.4\n62B 27.6 43.4 52.4 68.5\n540B 33.6 50.3 68.5 78.5\nLLaMA7B 2.9 6.9 11.0 18.1\n13B 3.9 8.8 17.8 29.3\n33B 7.1 15.2 35.6 53.1\n65B 10.6 20.5 50.9 69.7\nTable 7: Model performance on quantitative reason-\ning datasets. For majority voting, we use the same\nsetup as Minerva, with k= 256 samples for MATH\nandk= 100 for GSM8k (Minerva 540B uses k= 64\nfor MATH and and k= 40 for GSM8k). LLaMA-65B\noutperforms Minerva 62B on GSM8k, although it has\nnot been ﬁne-tuned on mathematical data.\ndocstring. The model needs to generate a Python\nprogram that ﬁts the description and satisﬁes the\ntest cases. In Table 8, we compare the pass@1\nscores of our models with existing language mod-\nels that have not been ﬁnetuned on code, namely\nPaLM and LaMDA (Thoppilan et al., 2022). PaLM\nand LLaMA were trained on datasets that contain\na similar number of code tokens.\nAs show in Table 8, for a similar number\nof parameters, LLaMA outperforms other gen-\neral models such as LaMDA and PaLM, which\nare not trained or ﬁnetuned speciﬁcally for code.\nLLaMA with 13B parameters and more outper-\nforms LaMDA 137B on both HumanEval and\nMBPP. LLaMA 65B also outperforms PaLM 62B,\neven when it is trained longer. The pass@1 results\nreported in this table were obtained by sampling\nwith temperature 0.1. The pass@100 and pass@80\nmetrics were obtained with temperature 0.8. We\nuse the same method as Chen et al. (2021) to obtain\nunbiased estimates of the pass@k.\nIt is possible to improve the performance on code\nby ﬁnetuning on code-speciﬁc tokens. For instance,\nPaLM-Coder (Chowdhery et al., 2022) increases\nthe pass@1 score of PaLM on HumanEval from\n26.2% for PaLM to 36%. Other models trained\nspeciﬁcally for code also perform better than gen-\neral models on these tasks (Chen et al., 2021; Ni-\njkamp et al., 2022; Fried et al., 2022). Finetuning\non code tokens is beyond the scope of this paper.Params HumanEval MBPP\npass@ @1 @100 @1 @80\nLaMDA 137B 14.0 47.3 14.8 62.4\nPaLM 8B 3.6∗18.7∗5.0∗35.7∗\nPaLM 62B 15.9 46.3∗21.4 63.2∗\nPaLM-cont 62B 23.7 - 31.2 -\nPaLM 540B 26.2 76.2 36.8 75.0\nLLaMA7B 10.5 36.5 17.7 56.2\n13B 15.8 52.5 22.0 64.0\n33B 21.7 70.7 30.2 73.4\n65B 23.7 79.3 37.7 76.8\nTable 8: Model performance for code generation.\nWe report the pass@ score on HumanEval and MBPP.\nHumanEval generations are done in zero-shot and\nMBBP with 3-shot prompts similar to Austin et al.\n(2021). The values marked with∗are read from ﬁgures\nin Chowdhery et al. (2022).\n3.6 Massive Multitask Language\nUnderstanding\nThe massive multitask language understanding\nbenchmark, or MMLU, introduced by Hendrycks\net al. (2020) consists of multiple choice questions\ncovering various domains of knowledge, includ-\ning humanities, STEM and social sciences. We\nevaluate our models in the 5-shot setting, using the\nexamples provided by the benchmark, and report\nresults in Table 9. On this benchmark, we observe\nthat the LLaMA-65B is behind both Chinchilla-\n70B and PaLM-540B by a few percent in average,\nand across most domains. A potential explanation\nis that we have used a limited amount of books\nand academic papers in our pre-training data, i.e.,\nArXiv, Gutenberg and Books3, that sums up to only\n177GB, while these models were trained on up to\n2TB of books. This large quantity of books used\nby Gopher, Chinchilla and PaLM may also explain\nwhy Gopher outperforms GPT-3 on this benchmark,\nwhile it is comparable on other benchmarks.\n3.7 Evolution of performance during training\nDuring training, we tracked the performance of our\nmodels on a few question answering and common\nsense benchmarks, and report them in Figure 2.\nOn most benchmarks, the performance improves\nsteadily, and correlates with the training perplexity\nof the model (see Figure 1). The exceptions are\nSIQA and WinoGrande. Most notably, on SIQA,\nwe observe a lot of variance in performance,']"," LLaMA's training dataset is distinctive for being composed exclusively of publicly available data, unlike the datasets used for GPT-3, Chinchilla, and PaLM, which include proprietary or undocumented data.",0.4166666666458333,1.0,0.9871325586195742,1.0,0.5659894119593623,0.930624314504116
"What detailed methodology does LLaMA utilize to ensure the diversity of its pre-training data, particularly in the context of filtering and language identification?","The retrieved context does not provide specific details about the methodology LLaMA uses to ensure the diversity of its pre-training data, including aspects of filtering and language identification. Therefore, I don't know the answer.","['using heuristics based on co-occurrences between subjects,\nobjects, and textual patterns in the pre-training data. Our\nmain focus is to measure the relationship between the knowl-\nedge learned by an LLM and the prevalence of that knowl-\nedge in the pre-training data. Moreover, we also conduct\nre-training experiments and study how model scaling and\nretrieval-augmentation affect knowledge learning.\nMemorization and Privacy Past work studies training\ndata memorization from the perspective of privacy, i.e., how\nLMs inadvertently reveal private text (Carlini et al., 2019;\n2021; Lee et al., 2021). These works focus on how LMs\nmemorize and repeat verbatim text samples, and the effect\nof duplicating those texts in the training set (Kandpal et al.,\n7'
 '2020; Petroni et al., 2019; De Cao et al., 2021). However, it\nremains unclear as to what kind of knowledge LMs actually\ncapture—for example, do they simply learn “easy” facts\nthat frequently appear in their pre-training data?\nWe study this question using closed-book QA evalua-\ntions (Roberts et al., 2020) of LLMs in the few-shot set-\nting (Brown et al., 2020). Models are prompted with in-\ncontext training examples (QA pairs) and a test question\nwithout any relevant background text. The goal of our work\nis to investigate the relationship between an LM’s ability\nto answer a question and the number of times information\nrelevant to that question appears in the pre-training data.\nOur Approach The key challenge is to efficiently iden-\ntify all of the documents that are relevant to a particular\nQA pair in pre-training datasets that are hundreds of giga-\nbytes in size. To tackle this, we begin by identifying the\nsalient entities that are contained in a question and its set\nof ground-truth answer aliases. We then identify relevant\npre-training documents by searching for instances where\nthe salient question entity and the answer entity co-occur.\n2'
 'responsibly constructed. We emphasize on the privacy of\ndata usage. Without privacy protection, LLMs can easily\nleak private information in generation [18]. One ethical\npractice is to filter the personally identifiable information,\nwhich has been adopted by some recent LLMs [73, 74, 75].\nSecondly, language models for release should be systemat-\nically evaluated on various scenarios and large-scale test\nsamples. We suggest that the benchmarks like HELM\ncould be set as the practice inside the future supply chain\nof language models. However, we also argue that most\ntasks of HELM only measure in the modality of natural\nlanguage, which is insufficient for multimodal LLMs, such\nas audio LLMs [76, 77, 78] and vision LLMs [78, 79, 80].\nDespite the rising benchmarks on multimodal tasks, the\nones for multimodal AI ethics have not yet been seriously\nconsidered. At the deployment stage, we note that LLMs\ncould be attacked to output malicious content or decisions,\nby unethical users [15, 29]. Thus, even internally ethical\nlanguage models can be used unethically by third parties.\nExisting strategies [81, 82] have demonstrated the effec-\ntiveness of preventing LLM abuse, though they can be\ninvalid via attacks [83]. We, therefore, encourage future\nworks to explore more feasible protections for language\nmodels. From the daily usage perspective, the users should\nbe fully aware of the shortcomings of the language model’s\napplication, and not abuse or attack language models\nfor performing unethical tasks. Most of the unethical\nbehaviors towards language models are deemed a great\nchallenge for the LLM producers, as they are almost\nunpredictable. Consequently, we would like to call for the\neducation and policy of model usage in the community.\nSpecifically, courses for proper machine learning model\nusage should be developed for guiding users to learn ‘Dos’\nand Dont’ in AI. Detailed policies could also be proposed\nto list all user’s responsibilities before the model access.\nC. Language Models Beyond ChatGPT\nThe examination of ethical implications associated with\nlanguage models necessitates a comprehensive examina-\ntion of the broader challenges that arise within the domain'
 'Large Language Models Struggle to Learn Long-Tail Knowledge\nNikhil Kandpal1Haikang Deng1Adam Roberts2Eric Wallace3Colin Raffel1\nAbstract\nThe Internet contains a wealth of knowledge—\nfrom the birthdays of historical figures to tutorials\non how to code—all of which may be learned by\nlanguage models. However, while certain pieces\nof information are ubiquitous on the web, oth-\ners appear extremely rarely. In this paper, we\nstudy the relationship between the knowledge\nmemorized by large language models and the in-\nformation in pre-training datasets scraped from\nthe web. In particular, we show that a language\nmodel’s ability to answer a fact-based question\nrelates to how many documents associated with\nthat question were seen during pre-training. We\nidentify these relevant documents by entity link-\ning pre-training datasets and counting documents\nthat contain the same entities as a given question-\nanswer pair. Our results demonstrate strong cor-\nrelational and causal relationships between accu-\nracy and relevant document count for numerous\nquestion answering datasets (e.g., TriviaQA), pre-\ntraining corpora (e.g., ROOTS), and model sizes\n(e.g., 176B parameters). Moreover, while larger\nmodels are better at learning long-tail knowledge,\nwe estimate that today’s models must be scaled by\nmany orders of magnitude to reach competitive\nQA performance on questions with little support\nin the pre-training data. Finally, we show that\nretrieval-augmentation can reduce the dependence\non relevant pre-training information, presenting a\npromising approach for capturing the long-tail.\n1. Introduction\nLarge language models (LLMs) trained on text from the\nInternet capture many facts about the world, ranging from\nwell-known factoids to esoteric domain-specific information.\nThese models implicitly store knowledge in their parameters\n1UNC Chapel Hill2Google Research3UC Berkeley. Corre-\nspondence to: Nikhil Kandpal <nkandpa2@cs.unc.edu >.\nProceedings of the 40thInternational Conference on Machine\nLearning , Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).\n100101102103104105106\nNumber of Relevant Pre-training Documents0.00.10.20.30.40.50.6QA Accuracy\nBLOOM Model\n176B\n7.1B\n3B\n1.7B\n1.1B\n560MFigure 1. Language models struggle to capture the long-tail of\ninformation on the web. Above, we plot accuracy for the BLOOM\nmodel family on TriviaQA as a function of how many documents\nin the model’s pre-training data are relevant to each question.\n(Petroni et al., 2019; Roberts et al., 2020), and given the\nscale of today’s pre-training datasets and LLMs, one would\nhope that they can learn a huge amount of information from\nweb-sourced text. However, not all of the knowledge on\nthe Internet appears equally often—there is a long-tail of\ninformation that appears rarely or only once.\nIn this work, we explore the relationship between the knowl-\nedge learned by an LLM and the information in its pre-\ntraining dataset. Specifically, we study how an LLM’s\nability to answer a question relates to how many docu-\nments associated with that question were seen during pre-\ntraining. We focus on factoid QA datasets (Joshi et al., 2017;\nKwiatkowski et al., 2019), which lets us ground question-\nanswer pairs into concrete subject-object co-occurrences.\nAs an example, for the QA pair ( In what city was the poet\nDante born? ,Florence ), we consider documents where the\nentities Dante andFlorence co-occur as highly relevant.\nTo identify these entity co-occurrences we apply a highly-\nparallelized entity linking pipeline to trillions of tokens\nfrom datasets such as C4 (Raffel et al., 2020), The Pile (Gao\net al., 2020), ROOTS (Lauren c ¸on et al., 2022), OpenWeb-\nText (Gokaslan & Cohen, 2019), and Wikipedia.\nWe observe a strong correlation between an LM’s ability to\nanswer a question and the number of pre-training documents\n1arXiv:2211.08411v2  [cs.CL]  27 Jul 2023']","LLaMA's methodology for ensuring data diversity includes comprehensive filtering criteria to remove low-quality content, language identification to support multilingual capabilities, and a balanced inclusion of various data sources such as Wikipedia, web text, and books, emphasizing the representation of a wide range of topics and languages.",0.249999999975,0.3333333333333333,0.0,0.0,0.21702817653471443,0.8681127061388577
"What are the specific domains covered by the multitask test, and why were they selected?","The multitask test covers 57 domains, including elementary mathematics, US history, computer science, law, and more. These domains were selected to evaluate a model's extensive world knowledge and problem-solving ability across a diverse set of subjects that humans typically learn. The aim is to comprehensively assess a model's academic and professional understanding, addressing the shortcomings of existing NLP benchmarks.","['Published as a conference paper at ICLR 2021\nMEASURING MASSIVE MULTITASK\nLANGUAGE UNDERSTANDING\nDan Hendrycks\nUC BerkeleyCollin Burns\nColumbia UniversitySteven Basart\nUChicagoAndy Zou\nUC Berkeley\nMantas Mazeika\nUIUCDawn Song\nUC BerkeleyJacob Steinhardt\nUC Berkeley\nABSTRACT\nWe propose a new test to measure a text model’s multitask accuracy. The test\ncovers 57 tasks including elementary mathematics, US history, computer science,\nlaw, and more. To attain high accuracy on this test, models must possess extensive\nworld knowledge and problem solving ability. We ﬁnd that while most recent\nmodels have near random-chance accuracy, the very largest GPT-3 model improves\nover random chance by almost 20 percentage points on average. However, on every\none of the 57 tasks, the best models still need substantial improvements before\nthey can reach expert-level accuracy. Models also have lopsided performance\nand frequently do not know when they are wrong. Worse, they still have near-\nrandom accuracy on some socially important subjects such as morality and law.\nBy comprehensively evaluating the breadth and depth of a model’s academic and\nprofessional understanding, our test can be used to analyze models across many\ntasks and to identify important shortcomings.\n1 I NTRODUCTION\nNatural Language Processing (NLP) models have achieved superhuman performance on a number of\nrecently proposed benchmarks. However, these models are still well below human level performance\nfor language understanding as a whole, suggesting a disconnect between our benchmarks and the\nactual capabilities of these models. The General Language Understanding Evaluation benchmark\n(GLUE) (Wang et al., 2018) was introduced in 2018 to evaluate performance on a wide range of NLP\ntasks, and top models achieved superhuman performance within a year. To address the shortcomings\nof GLUE, researchers designed the SuperGLUE benchmark with more difﬁcult tasks (Wang et al.,\n2019). About a year since the release of SuperGLUE, performance is again essentially human-level\n(Raffel et al., 2019). While these benchmarks evaluate linguistic skills more than overall language\nunderstanding, an array of commonsense benchmarks have been proposed to measure basic reasoning\nand everyday knowledge (Zellers et al., 2019; Huang et al., 2019; Bisk et al., 2019). However, these\nrecent benchmarks have similarly seen rapid progress (Khashabi et al., 2020). Overall, the near\nhuman-level performance on these benchmarks suggests that they are not capturing important facets\nof language understanding.\nTransformer models have driven this recent progress by pretraining on massive text corpora, including\nall of Wikipedia, thousands of books, and numerous websites. These models consequently see\nextensive information about specialized topics, most of which is not assessed by existing NLP\nbenchmarks. It consequently remains an open question just how capable current language models are\nat learning and applying knowledge from many domains.\nTo bridge the gap between the wide-ranging knowledge that models see during pretraining and the\nexisting measures of success, we introduce a new benchmark for assessing models across a diverse\nset of subjects that humans learn. We design the benchmark to measure knowledge acquired during\npretraining by evaluating models exclusively in zero-shot and few-shot settings. This makes the\nbenchmark more challenging and more similar to how we evaluate humans. The benchmark covers\n57subjects across STEM, the humanities, the social sciences, and more. It ranges in difﬁculty from\nan elementary level to an advanced professional level, and it tests both world knowledge and problem\nsolving ability. Subjects range from traditional areas, such as mathematics and history, to more\n1arXiv:2009.03300v3  [cs.CY]  12 Jan 2021'
 'consisting of Amazon Mechanical Turk Human Intelligence Tasks. These are well-deﬁned tasks that\nrequire models to interact with ﬂexible formats and demonstrate multimodal understanding.\nThe Internet as a Training Set. A major distinction between our benchmark and previous multitask\nNLP benchmarks is that we do not require large training sets. Instead, we assume that models have\nacquired the requisite knowledge from reading vast quantities of diverse text from the Internet. This\n7'
 'models, related to different degrees. Our experiments aim to\ntest both qualitative properties of the embedding and its per-\nformance on meta-learning tasks. We use an off-the-shelf\nResNet-34 pretrained on ImageNet as our probe network,\nwhich we found to give the best overall performance (see\nSect. 5.2). The collection of tasks is generated starting\nfrom the following four main datasets. iNaturalist [36]:\nEach task extracted corresponds to species classiﬁcation in\na given taxonomical order. For instance, the “Rodentia\ntask” is to classify species of rodents. Notice that each\ntask is deﬁned on a separate subset of the images in the\noriginal dataset; that is, the domains of the tasks are dis-\njoint. CUB-200 [37]: We use the same procedure as iNat-\nuralist to create tasks. In this case, all tasks are classiﬁca-\ntions inside orders of birds (the aves taxonomical class), and\nhave generally much less training samples than correspond-\ning tasks in iNaturalist. iMaterialist [1] and DeepFashion\n[23]: Each image in both datasets is associated with sev-\neral binary attributes ( e.g., style attributes) and categorical\nattributes ( e.g., color, type of dress, material). We binarize\nthe categorical attributes, and consider each attribute as a\nseparate task. Notice that, in this case, all tasks share the\nsame domain and are naturally correlated.\nIn total, our collection of tasks has 1460 tasks (207\niNaturalist, 25 CUB, 228 iMaterialist, 1000 DeepFashion).\nWhile a few tasks have many training examples ( e.g., hun-\ndred thousands), most have just hundreds or thousands of\nsamples. This simulates the heavy-tail distribution of data\nin real-world applications.'
 'set, and a test set. The few-shot development set has 5questions per subject, the validation set may\nbe used for selecting hyperparameters and is made of 1540 questions, and the test set has 14079\nquestions. Each subject contains 100test examples at the minimum, which is longer than most exams\ndesigned to assess people.\nHuman-level accuracy on this test varies. Unspecialized humans from Amazon Mechanical Turk\nobtain 34.5%accuracy on this test. Meanwhile, expert-level performance can be far higher. For\nexample, real-world test-taker human accuracy at the 95th percentile is around 87% for US Medical\nLicensing Examinations, and these questions make up our “Professional Medicine” task. If we take\nthe 95th percentile human test-taker accuracy for exams that build up our test, and if we make an\neducated guess when such information is unavailable, we then estimate that expert-level accuracy is\napproximately 89.8%.\nSince our test aggregates different subjects and several levels of difﬁculty, we measure more than\nstraightforward commonsense or narrow linguistic understanding. Instead, we measure arbitrary\n3']","The test covers 57 subjects across STEM, humanities, social sciences, and more, selected to evaluate a model's world knowledge and problem-solving ability across a wide range of difficulty levels, from elementary to advanced professional levels.",0.9999999999,1.0,0.9063625682710882,1.0,0.7892219088379706,0.9068876353518825
What specific enhancements are recommended for language models to bridge the knowledge application gap identified through comprehensive multitask testing?,"The retrieved context does not provide specific recommendations for enhancing language models to bridge the knowledge application gap. However, it mentions that current models need substantial improvements to reach expert-level accuracy and highlights the importance of identifying shortcomings. For detailed recommendations, further specific research would be needed.","['parameter language models, data may also become a bottleneck, as there is far less written about\nesoteric branches of knowledge than about everyday situations.\n6 C ONCLUSION\nWe introduced a new test that measures how well text models can learn and apply knowledge\nencountered during pretraining. By covering 57 subjects at varying levels of difﬁculty, the test\nassesses language understanding in greater breadth and depth than previous benchmarks. We found\nthat it has recently become possible for models to make meaningful progress on the test, but that\nstate-of-the-art models have lopsided performance and rarely excel at any individual task. We also\nshowed that current models are uncalibrated and have difﬁculty with tasks that require calculations.\nWorryingly, models also perform especially poorly on socially relevant subjects including morality\nand law. Our expansive test can help researchers pinpoint important shortcomings of models, making\nit easier to gain a clearer picture of state-of-the-art capabilities.\n8'
 'Published as a conference paper at ICLR 2021\nMEASURING MASSIVE MULTITASK\nLANGUAGE UNDERSTANDING\nDan Hendrycks\nUC BerkeleyCollin Burns\nColumbia UniversitySteven Basart\nUChicagoAndy Zou\nUC Berkeley\nMantas Mazeika\nUIUCDawn Song\nUC BerkeleyJacob Steinhardt\nUC Berkeley\nABSTRACT\nWe propose a new test to measure a text model’s multitask accuracy. The test\ncovers 57 tasks including elementary mathematics, US history, computer science,\nlaw, and more. To attain high accuracy on this test, models must possess extensive\nworld knowledge and problem solving ability. We ﬁnd that while most recent\nmodels have near random-chance accuracy, the very largest GPT-3 model improves\nover random chance by almost 20 percentage points on average. However, on every\none of the 57 tasks, the best models still need substantial improvements before\nthey can reach expert-level accuracy. Models also have lopsided performance\nand frequently do not know when they are wrong. Worse, they still have near-\nrandom accuracy on some socially important subjects such as morality and law.\nBy comprehensively evaluating the breadth and depth of a model’s academic and\nprofessional understanding, our test can be used to analyze models across many\ntasks and to identify important shortcomings.\n1 I NTRODUCTION\nNatural Language Processing (NLP) models have achieved superhuman performance on a number of\nrecently proposed benchmarks. However, these models are still well below human level performance\nfor language understanding as a whole, suggesting a disconnect between our benchmarks and the\nactual capabilities of these models. The General Language Understanding Evaluation benchmark\n(GLUE) (Wang et al., 2018) was introduced in 2018 to evaluate performance on a wide range of NLP\ntasks, and top models achieved superhuman performance within a year. To address the shortcomings\nof GLUE, researchers designed the SuperGLUE benchmark with more difﬁcult tasks (Wang et al.,\n2019). About a year since the release of SuperGLUE, performance is again essentially human-level\n(Raffel et al., 2019). While these benchmarks evaluate linguistic skills more than overall language\nunderstanding, an array of commonsense benchmarks have been proposed to measure basic reasoning\nand everyday knowledge (Zellers et al., 2019; Huang et al., 2019; Bisk et al., 2019). However, these\nrecent benchmarks have similarly seen rapid progress (Khashabi et al., 2020). Overall, the near\nhuman-level performance on these benchmarks suggests that they are not capturing important facets\nof language understanding.\nTransformer models have driven this recent progress by pretraining on massive text corpora, including\nall of Wikipedia, thousands of books, and numerous websites. These models consequently see\nextensive information about specialized topics, most of which is not assessed by existing NLP\nbenchmarks. It consequently remains an open question just how capable current language models are\nat learning and applying knowledge from many domains.\nTo bridge the gap between the wide-ranging knowledge that models see during pretraining and the\nexisting measures of success, we introduce a new benchmark for assessing models across a diverse\nset of subjects that humans learn. We design the benchmark to measure knowledge acquired during\npretraining by evaluating models exclusively in zero-shot and few-shot settings. This makes the\nbenchmark more challenging and more similar to how we evaluate humans. The benchmark covers\n57subjects across STEM, the humanities, the social sciences, and more. It ranges in difﬁculty from\nan elementary level to an advanced professional level, and it tests both world knowledge and problem\nsolving ability. Subjects range from traditional areas, such as mathematics and history, to more\n1arXiv:2009.03300v3  [cs.CY]  12 Jan 2021'
 'Large Language Models Struggle to Learn Long-Tail Knowledge\nKassner, N., Krojer, B., and Sch ¨utze, H. Are pretrained\nlanguage models symbolic reasoners over knowledge? In\nCoNLL , 2020.\nKwiatkowski, T., Palomaki, J., Rhinehart, O., Collins, M.,\nParikh, A., Alberti, C., Epstein, D., Polosukhin, I., Kelcey,\nM., Devlin, J., et al. Natural Questions: A benchmark for\nquestion answering research. In TACL , 2019.\nLauren c ¸on, H., Saulnier, L., Wang, T., Akiki, C., del Moral,\nA. V ., Scao, T. L., Werra, L. V ., Mou, C., Ponferrada,\nE. G., Nguyen, H., Frohberg, J., ˇSaˇsko, M., Lhoest, Q.,\nMcMillan-Major, A., et al. The BigScience ROOTS\ncorpus: A 1.6TB composite multilingual dataset. In\nNeurIPS , 2022.\nLee, K., Chang, M.-W., and Toutanova, K. Latent retrieval\nfor weakly supervised open domain question answering.\nInACL, 2019.\nLee, K., Ippolito, D., Nystrom, A., Zhang, C., Eck, D.,\nCallison-Burch, C., and Carlini, N. Deduplicating train-\ning data makes language models better. In ACL, 2021.\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V .,\nGoyal, N., K ¨uttler, H., Lewis, M., Yih, W.-t., Rockt ¨aschel,\nT., et al. Retrieval-augmented generation for knowledge-\nintensive NLP tasks. In NeurIPS , 2020.\nLiu, L., Lewis, P., Riedel, S., and Stenetorp, P. Challenges\nin generalization in open domain question answering. In\nFindings of NAACL , 2022.\nMallen, A., Asai, A., Zhong, V ., Das, R., Hajishirzi, H., and\nKhashabi, D. When not to trust language models: Investi-\ngating effectiveness and limitations of parametric and non-\nparametric memories. arXiv preprint arXiv:2212.10511 ,\n2022.\nMendes, P. N., Jakob, M., Garc ´ıa-Silva, A., and Bizer, C.\nDBpedia Spotlight: Shedding light on the web of docu-\nments. In International Conference on Semantic Systems ,\n2011.\nPetroni, F., Rockt ¨aschel, T., Lewis, P., Bakhtin, A., Wu,\nY ., Miller, A. H., and Riedel, S. Language models as\nknowledge bases? In EMNLP , 2019.\nPetroni, F., Lewis, P. S. H., Piktus, A., Rockt ¨aschel, T., Wu,\nY ., Miller, A. H., and Riedel, S. How context affects\nlanguage models’ factual predictions. In AKBC , 2020.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W., Liu, P. J., et al. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer. In JMLR , 2020.Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. SQuAD:\n100,000+ questions for machine comprehension of text.\nInEMNLP , 2016.\nRao, R. M., Liu, J., Verkuil, R., Meier, J., Canny, J., Abbeel,\nP., Sercu, T., and Rives, A. Msa transformer. In ICML ,\n2021.\nRazeghi, Y ., Logan IV , R. L., Gardner, M., and Singh, S.\nImpact of pretraining term frequencies on few-shot rea-\nsoning. In Findings of the Association for Computational\nLinguistics: EMNLP 2022 , 2022.\nRoberts, A., Raffel, C., and Shazeer, N. How much knowl-\nedge can you pack into the parameters of a language\nmodel? In EMNLP , 2020.\nRobertson, S. and Zaragoza, H. The probabilistic relevance\nframework: BM25 and beyond. Foundations and Trends\nin IR , 2009.\nScao, T. L., Fan, A., Akiki, C., Pavlick, E.-J., Ili’c, S.,\nHesslow, D., Castagn’e, R., Luccioni, A. S., Yvon, F.,\nGall´e, M., Tow, J., Rush, A. M., et al. BLOOM: A\n176b-parameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100 , 2022.\nShin, S., Lee, S.-W., Ahn, H., Kim, S., Kim, H., Kim, B.,\nCho, K., Lee, G., Park, W., Ha, J.-W., et al. On the\neffect of pretraining corpora on in-context learning by a\nlarge-scale language model. In NAACL , 2022.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. In NeurIPS , 2017.\nWang, B. and Komatsuzaki, A. GPT-J-6B: A 6 Billion\nParameter Autoregressive Language Model. https://\ngithub.com/kingoflolz/mesh-transformer-jax , 2021.\nWang, T., Roberts, A., Hesslow, D., Scao, T. L., Chung,\nH. W., Beltagy, I., Launay, J., and Raffel, C. What lan-\nguage model architecture and pretraining objective work'
 'Large Language Models Struggle to Learn Long-Tail Knowledge\n0.00.10.20.30.40.50.60.7QA Accuracy\nGPT-Neo Model\nHuman\n20B6B\n2.7B1.3B\n125M\n100101102103104105106\nNumber of Relevant Pre-training Documents0500010000Count\nFigure 7. Models with access to the required background context\ndo not struggle on questions with low relevant document count.\nConcretely, we provide questions and gold paragraphs to GPT-Neo\nmodels on Natural Questions, and their accuracy trends roughly\nmatch the trends of humans.\n4.3. Can We Use Retrieval Augmentation?\nThus far, we use LMs as isolated systems that do not lever-\nage external information. However, for knowledge-intensive\ntasks, a natural alternative is to make LMs retrieval-\naugmented , i.e., combine them with a retrieval module that\nreturns relevant textual contexts (Lewis et al., 2020; Guu\net al., 2020; Karpukhin et al., 2020). Here, we study whether\nretrieval-augmented models can mitigate the dependence on\nthe amount of relevant knowledge in the pre-training data.\nOracle Retrieval We first study an oracle setting where\nwe provide LMs with a gold paragraph from Wikipedia\nthat supports each answer in Natural Questions (Petroni\net al., 2020). We use the 300-word segment that surrounds\nthe ground-truth answer from the gold Wikipedia page and\nevaluate the 2-shot accuracy of GPT-Neo. Figure 7 shows\nthat oracle retrieval-augmentation dramatically boosts accu-\nracy over closed-book models, especially on rarer instances.\nSimilar to Liu et al. (2022), we also find that QA accuracy\nactually goes down as the number of relevant documents\nincreases—the opposite trend of closed-book LLMs. As\ndiscussed in Section 3.1, humans exhibit the same trend,\nlikely because rare questions are easier on average when\nrelevant context information.BM25 Retrieval We next follow a common retrieval-\naugmented baseline, where we use a BM25 re-\ntriever (Robertson & Zaragoza, 2009) to select paragraphs\nfrom Wikipedia. We add the top-3 highest scoring para-\ngraphs into the prompt for both the in-context training ex-\namples and the test question. We verify that at least one\nof the retrieved paragraphs contains the answer for each\nin-context training example, to ensure that the LM learns to\nutilize on the documents.\nWe first evaluate the BM25 retriever’s top- krecall on its\nknowledge corpus (Wikipedia) as a function of relevant doc-\nument count, and plot the results in Figure 8. We find that\nBM25 attains reasonably high recall, especially for larger\nvalues of k. However, the BM25 retriever still shows a mild\ndependence on relevant document count. We next evalu-\nate the accuracy of BM25-augmented GPT-Neo models on\nNatural Questions and plot the results in Figure 9. Overall,\nretrieval-augmented models outperform their closed-book\ncounterparts across all ranges of relevant document counts,\nand especially on rare examples. These results suggest that\nretrieval augmentation provides a promising path towards\nimproving performance on questions with few relevant doc-\numents in the pre-training dataset.\n5. Related Work\nIdentifying The Origins of Few-shot Learning Our\nwork contributes to an emerging line of research that ex-\nplains the success of zero- and few-shot learning in language\nmodels by tracing their behavior back to the pre-training\ndata. For example, Razeghi et al. (2022) show mathemat-\nical reasoning capabilities can be correlated with training\ndata frequency, and Shin et al. (2022) and Han & Tsvetkov\n(2022) show that training corpus source can influence few-\nshot accuracies.\nThe most similar work to ours in this context is Elazar et al.\n(2022), who use causal inference to measure the effect of\npre-training data statistics on QA performance. Their main\nfocus is testing the extent to which LMs answer questions\nusing heuristics based on co-occurrences between subjects,\nobjects, and textual patterns in the pre-training data. Our\nmain focus is to measure the relationship between the knowl-']","Enhancements should focus on developing models with improved procedural knowledge, superior calculation abilities, and a more accurate calibration between confidence and actual performance, directly addressing the weaknesses uncovered in current evaluations.",0.999999999975,0.75,0.0,1.0,0.21503020519377605,0.8601208207751042
What methodology does DetectGPT use to generate minor perturbations in the candidate passage for evaluation?,"DetectGPT uses a perturbation function q(· |x) that produces slightly modified versions of the candidate passage x, while preserving its meaning. This can involve techniques such as asking a human to rewrite sentences in the passage. The method evaluates the perturbation discrepancy between the original and perturbed texts to distinguish between machine-generated and human-written samples.","['models other than the source model. See Mireshghallah\net al. (2023) for a comprehensive evaluation in this setting.\nThe detection criterion we propose, DetectGPT, also makes\nuse of generic pre-trained mask-filling models in order to\ngenerate passages that are ‘nearby’ the candidate passage.\nHowever, these mask-filling models are used off-the-shelf,\nwithout any fine-tuning or adaptation to the target domain.\n4. DetectGPT: Zero-shot Machine-Generated\nText Detection with Random Perturbations\nDetectGPT is based on the hypothesis that samples from a\nsource model pθtypically lie in areas of negative curvature\nof the log probability function of pθ, unlike human text. In\nother words, if we apply small perturbations to a passage\nx∼pθ, producing ˜x, the quantity logpθ(x)−logpθ(˜x)\nshould be relatively large on average for machine-generated\nsamples compared to human-written text. To leverage this\nhypothesis, first consider a perturbation function q(· |x)\nthat gives a distribution over ˜x, slightly modified versions of\nxwith similar meaning (we will generally consider roughly\nparagraph-length texts x). As an example, q(· |x)might be\nthe result of simply asking a human to rewrite one of the\nsentences of x, while preserving the meaning of x. Using\nthe notion of a perturbation function, we can define the\nperturbation discrepancy d(x, pθ, q):\nd(x, pθ, q)≜logpθ(x)−E˜x∼q(·|x)logpθ(˜x)(1)\nWe state our hypothesis more formally as the Local Pertur-\nbation Discrepancy Gap Hypothesis, which describes a gap\nin the perturbation discrepancy for model-generated text\nand human-generated text.\nPerturbation Discrepancy Gap Hypothesis. Ifqproduces\nsamples on the data manifold, d(x, pθ, q)is positive and\nlarge with high probability for samples x∼pθ. For human-\nwritten text, d(x, pθ, q)tends toward zero for all x.\n3'
 'Zero-Shot Machine-Generated Text Detection using Probability Curvature\n0.0 0.1 0.20204060XSum\n0.1\n 0.0 0.1 0.2WritingPrompts\n0.0 0.1 0.20204060SQuAD\nHuman\nModel\n0.1\n 0.0 0.1 0.2 0.3PubMed\n0.0 0.2 0.4 0.6 0.8 1.0\nLog Probability Change (Perturbation Discrepancy)0.00.20.40.60.81.0Frequency\nFigure 9. Perturbation discrepancy distributions for GPT-Neo\n(2.7B) and humans across domains. A threshold of 0.1 gener-\nally separates model- and human-generated text well, which is\nimportant for practical scenarios where the domain is unknown.\nidentify a property of the log probability function computed\nby a wide variety of large language models, showing that a\ntractable approximation to the trace of the Hessian of the\nmodel’s log probability function provides a useful signal\nfor detecting model samples. Our experiments find that\nthis signal is more discriminative than existing zero-shot\ndetection methods and is competitive with bespoke detection\nmodels trained with millions of model samples.\nDetectGPT and Watermarking. One interpretation of\nthe perturbation function is producing semantically similar\nrephrasings of the original passage . If these rephrasings\nare systematically lower-probability than the original pas-\nsage, the model is exposing its bias toward the specific (and\nroughly arbitrary, by human standards) phrasing used. In\nother words, LLMs that do not perfectly imitate human\nwriting essentially watermark themselves implicitly. Under\nthis interpretation, efforts to manually add watermarking bi-\nases to model outputs (Aaronson, 2022; Kirchenbauer et al.,\n2023) may further improve the effectiveness of methods\nsuch as DetectGPT, even as LLMs continue to improve.\nLimitations. One limitation of probability-based methods\nfor zero-shot machine-generated text detection (like Detect-\nGPT) is the white-box assumption that we can evaluate log\nprobabilities of the model(s) in question. For models be-\nhind APIs that do provide probabilities (such as GPT-3),\nevaluating probabilities nonetheless costs money. Another\nassumption of DetectGPT is access to a reasonable pertur-\nbation function. While in this work, we use off-the-shelf\nmask-filling models such as T5 and mT5 (for non-English\nlanguages), some domains may see reduced performance\nif existing mask-filling models do not well represent the\nspace of meaningful rephrases, reducing the quality of the\ncurvature estimate. While DetectGPT provides the best\navailable detection performance for PubMedQA, its drop\nin performance compared to other datasets may be a result\nAverage length0.9850.9900.995AUROC\ngpt-2\nAverage length0.960.970.980.99AUROC\nopt-2.7\nXSum\nSQuAD\nWritingPrompts\n130 140 150 160 170\nAverage length0.8750.9000.9250.9500.975AUROC\nEleutherAI/gpt-j-6b\n130 140 150 160 170\nAverage length0.70.80.9AUROC\nEleutherAI/gpt-neox-20bFigure 10. DetectGPT AUROC vs passage length. The relation-\nship between detection performance and passage length generally\ndepends on the dataset and model (or tokenizer). Decreases in\ndetection quality with increasing length may be due to T5 failing\nto track many (20+) masks to fill at once; this problem may be\nmitigated by applying mask-fills in a sequence of smaller batches.\nof lower quality perturbations. Finally, DetectGPT is more\ncompute-intensive than other methods for detection, as it\nrequires sampling and scoring the set of perturbations for\neach candidate passage, rather than just the candidate pas-\nsage; a better tuned perturbation function or more efficient\ncurvature approximation may help mitigate these costs.\nFuture Work. While the methods in this work make no\nassumptions about the models generating the samples, fu-\nture work may explore how watermarking algorithms can be\nused in conjunction with detection algorithms like Detect-\nGPT to further improve detection robustness as language\nmodels continually improve their reproductions of human\ntext. Separately, the results in Section 5.2 suggest that ex-\ntending DetectGPT to use ensembles of models for scoring,'
 'across all augmented variants. In contrast, InstructGPT\nand GLM exhibit a statistically significant degradation\nin performance, averaging approximately 5% across the\nvarious perturbation methods. Notably, all three language\nmodels exhibit a heightened level of susceptibility to\nsemantics-altering perturbations in the IMDB sentiment\nanalysis scenario, as indicated by the EM ctstmetric.\nIn addition, the model performance is evaluated on\nthe BoolQA dataset using identical experimental settings.\nThe results of this evaluation are consistent with the\naforementioned findings, with ChatGPT outperforming\nthe other two baselines on both the original and per-\nturbed subsets. Furthermore, it is observed that the three\nlanguage models exhibit a degradation in performance\nwhen subjected to contrasting semantic perturbations,\nsuggesting that equivariance of robustness may pose a\nsignificant challenge for the current advanced language\nmodels and their applications.'
 'models continually improve their reproductions of human\ntext. Separately, the results in Section 5.2 suggest that ex-\ntending DetectGPT to use ensembles of models for scoring,\nrather than a single model, may improve detection in the\nblack box setting. Another topic that remains unexplored\nis the relationship between prompting and detection; that\nis, can a clever prompt successfully prevent a model’s gen-\nerations from being detected by existing methods? Finally,\nfuture work may explore whether the local log probabil-\nity curvature property we identify is present for generative\nmodels in other domains, such as audio, video, or images.\nWe hope that the present work serves as inspiration to fu-\nture work developing effective, general-purpose methods\nfor mitigating potential harms of machine-generated media.\nAcknowledgements\nEM gratefully acknowledges funding from a Knight-\nHennessy Graduate Fellowship. CF and CM are CIFAR\nFellows. The Stanford Center for Research on Foundation\nModels (CRFM) provided part of the compute resources\nused for the experiments in this work.\n9']","DetectGPT generates minor perturbations using a generic pre-trained model, such as T5, and then compares the log probability of the original sample with each perturbed sample.",0.99999999995,1.0,0.9218059454509913,1.0,0.37618714969515754,0.9047485987806302
Discuss the significance of DetectGPT's detection approach in the context of evolving LLM capabilities and the potential for misuse.,"DetectGPT's detection approach is significant as it addresses the challenge of identifying machine-generated text amidst the evolving capabilities of large language models (LLMs). With LLMs becoming increasingly sophisticated, the risk of misuse, such as generating misleading or harmful content, rises. DetectGPT aims to improve detection accuracy by leveraging model scoring strategies, thus helping to mitigate ethical risks associated with the proliferation of machine-generated media.","['models continually improve their reproductions of human\ntext. Separately, the results in Section 5.2 suggest that ex-\ntending DetectGPT to use ensembles of models for scoring,\nrather than a single model, may improve detection in the\nblack box setting. Another topic that remains unexplored\nis the relationship between prompting and detection; that\nis, can a clever prompt successfully prevent a model’s gen-\nerations from being detected by existing methods? Finally,\nfuture work may explore whether the local log probabil-\nity curvature property we identify is present for generative\nmodels in other domains, such as audio, video, or images.\nWe hope that the present work serves as inspiration to fu-\nture work developing effective, general-purpose methods\nfor mitigating potential harms of machine-generated media.\nAcknowledgements\nEM gratefully acknowledges funding from a Knight-\nHennessy Graduate Fellowship. CF and CM are CIFAR\nFellows. The Stanford Center for Research on Foundation\nModels (CRFM) provided part of the compute resources\nused for the experiments in this work.\n9'
 'to the efficient data usage. For example, experiments show\nthat curriculum learning [93], active learning [94] and\nprompting [95] could improve the data efficiency. However,\nmostofthesestrategiesarestillattheearlystageandneed\nthe further investigation.\nc) Computational Resource: As LLMs are growing\nbigger and bigger, the deployment and training of these\nmodels are getting more and more costly. Daily prac-\ntitioners in NLP and deep learning will find it hard\nto install the LLMs on their own devices. Previous\nstudy [96] also show that the computational resource\nrequirements for strong model scaling clearly outpaces\nthat of system hardware. We argue that model scaling\nmay be inevitable, which is determined by the scaling law.\nHowever, recent attempts among model design, tuning\nstrategy and compression could possibly mitigate the\nextreme consumption of the computational resources. As\nWu et al. [97] have summarized most works around this\ntopic,wedonottendtoelaboratetheintroductionofthese\napproaches and designs. In addition, the increasing de-\nmand of computational resources is leading to the energy\nconsumption and carbon emission, negatively impacting\nthe environment [97]. Hence, we encourage more advanced\nhardware-software co-designs in computation to optimize\nthe carbon footprint in LLMs.\nV. Conclusion\nWe present a comprehensive diagnosis on the AI ethics\nencoded by ChatGPT, including bias, robustness, reliabil-\nity and toxicity. By measuring on a number of benchmarks\nand case studies, we find that ChatGPT may perform\nslightly better than current SOTA language models, while\nshowing the evidence of ethical risks. Concretely, we reveal\nthat ChatGPT is sensible to prompt injections for unethi-\ncal behaviors. We further provide an outlook of ethical\nchallenges to develop advance language models. Then,\nwe provide suggestions on the directions and strategies\nto design ethical language models. We believe that our\nresearch can inspire researchers to focus more effort on\nlanguage models and their evaluations.\nVI. Limitations\nThe primary limitation of the study pertains to the\nvalidity of our empirical analysis of ChatGPT. It is ac-'
 'Zero-Shot Machine-Generated Text Detection using Probability Curvature\ngenerated text detection. Other work explores watermarks\nfor generated text (Kirchenbauer et al., 2023), which modify\na model’s generations to make them easier to detect. Our\nwork does not assume text is generated with the goal of easy\ndetection; DetectGPT detects text generated from publicly\navailable LLMs using standard LLM sampling strategies.\nThe widespread use of LLMs has led to much other con-\ntemporaneous work on detecting LLM output. Sadasivan\net al. (2023) show that the detection AUROC of the an de-\ntector is upper bounded by a function of the TV distance\nbetween the model and human text. However, we find that\nAUROC of DetectGPT is high even for the largest publicly-\navailable models (Table 2), suggesting that TV distance may\nnot correlate strongly with model scale and capability. This\ndisconnect may be exacerbated by new training objectives\nother than maximum likelihood, e.g., reinforcement learn-\ning with human feedback (Christiano et al., 2017; Ziegler\net al., 2020). Both Sadasivan et al. (2023) and Krishna et al.\n(2023) show the effectiveness of paraphrasing as a tool for\nevading detection, suggesting an important area of study\nfor future work. Liang et al. (2023) show that multi-lingual\ndetection is difficult, with non-DetectGPT detectors show-\ning bias against non-native speakers; this result highlights\nthe advantage of zero-shot detectors like DetectGPT, which\ngeneralize well to any data generated by the original gener-\nating model. Mireshghallah et al. (2023) study which proxy\nscoring models produce the most useful log probabilities\nfor detection when the generating model is not known (a\nlarge-scale version of our Figure 6). Surprisingly (but con-\nsistent with our findings), they find that smaller models are\nin fact better proxy models for performing detection with\nperturbation-based methods like DetectGPT.\nThe problem of machine-generated text detection echoes ear-\nlier work on detecting deepfakes, artificial images or videos\ngenerated by deep nets, which has spawned substantial ef-\nforts in detection of fake visual content (Dolhansky et al.,\n2020; Zi et al., 2020). While early works in deepfake de-\ntection used relatively general-purpose model architectures\n(G¨uera & Delp, 2018), many deepfake detection methods\nrely on the continuous nature of image data to achieve state-\nof-the-art performance (Zhao et al., 2021; Guarnera et al.,\n2020), making direct application to text difficult.\n3. The Zero-Shot Machine-Generated Text\nDetection Problem\nWe study zero-shot machine-generated text detection, the\nproblem of detecting whether a piece of text, or candidate\npassage x, is a sample from a source model pθ. The problem\nis zero-shot in the sense that we do not assume access to\nhuman-written or generated samples to perform detection.\nAs in prior work, we study a ‘white box’ setting (Gehrmann\net al., 2019) in which the detector may evaluate the log prob-Algorithm 1 DetectGPT model-generated text detection\n1:Input: passage x, source model pθ, perturbation function q,\nnumber of perturbations k, decision threshold ϵ\n2:˜xi∼q(· |x), i∈[1..k]// mask spans, sample replacements\n3:˜µ←1\nkP\nilogpθ(˜xi)// approximate expectation in Eq. 1\n4:ˆdx←logpθ(x)−˜µ // estimate d(x, pθ, q)\n5:˜σ2\nx←1\nk−1P\ni(logpθ(˜xi)−˜µ)2// variance for normalization\n6:ifˆdx√˜σx> ϵthen\n7: return true // probably model sample\n8:else\n9: return false // probably not model sample\nability of a sample logpθ(x). The white box setting does\nnotassume access to the model architecture or parameters.\nMost public APIs for LLMs (such as GPT-3) enable scoring\ntext, though some exceptions exist, notably ChatGPT. While\nmost of our experiments consider the white box setting, see\nSection 5.2 for experiments in which we score text using\nmodels other than the source model. See Mireshghallah\net al. (2023) for a comprehensive evaluation in this setting.\nThe detection criterion we propose, DetectGPT, also makes'
 'Red teaming ChatGPT via Jailbreaking:\nBias, Robustness, Reliability and Toxicity\nTerry Yue Zhuo1,2§, Yujin Huang2, Chunyang Chen2, Zhenchang Xing1,3\n1CSIRO’s Data61\n2Monash University\n3Australian National University\nWarning: this paper may contain content that is offen-\nsive or upsetting.\nAbstract—Recent breakthroughs in natural language pro-\ncessing (NLP) have permitted the synthesis and comprehension\nof coherent text in an open-ended way, therefore translating\nthe theoretical algorithms into practical applications. The large\nlanguagemodels(LLMs)havesignificantlyimpactedbusinesses\nsuch as report summarization software and copywriters. Ob-\nservations indicate, however, that LLMs may exhibit social\nprejudice and toxicity, posing ethical and societal dangers\nof consequences resulting from irresponsibility. Large-scale\nbenchmarks for accountable LLMs should consequently be\ndeveloped. Although several empirical investigations reveal\nthe existence of a few ethical difficulties in advanced LLMs,\nthere is little systematic examination and user study of the\nrisks and harmful behaviors of current LLM usage. To further\neducate future efforts on constructing ethical LLMs respon-\nsibly, we perform a qualitative research method called “red\nteaming” on OpenAI’s ChatGPT1to better understand the\npracticalfeaturesofethicaldangersinrecentLLMs.Weanalyze\nChatGPT comprehensively from four perspectives: 1) Bias 2)\nReliability 3) Robustness 4) Toxicity. In accordance with our\nstated viewpoints, we empirically benchmark ChatGPT on\nmultiple sample datasets. We find that a significant number of\nethical risks cannot be addressed by existing benchmarks, and\nhence illustrate them via additional case studies. In addition,\nwe examine the implications of our findings on AI ethics and\nharmal behaviors of ChatGPT, as well as future problems\nand practical design considerations for responsible LLMs. We\nbelieve that our findings may give light on future efforts to\ndetermine and mitigate the ethical hazards posed by machines\nin LLM applications.\nI. Introduction\nThe recent advancements in NLP have demonstrated\ntheir potential to positively impact society and successful\nimplementations in data-rich domains. LLMs have been\nutilized in various real-world scenarios, including search\nengines [1, 2], language translation [3, 4], and copywrit-\ning [5]. However, these applications may not fully engage\nusers due to a lack of interaction and communication [6].\nAs natural language is a medium of communication used\nby all human interlocutors, conversational language model\nagents, such as Amazon Echo [7] and Google Home [8],\nhave the potential to significantly impact people’s daily\nlives. Despite their potential benefits, unforeseen negative\n§Correspondence: terry.zhuo@monash.edu\n1In this paper, ChatGPT refers to the version released on Dec\n15th.effects on human-computer interaction have also emerged\nas NLP transitions from theory to reality. This includes\nissues such as the toxic language generated by Microsoft’s\nTwitter bot Tay [9] and the privacy breaches of Amazon\nAlexa [10]. Additionally, during the unsupervised pre-\ntraining stage, language models may inadvertently learn\nbias and toxicity from large, noisy corpora [11], which can\nbe difficult to mitigate.\nWhile studies have concluded that LLMs can be used\nfor social good in real-world applications [12], the vul-\nnerabilities described above can be exploited unethically\nfor unfair discrimination, automated misinformation, and\nillegitimate censorship [13]. Consequently, numerous re-\nsearch efforts have been undertaken on the AI ethics\nof LLMs, ranging from discovering unethical behavior to\nmitigating bias [14]. Weidinger et al. [15] systematically\nstructured the ethical risk landscape with LLMs, clearly\nidentifying six risk areas: 1) Discrimination, Exclusion,\nand Toxicity, 2) Information Hazards, 3) Misinformation\nHarms, 4) Malicious Uses, 5) Human-Computer Interac-\ntion Harms, 6) Automation, Access, and Environmental']","DtectGPT's approach is significant as it provides a robust, zero-shot detection method that does not require dataset collection or model retraining, addressing the challenge of rapidly evolving LLM capabilities and their potential misuse in generating misleading content.",0.8055555555287036,1.0,0.9205678185403658,1.0,0.8209105151055534,0.9358159734656915
"How is the student model, DistilBERT, initialized from the teacher model for effective training?","The student model, DistilBERT, is initialized by taking one layer out of every two from the teacher model, which allows it to leverage the common dimensionality between the teacher and student networks. This initialization helps the student network converge effectively during training. Additionally, this approach contributes to the overall efficiency and performance of DistilBERT.","['In this paper, we show that it is possible to reach similar performances on many downstream-tasks\nusing much smaller language models pre-trained with knowledge distillation, resulting in models\nthat are lighter and faster at inference time, while also requiring a smaller computational training\nbudget. Our general-purpose pre-trained models can be ﬁne-tuned with good performances on several\ndownstream tasks, keeping the ﬂexibility of larger models. We also show that our compressed models\nare small enough to run on the edge, e.g. on mobile devices.\nUsing a triple loss, we show that a 40% smaller Transformer (Vaswani et al. [2017]) pre-trained\nthrough distillation via the supervision of a bigger Transformer language model can achieve similar\nperformance on a variety of downstream tasks, while being 60% faster at inference time. Further\nablation studies indicate that all the components of the triple loss are important for best performances.\nWe have made the trained weights available along with the training code in the Transformers2\nlibrary from HuggingFace [Wolf et al., 2019].\n2 Knowledge distillation\nKnowledge distillation [Bucila et al., 2006, Hinton et al., 2015] is a compression technique in which\na compact model - the student - is trained to reproduce the behaviour of a larger model - the teacher -\nor an ensemble of models.\nIn supervised learning, a classiﬁcation model is generally trained to predict an instance class by\nmaximizing the estimated probability of gold labels. A standard training objective thus involves\nminimizing the cross-entropy between the model’s predicted distribution and the one-hot empirical\ndistribution of training labels. A model performing well on the training set will predict an output\ndistribution with high probability on the correct class and with near-zero probabilities on other\nclasses. But some of these ""near-zero"" probabilities are larger than others and reﬂect, in part, the\ngeneralization capabilities of the model and how well it will perform on the test set3.\nTraining loss The student is trained with a distillation loss over the soft target probabilities of\nthe teacher: Lce=∑\niti∗log(si)where ti(resp. si) is a probability estimated by the teacher\n(resp. the student). This objective results in a rich training signal by leveraging the full teacher\ndistribution. Following Hinton et al. [2015] we used a softmax-temperature :pi=exp(zi/T)∑\njexp(zj/T)\nwhere Tcontrols the smoothness of the output distribution and ziis the model score for the class i.\nThe same temperature Tis applied to the student and the teacher at training time, while at inference,\nTis set to 1 to recover a standard softmax .\nThe ﬁnal training objective is a linear combination of the distillation loss Lcewith the supervised\ntraining loss, in our case the masked language modeling lossLmlm [Devlin et al., 2018]. We found it\nbeneﬁcial to add a cosine embedding loss (Lcos) which will tend to align the directions of the student\nand teacher hidden states vectors.\n3 DistilBERT: a distilled version of BERT\nStudent architecture In the present work, the student - DistilBERT - has the same general architec-\nture as BERT. The token-type embeddings and the pooler are removed while the number of layers\nis reduced by a factor of 2. Most of the operations used in the Transformer architecture ( linear\nlayer andlayer normalisation ) are highly optimized in modern linear algebra frameworks and our\ninvestigations showed that variations on the last dimension of the tensor (hidden size dimension) have\na smaller impact on computation efﬁciency (for a ﬁxed parameters budget) than variations on other\nfactors like the number of layers. Thus we focus on reducing the number of layers.\nStudent initialization In addition to the previously described optimization and architectural choices,\nan important element in our training procedure is to ﬁnd the right initialization for the sub-network to'
 'an important element in our training procedure is to ﬁnd the right initialization for the sub-network to\nconverge. Taking advantage of the common dimensionality between teacher and student networks,\nwe initialize the student from the teacher by taking one layer out of two.\n2https://github.com/huggingface/transformers\n3E.g. BERT-base’s predictions for a masked token in "" I think this is the beginning of a\nbeautiful [MASK] "" comprise two high probability tokens ( dayandlife) and a long tail of valid predictions\n(future ,story ,world . . . ).\n2'
 '6 Conclusion and future work\nWe introduced DistilBERT, a general-purpose pre-trained version of BERT, 40% smaller, 60% faster,\nthat retains 97% of the language understanding capabilities. We showed that a general-purpose\nlanguage model can be successfully trained with distillation and analyzed the various components\nwith an ablation study. We further demonstrated that DistilBERT is a compelling option for edge\napplications.\nReferences\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. In NAACL-HLT , 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\nunsupervised multitask learners. 2019.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar S. Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke S. Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv ,\nabs/1907.11692, 2019.\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green ai. ArXiv , abs/1907.10597, 2019.\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in\nnlp. In ACL, 2019.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In NIPS , 2017.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. Transformers: State-of-the-art natural language\nprocessing, 2019.\nCristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In KDD , 2006.\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. ArXiv ,\nabs/1503.02531, 2015.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading\nbooks. 2015 IEEE International Conference on Computer Vision (ICCV) , pages 19–27, 2015.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A\nmulti-task benchmark and analysis platform for natural language understanding. In ICLR , 2018.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. Deep contextualized word representations. In NAACL , 2018.\nAlex Wang, Ian F. Tenney, Yada Pruksachatkun, Katherin Yu, Jan Hula, Patrick Xia, Raghu Pappagari, Shuning\nJin, R. Thomas McCoy, Roma Patel, Yinghui Huang, Jason Phang, Edouard Grave, Najoung Kim, Phu Mon\nHtut, Thibault F’evry, Berlin Chen, Nikita Nangia, Haokun Liu, Anhad Mohananey, Shikha Bordia, Nicolas\nPatry, Ellie Pavlick, and Samuel R. Bowman. jiant 1.1: A software toolkit for research on general-purpose\ntext understanding models. http://jiant.info/ , 2019.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y . Ng, and Christopher Potts. Learning\nword vectors for sentiment analysis. In ACL, 2011.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for machine\ncomprehension of text. In EMNLP , 2016.\nRaphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. Distilling task-speciﬁc\nknowledge from bert into simple neural networks. ArXiv , abs/1903.12136, 2019.\nDebajyoti Chatterjee. Making neural machine reading comprehension faster. ArXiv , abs/1904.00796, 2019.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better: The impact\nof student initialization on knowledge distillation. ArXiv , abs/1908.08962, 2019.\nZe Yang, Linjun Shou, Ming Gong, Wutao Lin, and Daxin Jiang. Model compression with multi-task knowledge\ndistillation for web-scale question answering system. ArXiv , abs/1904.09636, 2019.'
 'DistilBERT, a distilled version of BERT: smaller,\nfaster, cheaper and lighter\nVictor SANH, Lysandre DEBUT, Julien CHAUMOND, Thomas WOLF\nHugging Face\n{victor,lysandre,julien,thomas}@huggingface.co\nAbstract\nAs Transfer Learning from large-scale pre-trained models becomes more prevalent\nin Natural Language Processing (NLP), operating these large models in on-the-\nedge and/or under constrained computational training or inference budgets remains\nchallenging. In this work, we propose a method to pre-train a smaller general-\npurpose language representation model, called DistilBERT, which can then be ﬁne-\ntuned with good performances on a wide range of tasks like its larger counterparts.\nWhile most prior work investigated the use of distillation for building task-speciﬁc\nmodels, we leverage knowledge distillation during the pre-training phase and show\nthat it is possible to reduce the size of a BERT model by 40%, while retaining 97%\nof its language understanding capabilities and being 60% faster. To leverage the\ninductive biases learned by larger models during pre-training, we introduce a triple\nloss combining language modeling, distillation and cosine-distance losses. Our\nsmaller, faster and lighter model is cheaper to pre-train and we demonstrate its\ncapabilities for on-device computations in a proof-of-concept experiment and a\ncomparative on-device study.\n1 Introduction\nFigure 1: Parameter counts of several recently released\npretrained language models.The last two years have seen the rise\nof Transfer Learning approaches in\nNatural Language Processing (NLP)\nwith large-scale pre-trained language\nmodels becoming a basic tool in\nmany NLP tasks [Devlin et al., 2018,\nRadford et al., 2019, Liu et al., 2019].\nWhile these models lead to signiﬁ-\ncant improvement, they often have\nseveral hundred million parameters\nand current research1on pre-trained\nmodels indicates that training even\nlarger models still leads to better per-\nformances on downstream tasks.\nThe trend toward bigger models\nraises several concerns. First is the\nenvironmental cost of exponentially scaling these models’ computational requirements as mentioned\nin Schwartz et al. [2019], Strubell et al. [2019]. Second, while operating these models on-device\nin real-time has the potential to enable novel and interesting language processing applications, the\ngrowing computational and memory requirements of these models may hamper wide adoption.\n1See for instance the recently released MegatronLM ( https://nv-adlr.github.io/MegatronLM )\nEMC^2: 5th Edition Co-located with NeurIPS’19arXiv:1910.01108v4  [cs.CL]  1 Mar 2020']",DistilBERT is initialized from the teacher model by adopting every other layer to maintain some structural and knowledge integrity.,0.9166666666361111,0.6666666666666666,0.9707893732180417,1.0,0.777168612859751,0.9268562696208225
Explain how BERT uses the 'masked LM' (MLM) for its pre-training.,"BERT uses the masked language model (MLM) during pre-training by randomly selecting 15% of the input tokens and replacing them with a special token [MASK]. The objective is to predict the original tokens that were masked, utilizing both left and right context, which allows BERT to learn deep bidirectional representations. This approach contrasts with traditional unidirectional models, enabling BERT to capture more contextual information for improved language understanding.","['word based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\ntrain a deep bidirectional Transformer. In addi-\ntion to the masked language model, we also use\na “next sentence prediction” task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n• We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n• We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeciﬁc architectures. BERT is the ﬁrst ﬁne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level andtoken-level tasks, outper-\nforming many task-speciﬁc architectures.\n• BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert .\n2 Related Work\nThere is a long history of pre-training general lan-\nguage representations, and we brieﬂy review the\nmost widely-used approaches in this section.\n2.1 Unsupervised Feature-based Approaches\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods. Pre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signiﬁcant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).These approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speciﬁc architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n2.2 Unsupervised Fine-tuning Approaches\nAs with the feature-based approaches, the ﬁrst\nworks in this direction only pre-trained word em-\nbedding parameters from unlabeled text (Col-\nlobert and Weston, 2008).'
 'alternatives that lead to better downstream task\nperformance; (2) We use a novel dataset, CC-\nNEWS, and conﬁrm that using more data for pre-\ntraining further improves performance on down-\nstream tasks; (3) Our training improvements show\nthat masked language model pretraining, under\nthe right design choices, is competitive with all\nother recently published methods. We release our\nmodel, pretraining and ﬁne-tuning code imple-\nmented in PyTorch ( Paszke et al. ,2017 ).\n2 Background\nIn this section, we give a brief overview of the\nBERT ( Devlin et al. ,2019 ) pretraining approach\nand some of the training choices that we will ex-\namine experimentally in the following section.\n2.1 Setup\nBERT takes as input a concatenation of two\nsegments (sequences of tokens), x1,...,x N\nandy1,...,yM. Segments usually consist of\nmore than one natural sentence. The two seg-\nments are presented as a single input sequence\nto BERT with special tokens delimiting them:\n[CLS],x1,...,x N,[SEP],y1,...,yM,[EOS].\nMandNare constrained such that M+N < T ,\nwhereTis a parameter that controls the maximum\nsequence length during training.\nThe model is ﬁrst pretrained on a large unla-\nbeled text corpus and subsequently ﬁnetuned us-\ning end-task labeled data.\n2.2 Architecture\nBERT uses the now ubiquitous transformer archi-\ntecture ( Vaswani et al. ,2017 ), which we will not\nreview in detail. We use a transformer architecture\nwithLlayers. Each block uses Aself-attention\nheads and hidden dimension H.\n2.3 Training Objectives\nDuring pretraining, BERT uses two objectives:\nmasked language modeling and next sentence pre-\ndiction.\nMasked Language Model (MLM) A random\nsample of the tokens in the input sequence is\nselected and replaced with the special token\n[MASK]. The MLM objective is a cross-entropy\nloss on predicting the masked tokens. BERT uni-\nformly selects 15% of the input tokens for possi-\nble replacement. Of the selected tokens, 80% are\nreplaced with [MASK], 10% are left unchanged,and 10% are replaced by a randomly selected vo-\ncabulary token.\nIn the original implementation, random mask-\ning and replacement is performed once in the be-\nginning and saved for the duration of training, al-\nthough in practice, data is duplicated so the mask\nis not always the same for every training sentence\n(see Section 4.1).\nNext Sentence Prediction (NSP) NSP is a bi-\nnary classiﬁcation loss for predicting whether two\nsegments follow each other in the original text.\nPositive examples are created by taking consecu-\ntive sentences from the text corpus. Negative ex-\namples are created by pairing segments from dif-\nferent documents. Positive and negative examples\nare sampled with equal probability.\nThe NSP objective was designed to improve\nperformance on downstream tasks, such as Natural\nLanguage Inference ( Bowman et al. ,2015 ), which\nrequire reasoning about the relationships between\npairs of sentences.\n2.4 Optimization\nBERT is optimized with Adam ( Kingma and Ba ,\n2015 ) using the following parameters: β1= 0.9,\nβ2= 0.999,ǫ=1e-6 and L2weight de-\ncay of0.01. The learning rate is warmed up\nover the ﬁrst 10,000 steps to a peak value of\n1e-4, and then linearly decayed. BERT trains\nwith a dropout of 0.1 on all layers and at-\ntention weights, and a GELU activation func-\ntion ( Hendrycks and Gimpel ,2016 ). Models are\npretrained for S=1,000,000 updates, with mini-\nbatches containing B=256 sequences of maxi-\nmum length T=512 tokens.\n2.5 Data\nBERT is trained on a combination of B OOK COR-\nPUS (Zhu et al. ,2015 ) plus English W IKIPEDIA ,\nwhich totals 16GB of uncompressed text.3\n3 Experimental Setup\nIn this section, we describe the experimental setup\nfor our replication study of BERT.\n3.1 Implementation\nWe reimplement BERT in FAIRSEQ (Ott et al. ,\n2019 ). We primarily follow the original BERT\n3Yang et al. (2019 ) use the same dataset but report having\nonly 13GB of text after data cleaning. This is most likely due\nto subtle differences in cleaning of the Wikipedia data.'
 'BERT (Ours) \nTrm Trm Trm\nTrm Trm Trm...\n...Trm Trm Trm\nTrm Trm Trm...\n...OpenAI GPT \nLstm ELMo \nLstm Lstm \nLstm Lstm Lstm Lstm Lstm Lstm \nLstm Lstm Lstm  T1 T2 TN...\n...\n......\n...\n E1 E2 EN... T1 T2TN...\n E1 E2 EN ... T1 T2 TN...\n E1 E2 EN...Figure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT\nuses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to-\nleft LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly\nconditioned on both left and right context in all layers. In addition to the architecture differences, BERT and\nOpenAI GPT are ﬁne-tuning approaches, while ELMo is a feature-based approach.\nto converge. In Section C.1 we demonstrate that\nMLM does converge marginally slower than a left-\nto-right model (which predicts every token), but\nthe empirical improvements of the MLM model\nfar outweigh the increased training cost.\nNext Sentence Prediction The next sentence\nprediction task can be illustrated in the following\nexamples.\nInput =[CLS] the man went to [MASK] store [SEP]\nhe bought a gallon [MASK] milk [SEP]\nLabel =IsNext\nInput =[CLS] the man [MASK] to the store [SEP]\npenguin [MASK] are flight ##less birds [SEP]\nLabel =NotNext\nA.2 Pre-training Procedure\nTo generate each training input sequence, we sam-\nple two spans of text from the corpus, which we\nrefer to as “sentences” even though they are typ-\nically much longer than single sentences (but can\nbe shorter also). The ﬁrst sentence receives the A\nembedding and the second receives the Bembed-\nding. 50% of the time Bis the actual next sentence\nthat follows Aand 50% of the time it is a random\nsentence, which is done for the “next sentence pre-\ndiction” task. They are sampled such that the com-\nbined length is≤512 tokens. The LM masking is\napplied after WordPiece tokenization with a uni-\nform masking rate of 15%, and no special consid-\neration given to partial word pieces.\nWe train with batch size of 256 sequences (256\nsequences * 512 tokens = 128,000 tokens/batch)\nfor 1,000,000 steps, which is approximately 40epochs over the 3.3 billion word corpus. We\nuse Adam with learning rate of 1e-4, β1= 0.9,\nβ2= 0.999, L2 weight decay of 0.01, learning\nrate warmup over the ﬁrst 10,000 steps, and linear\ndecay of the learning rate. We use a dropout prob-\nability of 0.1 on all layers. We use a gelu acti-\nvation (Hendrycks and Gimpel, 2016) rather than\nthe standard relu , following OpenAI GPT. The\ntraining loss is the sum of the mean masked LM\nlikelihood and the mean next sentence prediction\nlikelihood.\nTraining of BERT BASE was performed on 4\nCloud TPUs in Pod conﬁguration (16 TPU chips\ntotal).13Training of BERT LARGE was performed\non 16 Cloud TPUs (64 TPU chips total). Each pre-\ntraining took 4 days to complete.\nLonger sequences are disproportionately expen-\nsive because attention is quadratic to the sequence\nlength. To speed up pretraing in our experiments,\nwe pre-train the model with sequence length of\n128 for 90% of the steps. Then, we train the rest\n10% of the steps of sequence of 512 to learn the\npositional embeddings.\nA.3 Fine-tuning Procedure\nFor ﬁne-tuning, most model hyperparameters are\nthe same as in pre-training, with the exception of\nthe batch size, learning rate, and number of train-\ning epochs. The dropout probability was always\nkept at 0.1. The optimal hyperparameter values\nare task-speciﬁc, but we found the following range\nof possible values to work well across all tasks:\n•Batch size : 16, 32\n13https://cloudplatform.googleblog.com/2018/06/Cloud-\nTPU-now-offers-preemptible-pricing-and-global-\navailability.html'
 'BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout }@google.com\nAbstract\nWe introduce a new language representa-\ntion model called BERT , which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be ﬁne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeciﬁc architecture modiﬁcations.\nBERT is conceptually simple and empirically\npowerful. It obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n1 Introduction\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce ﬁne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).There are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based andﬁne-tuning . The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speciﬁc architectures that\ninclude the pre-trained representations as addi-\ntional features. The ﬁne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speciﬁc parameters, and is trained on the\ndownstream tasks by simply ﬁne-tuning allpre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the ﬁne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying ﬁne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\nIn this paper, we improve the ﬁne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations from Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a “masked lan-\nguage model” (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the maskedarXiv:1810.04805v2  [cs.CL]  24 May 2019']","In MLM, a percentage of input tokens are masked randomly, and the model predicts these masked tokens based on their context.",0.999999999975,0.8888888888888888,0.866095539204797,1.0,0.5709589845857295,0.8992205537275333
Discuss the impact of model size on BERT's performance across different tasks.,"Increasing the model size in BERT leads to improved performance on both large-scale tasks like machine translation and smaller tasks, as demonstrated by lower LM perplexity and higher accuracy on various benchmarks. The results indicate that larger models, such as BERT LARGE, outperform BERT BASE across tasks like MNLI and SST-2, with substantial differences in F1 scores for Named Entity Recognition (NER). However, the effectiveness of scaling may vary depending on the specific task and the amount of downstream task data available.","['is (L=64, H=512, A=2) with 235M parameters\n(Al-Rfou et al., 2018). By contrast, BERT BASE\ncontains 110M parameters and BERT LARGE con-\ntains 340M parameters.\nIt has long been known that increasing the\nmodel size will lead to continual improvements\non large-scale tasks such as machine translation\nand language modeling, which is demonstrated\nby the LM perplexity of held-out training data\nshown in Table 6. However, we believe that\nthis is the ﬁrst work to demonstrate convinc-\ningly that scaling to extreme model sizes also\nleads to large improvements on very small scale\ntasks, provided that the model has been sufﬁ-\nciently pre-trained. Peters et al. (2018b) presented'
 'mixed results on the downstream task impact of\nincreasing the pre-trained bi-LM size from two\nto four layers and Melamud et al. (2016) men-\ntioned in passing that increasing hidden dimen-\nsion size from 200 to 600 helped, but increasing\nfurther to 1,000 did not bring further improve-\nments. Both of these prior works used a feature-\nbased approach — we hypothesize that when the\nmodel is ﬁne-tuned directly on the downstream\ntasks and uses only a very small number of ran-\ndomly initialized additional parameters, the task-\nspeciﬁc models can beneﬁt from the larger, more\nexpressive pre-trained representations even when\ndownstream task data is very small.\n5.3 Feature-based Approach with BERT\nAll of the BERT results presented so far have used\nthe ﬁne-tuning approach, where a simple classiﬁ-\ncation layer is added to the pre-trained model, and\nall parameters are jointly ﬁne-tuned on a down-\nstream task. However, the feature-based approach,\nwhere ﬁxed features are extracted from the pre-\ntrained model, has certain advantages. First, not\nall tasks can be easily represented by a Trans-\nformer encoder architecture, and therefore require\na task-speciﬁc model architecture to be added.\nSecond, there are major computational beneﬁts\nto pre-compute an expensive representation of the\ntraining data once and then run many experiments\nwith cheaper models on top of this representation.\nIn this section, we compare the two approaches\nby applying BERT to the CoNLL-2003 Named\nEntity Recognition (NER) task (Tjong Kim Sang\nand De Meulder, 2003). In the input to BERT, we\nuse a case-preserving WordPiece model, and we\ninclude the maximal document context provided\nby the data. Following standard practice, we for-\nmulate this as a tagging task but do not use a CRF\nHyperparams Dev Set Accuracy\n#L #H #A LM (ppl) MNLI-m MRPC SST-2\n3 768 12 5.84 77.9 79.8 88.4\n6 768 3 5.24 80.6 82.2 90.7\n6 768 12 4.68 81.9 84.8 91.3\n12 768 12 3.99 84.4 86.7 92.9\n12 1024 16 3.54 85.7 86.9 93.3\n24 1024 16 3.23 86.6 87.8 93.7\nTable 6: Ablation over BERT model size. #L = the\nnumber of layers; #H = hidden size; #A = number of at-\ntention heads. “LM (ppl)” is the masked LM perplexity\nof held-out training data.System Dev F1 Test F1\nELMo (Peters et al., 2018a) 95.7 92.2\nCVT (Clark et al., 2018) - 92.6\nCSE (Akbik et al., 2018) - 93.1\nFine-tuning approach\nBERT LARGE 96.6 92.8\nBERT BASE 96.4 92.4\nFeature-based approach (BERT BASE)\nEmbeddings 91.0 -\nSecond-to-Last Hidden 95.6 -\nLast Hidden 94.9 -\nWeighted Sum Last Four Hidden 95.9 -\nConcat Last Four Hidden 96.1 -\nWeighted Sum All 12 Layers 95.5 -\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\nlayer in the output. We use the representation of\nthe ﬁrst sub-token as the input to the token-level\nclassiﬁer over the NER label set.\nTo ablate the ﬁne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without ﬁne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classiﬁcation layer.\nResults are presented in Table 7. BERT LARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind ﬁne-tuning the entire model. This\ndemonstrates that BERT is effective for both ﬁne-\ntuning and feature-based approaches.\n6 Conclusion\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to beneﬁt from deep unidirectional architec-\ntures. Our major contribution is further general-'
 'by traditional single-GPU-per-model training by implement-\ning model parallelism with only a few modiﬁcations to\nthe existing PyTorch transformer implementations. We ef-\nﬁciently trained transformer based models up to 8.3 bil-\nlion parameter on 512 NVIDIA V100 GPUs with 8-way\nmodel parallelism and achieved up to 15.1 PetaFLOPs sus-\ntained over the entire application. We also showed that for\nBERT models, careful attention to the placement of layer\nnormalization in BERT-like models is critical to achieving\nincreased accuracies as the model size increases. We study\nthe effect of model size on down-stream task accuracy and\nachieve far superior results on downstream tasks and estab-\nlish new SOTA for WikiText103, LAMBADA, and RACE\ndatasets. Finally, we open sourced our code to enable future\nwork leveraging model parallel transformers.\nThere are several directions for future work. Continuing\nto increase the scale of pretraining is a promising line of'
 'Figure 5 shows scaling values for both model and\nmodel+data parallelism. We observe excellent scaling num-\nbers in both settings. For example, the 8.3 billion parame-\nters case with 8-way (8 GPU) model parallelism achieves\n77% of linear scaling. Model+data parallelism requires fur-\nther communication of gradients and as a result the scaling\nnumbers drop slightly. However, even for the largest conﬁg-\nuration (8.3 billion parameters) running on 512 GPUs, we\nachieve 74% scaling relative to linear scaling of the strong\nsingle GPU baseline conﬁguration (1.2 billion parameters).\nFurther scaling analysis is provided in Appendix D\n5.2. Language Modeling Results Using GPT-2\nTo demonstrate that large language models can further ad-\nvance the state of the art, we consider training GPT-2 models\nof the sizes and conﬁgurations listed in Table 2. The 355M\nmodel is equivalent in size and conﬁguration of BERT-Large\nmodel (Devlin et al., 2018). The 2.5B model is bigger than\nthe previous largest GPT-2 model, and the 8.3B model is\nlarger than any left-to-right transformer language model\never trained, to the best of our knowledge. To train and eval-']","Larger BERT models consistently show better performance across tasks, indicating the importance of model size in achieving high accuracy.",0.999999999975,1.0,0.9547520696874955,1.0,0.4625966940583812,0.9273098531566016
What are the hyperparameters of the AdamW optimizer used in training the LLaMA models?,"The AdamW optimizer used in training the LLaMA models has the following hyperparameters: β1 = 0.9, β2 = 0.95, a weight decay of 0.1, and gradient clipping of 1.0. Additionally, a cosine learning rate schedule is employed, where the final learning rate is set to 10% of the maximum learning rate.","['params dimension nheadsnlayers learning rate batch size ntokens\n6.7B 4096 32 32 3.0e−44M 1.0T\n13.0B 5120 40 40 3.0e−44M 1.0T\n32.5B 6656 52 60 1.5e−44M 1.4T\n65.2B 8192 64 80 1.5e−44M 1.4T\nTable 2: Model sizes, architectures, and optimization hyper-parameters.\nOverall, our entire training dataset contains\nroughly 1.4T tokens after tokenization. For most of\nour training data, each token is used only once dur-\ning training, with the exception of the Wikipedia\nand Books domains, over which we perform ap-\nproximately two epochs.\n2.2 Architecture\nFollowing recent work on large language models,\nour network is based on the transformer architec-\nture (Vaswani et al., 2017). We leverage various\nimprovements that were subsequently proposed,\nand used in different models such as PaLM. Here\nare the main difference with the original architec-\nture, and where we were found the inspiration for\nthis change (in bracket):\nPre-normalization [GPT3]. To improve the\ntraining stability, we normalize the input of each\ntransformer sub-layer, instead of normalizing the\noutput. We use the RMSNorm normalizing func-\ntion, introduced by Zhang and Sennrich (2019).\nSwiGLU activation function [PaLM]. We re-\nplace the ReLU non-linearity by the SwiGLU ac-\ntivation function, introduced by Shazeer (2020) to\nimprove the performance. We use a dimension of\n2\n34dinstead of 4das in PaLM.\nRotary Embeddings [GPTNeo]. We remove the\nabsolute positional embeddings, and instead, add\nrotary positional embeddings (RoPE), introduced\nby Su et al. (2021), at each layer of the network.\nThe details of the hyper-parameters for our dif-\nferent models are given in Table 2.\n2.3 Optimizer\nOur models are trained using the AdamW opti-\nmizer (Loshchilov and Hutter, 2017), with the fol-\nlowing hyper-parameters: β1= 0.9,β2= 0.95.\nWe use a cosine learning rate schedule, such that\nthe ﬁnal learning rate is equal to 10% of the maxi-\nmal learning rate. We use a weight decay of 0.1and\ngradient clipping of 1.0. We use 2,000warmup\n0 200 400 600 800 1000 1200 1400\nBillion of tokens1.51.61.71.81.92.02.12.2Training lossLLaMA 7B\nLLaMA 13B\nLLaMA 33B\nLLaMA 65BFigure 1: Training loss over train tokens for the 7B,\n13B, 33B, and 65 models. LLaMA-33B and LLaMA-\n65B were trained on 1.4T tokens. The smaller models\nwere trained on 1.0T tokens. All models are trained\nwith a batch size of 4M tokens.\nsteps, and vary the learning rate and batch size with\nthe size of the model (see Table 2 for details).\n2.4 Efﬁcient implementation\nWe make several optimizations to improve the train-\ning speed of our models. First, we use an efﬁcient\nimplementation of the causal multi-head attention\nto reduce memory usage and runtime. This imple-\nmentation, available in the xformers library,2is\ninspired by Rabe and Staats (2021) and uses the\nbackward from Dao et al. (2022). This is achieved\nby not storing the attention weights and not com-\nputing the key/query scores that are masked due to\nthe causal nature of the language modeling task.\nTo further improve training efﬁciency, we re-\nduced the amount of activations that are recom-\nputed during the backward pass with checkpoint-\ning. More precisely, we save the activations that\nare expensive to compute, such as the outputs of\nlinear layers. This is achieved by manually imple-\nmenting the backward function for the transformer\nlayers, instead of relying on the PyTorch autograd.\nTo fully beneﬁt from this optimization, we need to\n2https://github.com/facebookresearch/xformers'
 'LLaMA: Open and Efﬁcient Foundation Language Models\nHugo Touvron∗, Thibaut Lavril∗, Gautier Izacard∗, Xavier Martinet\nMarie-Anne Lachaux, Timothee Lacroix, Baptiste Rozière, Naman Goyal\nEric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin\nEdouard Grave∗, Guillaume Lample∗\nMeta AI\nAbstract\nWe introduce LLaMA, a collection of founda-\ntion language models ranging from 7B to 65B\nparameters. We train our models on trillions\nof tokens, and show that it is possible to train\nstate-of-the-art models using publicly avail-\nable datasets exclusively, without resorting\nto proprietary and inaccessible datasets. In\nparticular, LLaMA-13B outperforms GPT-3\n(175B) on most benchmarks, and LLaMA-\n65B is competitive with the best models,\nChinchilla-70B and PaLM-540B. We release\nall our models to the research community1.\n1 Introduction\nLarge Languages Models (LLMs) trained on mas-\nsive corpora of texts have shown their ability to per-\nform new tasks from textual instructions or from a\nfew examples (Brown et al., 2020). These few-shot\nproperties ﬁrst appeared when scaling models to a\nsufﬁcient size (Kaplan et al., 2020), resulting in a\nline of work that focuses on further scaling these\nmodels (Chowdhery et al., 2022; Rae et al., 2021).\nThese efforts are based on the assumption that\nmore parameters will lead to better performance.\nHowever, recent work from Hoffmann et al. (2022)\nshows that, for a given compute budget, the best\nperformances are not achieved by the largest mod-\nels, but by smaller models trained on more data.\nThe objective of the scaling laws from Hoff-\nmann et al. (2022) is to determine how to best\nscale the dataset and model sizes for a particular\ntraining compute budget. However, this objective\ndisregards the inference budget, which becomes\ncritical when serving a language model at scale.\nIn this context, given a target level of performance,\nthe preferred model is not the fastest to train but the\nfastest at inference, and although it may be cheaper\nto train a large model to reach a certain level of\n∗Equal contribution. Correspondence: {htouvron,\nthibautlav,gizacard,egrave,glample}@meta.com\n1https://github.com/facebookresearch/llamaperformance, a smaller one trained longer will\nultimately be cheaper at inference. For instance,\nalthough Hoffmann et al. (2022) recommends\ntraining a 10B model on 200B tokens, we ﬁnd\nthat the performance of a 7B model continues to\nimprove even after 1T tokens.\nThe focus of this work is to train a series of\nlanguage models that achieve the best possible per-\nformance at various inference budgets, by training\non more tokens than what is typically used. The\nresulting models, called LLaMA , ranges from 7B\nto 65B parameters with competitive performance\ncompared to the best existing LLMs. For instance,\nLLaMA-13B outperforms GPT-3 on most bench-\nmarks, despite being 10 ×smaller. We believe that\nthis model will help democratize the access and\nstudy of LLMs, since it can be run on a single GPU.\nAt the higher-end of the scale, our 65B-parameter\nmodel is also competitive with the best large lan-\nguage models such as Chinchilla or PaLM-540B.\nUnlike Chinchilla, PaLM, or GPT-3, we only\nuse publicly available data, making our work com-\npatible with open-sourcing, while most existing\nmodels rely on data which is either not publicly\navailable or undocumented (e.g. “Books – 2TB” or\n“Social media conversations”). There exist some\nexceptions, notably OPT (Zhang et al., 2022),\nGPT-NeoX (Black et al., 2022), BLOOM (Scao\net al., 2022) and GLM (Zeng et al., 2022), but none\nthat are competitive with PaLM-62B or Chinchilla.\nIn the rest of this paper, we present an overview\nof the modiﬁcations we made to the transformer\narchitecture (Vaswani et al., 2017), as well as our\ntraining method. We then report the performance of\nour models and compare with others LLMs on a set\nof standard benchmarks. Finally, we expose some\nof the biases and toxicity encoded in our models,\nusing some of the most recent benchmarks from'
 'GPU TypeGPU PowerGPU-hoursTotal power Carbon emitted\nconsumption consumption (tCO 2eq)\nOPT-175B A100-80GB 400W 809,472 356 MWh 137\nBLOOM-175B A100-80GB 400W 1,082,880 475 MWh 183\nLLaMA-7B A100-80GB 400W 82,432 36 MWh 14\nLLaMA-13B A100-80GB 400W 135,168 59 MWh 23\nLLaMA-33B A100-80GB 400W 530,432 233 MWh 90\nLLaMA-65B A100-80GB 400W 1,022,362 449 MWh 173\nTable 15: Carbon footprint of training different models in the same data center. We follow Wu et al. (2022)\nto compute carbon emission of training OPT, BLOOM and our models in the same data center. For the power\nconsumption of a A100-80GB, we take the thermal design power for NVLink systems, that is 400W. We take a\nPUE of 1.1 and a carbon intensity factor set at the national US average of 0.385 kg CO 2e per KWh.\nstarting from feed forward models (Bengio et al.,\n2000), recurrent neural networks (Elman, 1990;\nMikolov et al., 2010) and LSTMs (Hochreiter and\nSchmidhuber, 1997; Graves, 2013). More recently,\ntransformer networks, based on self-attention, have\nled to important improvements, especially for cap-\nturing long range dependencies (Vaswani et al.,\n2017; Radford et al., 2018; Dai et al., 2019).\nScaling. There is a long history of scaling for\nlanguage models, for both the model and dataset\nsizes. Brants et al. (2007) showed the beneﬁts of\nusing language models trained on 2 trillion tokens,\nresulting in 300 billion n-grams, on the quality of\nmachine translation. While this work relied on a\nsimple smoothing technique, called Stupid Backoff ,\nHeaﬁeld et al. (2013) later showed how to scale\nKneser-Ney smoothing to Web-scale data. This\nallowed to train a 5-gram model on 975 billions to-\nkens from CommonCrawl, resulting in a model\nwith 500 billions n-grams (Buck et al., 2014).\nChelba et al. (2013) introduced the One Billion\nWord benchmark, a large scale training dataset to\nmeasure the progress of language models.\nIn the context of neural language models, Joze-\nfowicz et al. (2016) obtained state-of-the-art re-\nsults on the Billion Word benchmark by scaling\nLSTMs to 1 billion parameters. Later, scaling\ntransformers lead to improvement on many NLP\ntasks. Notable models include BERT (Devlin et al.,\n2018), GPT-2 (Radford et al., 2019), Megatron-\nLM (Shoeybi et al., 2019), and T5 (Raffel et al.,\n2020). A signiﬁcant breakthrough was obtained\nwith GPT-3 (Brown et al., 2020), a model with\n175 billion parameters. This lead to a series of\nLarge Language Models , such as Jurassic-1 (Lieber\net al., 2021), Megatron-Turing NLG (Smith et al.,2022), Gopher (Rae et al., 2021), Chinchilla (Hoff-\nmann et al., 2022), PaLM (Chowdhery et al., 2022),\nOPT (Zhang et al., 2022), and GLM (Zeng et al.,\n2022). Hestness et al. (2017) and Rosenfeld et al.\n(2019) studied the impact of scaling on the perfor-\nmance of deep learning models, showing the exis-\ntence of power laws between the model and dataset\nsizes and the performance of the system. Kaplan\net al. (2020) derived power laws speciﬁcally for\ntransformer based language models, which were\nlater reﬁned by Hoffmann et al. (2022), by adapting\nthe learning rate schedule when scaling datasets.\nFinally, Wei et al. (2022) studied the effect of scal-\ning on the abilities of large language models.\n8 Conclusion\nIn this paper, we presented a series of language\nmodels that are released openly, and competitive\nwith state-of-the-art foundation models. Most\nnotably, LLaMA-13B outperforms GPT-3 while\nbeing more than 10 ×smaller, and LLaMA-65B is\ncompetitive with Chinchilla-70B and PaLM-540B.\nUnlike previous studies, we show that it is possible\nto achieve state-of-the-art performance by training\nexclusively on publicly available data, without\nresorting to proprietary datasets. We hope that\nreleasing these models to the research community\nwill accelerate the development of large language\nmodels, and help efforts to improve their robust-\nness and mitigate known issues such as toxicity and\nbias. Additionally, we observed like Chung et al.'
 'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nTable 7. Effect of number of attention heads on scaling on 8.3\nbillion of parameters with 8-way model parallelism.\nAttention heads Hidden size per head Scaling Efﬁciency\n16 192 82%\n24 128 80%\n32 96 77%\nTable 8. Speedup obtained for the 1.2 billion parameters model\nusing model parallelism while keeping the batch size constant.\n# of GPUs 1 2 4 8\nSpeedup 1.0 1.64 2.34 2.98\nsingle GPU, but it can also accelerate the training of smaller\nmodels without increasing the batch size. To measure this\nacceleration we train a model with a ﬁxed 1.2 billion parame-\nters. We use a ﬁxed batch size of 8 samples per iteration and\nincrease the number of GPUs using model parallelism. The\nresults are listed in Table 8. Using two GPUs makes training\n64% faster. Above that we see diminishing returns as the\nper-GPU computation decreases and the memory bandwidth\nand communication overheads begin to dominate.\nE. Evaluating Language Models Using\nWikiText103 and LAMBADA\nIn this section we detail our evaluation methodology for the\nWikiText103 dataset (Merity et al., 2016) and cloze-style\nprediction accuracy on the LAMBADA dataset(Paperno\net al., 2016).\nE.1. Wikitext103 Perplexity\nWikiText103 perplexity is an evaluation criterion that has\nbeen well studied over the past few years since the creation\nof the benchmark dataset. Perplexity is the exponentiation\nof the average cross entropy of a corpus (Mikolov et al.,\n2011). This makes it a natural evaluation metric for lan-\nguage models which represent a probability distribution\nover entire sentences or texts.\nPPL = exp(−1\nToT∑\ntlogP(t|0 :t−1)) (4)\nTo calculate perplexity in (4) we tokenize the WikiText103\ntest corpus according to our subword vocabulary and sum\nthe cross entropy loss from each token [0,T]. We then nor-\nmalize the cross entropy loss by the number of tokens in the\noriginal tokenization scheme To. The WikiText103 test cor-\npus already comes pre-tokenized with word level tokens that\nprior works have used to compute perplexity. To evaluate\nour models’ perplexities on a level playing ﬁeld with priorworks we must normalize by the original number of tokens,\nTo, rather than the number of tokens, T, actually in the tok-\nenized data fed as input to our model. This pre-tokenization\nalso introduces artifacts in the text that are not present in our\ntraining data. To alleviate this distributional mismatch, we\nﬁrst preprocess the WikiText103 test dataset with invertible\ndetokenizers to remove various artifacts related to punctua-\ntion and whitespace. The value of Tois calculated before\nthis preprocessing. For WikiText103’s test set To= 245566\nandT= 270329 .\nWe must also make one further transformer-speciﬁc mod-\niﬁcation to the perplexity calculation. Unlike RNN-based\nlanguage models, transformers operate on a ﬁxed window in-\nput size. Therefore they cannot fully calculate P(t|0 :t−1)\nand can only calculate P(t|t−w:t−1)wherewis the\nsize of our context: 1024 tokens. However, calculating this\nvalue for every token in our dataset is prohibitively expen-\nsive since we must compute approximately Tevaluations\nof awsized context. To evaluate our models efﬁciently we\ntake a middle ground approach termed overlapping evalu-\nation where we advance the sliding window by some over-\nlapoeach time and only compute the cross entropy losses\ncorresponding to the last otokens of the window. In our\nexperiments we utilize an overlap oof 32, and compute\nlosses over all sliding windows in such a fashion.\nE.2. LAMBADA Cloze Accuracy\nThe capability to handle long term contexts is crucial for\nstate of the art language models and is a necessary prerequi-\nsite for problems like long-form generation and document-\nbased question answering. Cloze-style datasets like LAM-\nBADA are designed to measure a model’s ability to operate\nin and reason about these types of long term contexts. Cloze-\nstyle reading comprehension uses a context of word tokens']","The AdamW optimizer for LLaMA models uses hyperparameters 棺1 = 0.9, 棺2 = 0.95, with a cosine learning rate schedule, a weight decay of 0.1, and gradient clipping set to 1.0.",0.9999999999,1.0,0.9940768624671383,1.0,0.9319646246440669,0.9586277293454987
"In what ways does LLaMA's evaluation strategy extend beyond standard NLP tasks, and what new dimensions of model performance does this reveal?","LLaMA's evaluation strategy extends beyond standard NLP tasks by introducing a comprehensive test that covers 57 subjects at varying levels of difficulty, assessing both breadth and depth of language understanding. It reveals important dimensions of model performance, including uncalibrated outputs, challenges in tasks requiring calculations, and especially poor performance on socially relevant subjects like morality and law. This expansive evaluation helps researchers identify significant shortcomings in state-of-the-art models.","['parameter language models, data may also become a bottleneck, as there is far less written about\nesoteric branches of knowledge than about everyday situations.\n6 C ONCLUSION\nWe introduced a new test that measures how well text models can learn and apply knowledge\nencountered during pretraining. By covering 57 subjects at varying levels of difﬁculty, the test\nassesses language understanding in greater breadth and depth than previous benchmarks. We found\nthat it has recently become possible for models to make meaningful progress on the test, but that\nstate-of-the-art models have lopsided performance and rarely excel at any individual task. We also\nshowed that current models are uncalibrated and have difﬁculty with tasks that require calculations.\nWorryingly, models also perform especially poorly on socially relevant subjects including morality\nand law. Our expansive test can help researchers pinpoint important shortcomings of models, making\nit easier to gain a clearer picture of state-of-the-art capabilities.\n8'
 'HellaSwag (Zellers et al., 2019), WinoGrande (Sak-\naguchi et al., 2021), ARC easy and challenge (Clark\net al., 2018) and OpenBookQA (Mihaylov et al.,\n2018). These datasets include Cloze and Winograd\nstyle tasks, as well as multiple choice question an-\nswering. We evaluate in the zero-shot setting as\ndone in the language modeling community.\nIn Table 3, we compare with existing models\nof various sizes and report numbers from the cor-\nresponding papers. First, LLaMA-65B outper-\nforms Chinchilla-70B on all reported benchmarks\nbut BoolQ. Similarly, this model surpasses PaLM-\n540B everywhere but on BoolQ and WinoGrande.\nLLaMA-13B model also outperforms GPT-3 on\nmost benchmarks despite being 10 ×smaller.\n3.2 Closed-book Question Answering\nWe compare LLaMA to existing large language\nmodels on two closed-book question answering\nbenchmarks: Natural Questions (Kwiatkowski\net al., 2019) and TriviaQA (Joshi et al., 2017). For\nboth benchmarks, we report exact match perfor-\nmance in a closed book setting, i.e., where the mod-\nels do not have access to documents that contain\nevidence to answer the question. In Table 4, we\nreport performance on NaturalQuestions, and in Ta-\nble 5, we report on TriviaQA. On both benchmarks,\nLLaMA-65B achieve state-of-the-arts performance\nin the zero-shot and few-shot settings. More im-\nportantly, the LLaMA-13B is also competitive on\nthese benchmarks with GPT-3 and Chinchilla, de-\nspite being 5-10 ×smaller. This model runs on a\nsingle V100 GPU during inference.\n0-shot 1-shot 5-shot 64-shot\nGopher 280B 43.5 - 57.0 57.2\nChinchilla 70B 55.4 - 64.1 64.6\nLLaMA7B 50.0 53.4 56.3 57.6\n13B 56.6 60.5 63.1 64.0\n33B 65.1 67.9 69.9 70.4\n65B 68.2 71.6 72.6 73.0\nTable 5: TriviaQA. Zero-shot and few-shot exact\nmatch performance on the ﬁltered dev set.\n3.3 Reading Comprehension\nWe evaluate our models on the RACE reading com-\nprehension benchmark (Lai et al., 2017). This\ndataset was collected from English reading com-\nprehension exams designed for middle and highRACE-middle RACE-high\nGPT-3 175B 58.4 45.5\nPaLM8B 57.9 42.3\n62B 64.3 47.5\n540B 68.1 49.1\nLLaMA7B 61.1 46.9\n13B 61.6 47.2\n33B 64.1 48.3\n65B 67.9 51.6\nTable 6: Reading Comprehension. Zero-shot accu-\nracy.\nschool Chinese students. We follow the evaluation\nsetup from Brown et al. (2020) and report results\nin Table 6. On these benchmarks, LLaMA-65B is\ncompetitive with PaLM-540B, and, LLaMA-13B\noutperforms GPT-3 by a few percents.\n3.4 Mathematical reasoning\nWe evaluate our models on two mathematical rea-\nsoning benchmarks: MATH (Hendrycks et al.,\n2021) and GSM8k (Cobbe et al., 2021). MATH\nis a dataset of 12K middle school and high school\nmathematics problems written in LaTeX. GSM8k\nis a set of middle school mathematical problems.\nIn Table 7, we compare with PaLM and Min-\nerva (Lewkowycz et al., 2022). Minerva is a series\nof PaLM models ﬁnetuned on 38.5B tokens ex-\ntracted from ArXiv and Math Web Pages, while\nneither PaLM or LLaMA are ﬁnetuned on mathe-\nmatical data. The numbers for PaLM and Minerva\nare taken from Lewkowycz et al. (2022), and we\ncompare with and without maj1@k .maj1@k de-\nnotes evaluations where we generate ksamples for\neach problem and perform a majority voting (Wang\net al., 2022). On GSM8k, we observe that LLaMA-\n65B outperforms Minerva-62B, although it has not\nbeen ﬁne-tuned on mathematical data.\n3.5 Code generation\nWe evaluate the ability of our models to write\ncode from a natural language description on two\nbenchmarks: HumanEval (Chen et al., 2021) and\nMBPP (Austin et al., 2021). For both tasks, the\nmodel receives a description of the program in a\nfew sentences, as well as a few input-output ex-\namples. In HumanEval, it also receives a function\nsignature, and the prompt is formatted as natural\ncode with the textual description and tests in a'
 'A Question Answering\nWe evaluate LLaMA on Natural Questions and TriviaQA. For Natural Questions we use the test split used\nfor open-domain question answering containing 3610 questions. For TriviaQA we evaluate on the dev set\nof the ﬁltered set. This differs from GPT-3 and PaLM, which evaluate on the test set of the unﬁltered set\nfor which the online evaluation server is not available anymore5.\nWe generate answers using greedy decoding, and extract an answer from the generation by stopping\nat the ﬁrst line break, ﬁnal dot or comma. Generated answers are evaluated with the standard exact\nmatch metric: a generated answer is considered correct if it matches any answer of the list of answers\nafter normalization. For this normalization step we lowercase generated answers and remove articles,\npunctuation and duplicate whitespaces. Figure 3 presents formatted examples in the 1-shot setting for\nNatural Questions and TriviaQA respectively. In all settings, we preprend the string Answer these\nquestions:\\n to the list of questions and answers.\nContext→Answer these questions: Context→Answer these questions:\nQ: Who sang who wants to be a millionaire in high society? Q: In Scotland a bothy/bothie is a?\nA: Frank Sinatra A: House\nQ: Who wrote the book the origin of species? Q: The ancient city of Troy is located in what modern country?\nA: A:\nTarget→Charles Darwin Target→Turkey\nFigure 3: Formatted dataset example for Natural Questions (left) & TriviaQA (right).\n5https://competitions.codalab.org/competitions/17208'
 'ence. Altogether, it contributes to the high quality of GLM-130B in terms of both language perfor-\nmance on 112 tasks and ethical results on bias and toxicity benchmarks. Our experiences of both\nsuccess and failure are condensed into the lessons for training 100B-scale LLMs, attached in the\nAppendix B.10.\n9']","LLaMA's evaluation strategy includes novel tasks that assess the model's ability in reasoning, domain-specific knowledge (e.g., legal or medical), and its proficiency in languages other than English. This approach reveals dimensions of model performance such as cross-domain adaptability, fine-grained linguistic understanding, and the capacity to generalize knowledge across different contexts and languages.",0.99999999995,1.0,0.9323832682132692,1.0,0.5109619939769945,0.9529388849988873
Describe the methodology used for evaluating models on the multitask test. How does it differ from traditional model evaluations?,"The methodology for evaluating models on the multitask test involves assessing their performance across 57 subjects at varying difficulty levels in zero-shot and few-shot settings, which reflects how humans are evaluated. This approach differs from traditional model evaluations, which often rely on large training sets and specific benchmarks that may not capture the full breadth of language understanding. By focusing on knowledge acquired during pretraining from diverse text sources, this test aims to provide a more comprehensive analysis of a model's capabilities.","['Published as a conference paper at ICLR 2021\nMEASURING MASSIVE MULTITASK\nLANGUAGE UNDERSTANDING\nDan Hendrycks\nUC BerkeleyCollin Burns\nColumbia UniversitySteven Basart\nUChicagoAndy Zou\nUC Berkeley\nMantas Mazeika\nUIUCDawn Song\nUC BerkeleyJacob Steinhardt\nUC Berkeley\nABSTRACT\nWe propose a new test to measure a text model’s multitask accuracy. The test\ncovers 57 tasks including elementary mathematics, US history, computer science,\nlaw, and more. To attain high accuracy on this test, models must possess extensive\nworld knowledge and problem solving ability. We ﬁnd that while most recent\nmodels have near random-chance accuracy, the very largest GPT-3 model improves\nover random chance by almost 20 percentage points on average. However, on every\none of the 57 tasks, the best models still need substantial improvements before\nthey can reach expert-level accuracy. Models also have lopsided performance\nand frequently do not know when they are wrong. Worse, they still have near-\nrandom accuracy on some socially important subjects such as morality and law.\nBy comprehensively evaluating the breadth and depth of a model’s academic and\nprofessional understanding, our test can be used to analyze models across many\ntasks and to identify important shortcomings.\n1 I NTRODUCTION\nNatural Language Processing (NLP) models have achieved superhuman performance on a number of\nrecently proposed benchmarks. However, these models are still well below human level performance\nfor language understanding as a whole, suggesting a disconnect between our benchmarks and the\nactual capabilities of these models. The General Language Understanding Evaluation benchmark\n(GLUE) (Wang et al., 2018) was introduced in 2018 to evaluate performance on a wide range of NLP\ntasks, and top models achieved superhuman performance within a year. To address the shortcomings\nof GLUE, researchers designed the SuperGLUE benchmark with more difﬁcult tasks (Wang et al.,\n2019). About a year since the release of SuperGLUE, performance is again essentially human-level\n(Raffel et al., 2019). While these benchmarks evaluate linguistic skills more than overall language\nunderstanding, an array of commonsense benchmarks have been proposed to measure basic reasoning\nand everyday knowledge (Zellers et al., 2019; Huang et al., 2019; Bisk et al., 2019). However, these\nrecent benchmarks have similarly seen rapid progress (Khashabi et al., 2020). Overall, the near\nhuman-level performance on these benchmarks suggests that they are not capturing important facets\nof language understanding.\nTransformer models have driven this recent progress by pretraining on massive text corpora, including\nall of Wikipedia, thousands of books, and numerous websites. These models consequently see\nextensive information about specialized topics, most of which is not assessed by existing NLP\nbenchmarks. It consequently remains an open question just how capable current language models are\nat learning and applying knowledge from many domains.\nTo bridge the gap between the wide-ranging knowledge that models see during pretraining and the\nexisting measures of success, we introduce a new benchmark for assessing models across a diverse\nset of subjects that humans learn. We design the benchmark to measure knowledge acquired during\npretraining by evaluating models exclusively in zero-shot and few-shot settings. This makes the\nbenchmark more challenging and more similar to how we evaluate humans. The benchmark covers\n57subjects across STEM, the humanities, the social sciences, and more. It ranges in difﬁculty from\nan elementary level to an advanced professional level, and it tests both world knowledge and problem\nsolving ability. Subjects range from traditional areas, such as mathematics and history, to more\n1arXiv:2009.03300v3  [cs.CY]  12 Jan 2021'
 'parameter language models, data may also become a bottleneck, as there is far less written about\nesoteric branches of knowledge than about everyday situations.\n6 C ONCLUSION\nWe introduced a new test that measures how well text models can learn and apply knowledge\nencountered during pretraining. By covering 57 subjects at varying levels of difﬁculty, the test\nassesses language understanding in greater breadth and depth than previous benchmarks. We found\nthat it has recently become possible for models to make meaningful progress on the test, but that\nstate-of-the-art models have lopsided performance and rarely excel at any individual task. We also\nshowed that current models are uncalibrated and have difﬁculty with tasks that require calculations.\nWorryingly, models also perform especially poorly on socially relevant subjects including morality\nand law. Our expansive test can help researchers pinpoint important shortcomings of models, making\nit easier to gain a clearer picture of state-of-the-art capabilities.\n8'
 'consisting of Amazon Mechanical Turk Human Intelligence Tasks. These are well-deﬁned tasks that\nrequire models to interact with ﬂexible formats and demonstrate multimodal understanding.\nThe Internet as a Training Set. A major distinction between our benchmark and previous multitask\nNLP benchmarks is that we do not require large training sets. Instead, we assume that models have\nacquired the requisite knowledge from reading vast quantities of diverse text from the Internet. This\n7'
 'e.g. medical notes or scientiﬁc papers.\nEvaluability: Tasks must have an automatic performance metric that corresponds well to human\njudgments of output quality. Some text generation tasks fail to meet this criteria due to issues with\nautomatic metrics like ROUGE and BLEU (Callison-Burch et al., 2006; Liu et al., 2016, i.a.).\n4']","Models were evaluated in zero-shot and few-shot settings to measure their ability to apply pretraining knowledge without further fine-tuning, a method designed to be more akin to human learning and evaluation processes.",0.99999999995,1.0,0.9480287825170205,1.0,0.8448101220962123,0.9246950338393944
What characteristic of large language model (LLM) generated text's probability function does DetectGPT exploit for detection?,"DetectGPT exploits the tendency of LLM-generated text to occupy negative curvature regions of the model’s log probability function. This property allows DetectGPT to use a curvature-based criterion to judge whether a passage is generated from a specific LLM. By evaluating log probabilities and comparing them with perturbed samples, DetectGPT can effectively detect model samples without requiring a separate classifier or training data.","['Zero-Shot Machine-Generated Text Detection using Probability Curvature\ngenerated text detection. Other work explores watermarks\nfor generated text (Kirchenbauer et al., 2023), which modify\na model’s generations to make them easier to detect. Our\nwork does not assume text is generated with the goal of easy\ndetection; DetectGPT detects text generated from publicly\navailable LLMs using standard LLM sampling strategies.\nThe widespread use of LLMs has led to much other con-\ntemporaneous work on detecting LLM output. Sadasivan\net al. (2023) show that the detection AUROC of the an de-\ntector is upper bounded by a function of the TV distance\nbetween the model and human text. However, we find that\nAUROC of DetectGPT is high even for the largest publicly-\navailable models (Table 2), suggesting that TV distance may\nnot correlate strongly with model scale and capability. This\ndisconnect may be exacerbated by new training objectives\nother than maximum likelihood, e.g., reinforcement learn-\ning with human feedback (Christiano et al., 2017; Ziegler\net al., 2020). Both Sadasivan et al. (2023) and Krishna et al.\n(2023) show the effectiveness of paraphrasing as a tool for\nevading detection, suggesting an important area of study\nfor future work. Liang et al. (2023) show that multi-lingual\ndetection is difficult, with non-DetectGPT detectors show-\ning bias against non-native speakers; this result highlights\nthe advantage of zero-shot detectors like DetectGPT, which\ngeneralize well to any data generated by the original gener-\nating model. Mireshghallah et al. (2023) study which proxy\nscoring models produce the most useful log probabilities\nfor detection when the generating model is not known (a\nlarge-scale version of our Figure 6). Surprisingly (but con-\nsistent with our findings), they find that smaller models are\nin fact better proxy models for performing detection with\nperturbation-based methods like DetectGPT.\nThe problem of machine-generated text detection echoes ear-\nlier work on detecting deepfakes, artificial images or videos\ngenerated by deep nets, which has spawned substantial ef-\nforts in detection of fake visual content (Dolhansky et al.,\n2020; Zi et al., 2020). While early works in deepfake de-\ntection used relatively general-purpose model architectures\n(G¨uera & Delp, 2018), many deepfake detection methods\nrely on the continuous nature of image data to achieve state-\nof-the-art performance (Zhao et al., 2021; Guarnera et al.,\n2020), making direct application to text difficult.\n3. The Zero-Shot Machine-Generated Text\nDetection Problem\nWe study zero-shot machine-generated text detection, the\nproblem of detecting whether a piece of text, or candidate\npassage x, is a sample from a source model pθ. The problem\nis zero-shot in the sense that we do not assume access to\nhuman-written or generated samples to perform detection.\nAs in prior work, we study a ‘white box’ setting (Gehrmann\net al., 2019) in which the detector may evaluate the log prob-Algorithm 1 DetectGPT model-generated text detection\n1:Input: passage x, source model pθ, perturbation function q,\nnumber of perturbations k, decision threshold ϵ\n2:˜xi∼q(· |x), i∈[1..k]// mask spans, sample replacements\n3:˜µ←1\nkP\nilogpθ(˜xi)// approximate expectation in Eq. 1\n4:ˆdx←logpθ(x)−˜µ // estimate d(x, pθ, q)\n5:˜σ2\nx←1\nk−1P\ni(logpθ(˜xi)−˜µ)2// variance for normalization\n6:ifˆdx√˜σx> ϵthen\n7: return true // probably model sample\n8:else\n9: return false // probably not model sample\nability of a sample logpθ(x). The white box setting does\nnotassume access to the model architecture or parameters.\nMost public APIs for LLMs (such as GPT-3) enable scoring\ntext, though some exceptions exist, notably ChatGPT. While\nmost of our experiments consider the white box setting, see\nSection 5.2 for experiments in which we score text using\nmodels other than the source model. See Mireshghallah\net al. (2023) for a comprehensive evaluation in this setting.\nThe detection criterion we propose, DetectGPT, also makes'
 'DetectGPT: Zero-Shot Machine-Generated Text Detection\nusing Probability Curvature\nEric Mitchell1Yoonho Lee1Alexander Khazatsky1Christopher D. Manning1Chelsea Finn1\nAbstract\nThe increasing fluency and widespread usage of\nlarge language models (LLMs) highlight the de-\nsirability of corresponding tools aiding detection\nof LLM-generated text. In this paper, we identify\na property of the structure of an LLM’s proba-\nbility function that is useful for such detection.\nSpecifically, we demonstrate that text sampled\nfrom an LLM tends to occupy negative curva-\nture regions of the model’s log probability func-\ntion. Leveraging this observation, we then define\na new curvature-based criterion for judging if a\npassage is generated from a given LLM. This\napproach, which we call DetectGPT, does not re-\nquire training a separate classifier, collecting a\ndataset of real or generated passages, or explic-\nitly watermarking generated text. It uses only\nlog probabilities computed by the model of in-\nterest and random perturbations of the passage\nfrom another generic pre-trained language model\n(e.g., T5). We find DetectGPT is more discrimi-\nnative than existing zero-shot methods for model\nsample detection, notably improving detection of\nfake news articles generated by 20B parameter\nGPT-NeoX from 0.81 AUROC for the strongest\nzero-shot baseline to 0.95 AUROC for Detect-\nGPT. See ericmitchell.ai/detectgpt\nfor code, data, and other project information.\n1. Introduction\nLarge language models (LLMs) have proven able to gen-\nerate remarkably fluent responses to a wide variety of user\nqueries. Models such as GPT-3 (Brown et al., 2020), PaLM\n(Chowdhery et al., 2022), and ChatGPT (OpenAI, 2022)\ncan convincingly answer complex questions about science,\nmathematics, historical and current events, and social trends.\n1Stanford University. Correspondence to: Eric Mitchell\n<eric.mitchell@cs.stanford.edu >.\nProceedings of the 40thInternational Conference on Machine\nLearning , Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).\n Candidate passage : “Joe Biden recently made a move to the White House that included bringing along his pet German Shepherd…”\nDetectGPT\nx\n...GPT-3(1)Perturb(2) Score(3) Compare\n🤖  from GPT-3\nYes(reword with T5)\n“made a move”  “moved”→“pet”  “dog”→Delete “bringing along”\n...\n🤔  from other source\nNo\nFigure 1. We aim to determine whether a piece of text was gener-\nated by a particular LLM p, such as GPT-3. To classify a candidate\npassage x, DetectGPT first generates minor perturbations of the\npassage ˜xiusing a generic pre-trained model such as T5. Then\nDetectGPT compares the log probability under pof the original\nsample xwith each perturbed sample ˜xi. If the average log ratio\nis high, the sample is likely from the source model.\nWhile recent work has found that cogent-sounding LLM-\ngenerated responses are often simply wrong (Lin et al.,\n2022), the articulate nature of such generated text may still\nmake LLMs attractive for replacing human labor in some\ncontexts, notably student essay writing and journalism. At\nleast one major news source has released AI-written content\nwith limited human review, leading to substantial factual er-\nrors in some articles (Christian, 2023). Such applications of\nLLMs are problematic for a variety of reasons, making fair\nstudent assessment difficult, impairing student learning, and\nproliferating convincing-but-inaccurate news articles. Un-\nfortunately, humans perform only slightly better than chance\nwhen classifying machine-generated vs human-written text\n(Gehrmann et al., 2019), leading researchers to consider\nautomated detection methods that may identify signals dif-\nficult for humans to recognize. Such methods might give\nteachers and news-readers more confidence in the human\norigin of the text that they consume.\nAs in prior work (Jawahar et al., 2020), we study the\n1arXiv:2301.11305v2  [cs.CL]  23 Jul 2023'
 'Zero-Shot Machine-Generated Text Detection using Probability Curvature\n0.0 0.1 0.20204060XSum\n0.1\n 0.0 0.1 0.2WritingPrompts\n0.0 0.1 0.20204060SQuAD\nHuman\nModel\n0.1\n 0.0 0.1 0.2 0.3PubMed\n0.0 0.2 0.4 0.6 0.8 1.0\nLog Probability Change (Perturbation Discrepancy)0.00.20.40.60.81.0Frequency\nFigure 9. Perturbation discrepancy distributions for GPT-Neo\n(2.7B) and humans across domains. A threshold of 0.1 gener-\nally separates model- and human-generated text well, which is\nimportant for practical scenarios where the domain is unknown.\nidentify a property of the log probability function computed\nby a wide variety of large language models, showing that a\ntractable approximation to the trace of the Hessian of the\nmodel’s log probability function provides a useful signal\nfor detecting model samples. Our experiments find that\nthis signal is more discriminative than existing zero-shot\ndetection methods and is competitive with bespoke detection\nmodels trained with millions of model samples.\nDetectGPT and Watermarking. One interpretation of\nthe perturbation function is producing semantically similar\nrephrasings of the original passage . If these rephrasings\nare systematically lower-probability than the original pas-\nsage, the model is exposing its bias toward the specific (and\nroughly arbitrary, by human standards) phrasing used. In\nother words, LLMs that do not perfectly imitate human\nwriting essentially watermark themselves implicitly. Under\nthis interpretation, efforts to manually add watermarking bi-\nases to model outputs (Aaronson, 2022; Kirchenbauer et al.,\n2023) may further improve the effectiveness of methods\nsuch as DetectGPT, even as LLMs continue to improve.\nLimitations. One limitation of probability-based methods\nfor zero-shot machine-generated text detection (like Detect-\nGPT) is the white-box assumption that we can evaluate log\nprobabilities of the model(s) in question. For models be-\nhind APIs that do provide probabilities (such as GPT-3),\nevaluating probabilities nonetheless costs money. Another\nassumption of DetectGPT is access to a reasonable pertur-\nbation function. While in this work, we use off-the-shelf\nmask-filling models such as T5 and mT5 (for non-English\nlanguages), some domains may see reduced performance\nif existing mask-filling models do not well represent the\nspace of meaningful rephrases, reducing the quality of the\ncurvature estimate. While DetectGPT provides the best\navailable detection performance for PubMedQA, its drop\nin performance compared to other datasets may be a result\nAverage length0.9850.9900.995AUROC\ngpt-2\nAverage length0.960.970.980.99AUROC\nopt-2.7\nXSum\nSQuAD\nWritingPrompts\n130 140 150 160 170\nAverage length0.8750.9000.9250.9500.975AUROC\nEleutherAI/gpt-j-6b\n130 140 150 160 170\nAverage length0.70.80.9AUROC\nEleutherAI/gpt-neox-20bFigure 10. DetectGPT AUROC vs passage length. The relation-\nship between detection performance and passage length generally\ndepends on the dataset and model (or tokenizer). Decreases in\ndetection quality with increasing length may be due to T5 failing\nto track many (20+) masks to fill at once; this problem may be\nmitigated by applying mask-fills in a sequence of smaller batches.\nof lower quality perturbations. Finally, DetectGPT is more\ncompute-intensive than other methods for detection, as it\nrequires sampling and scoring the set of perturbations for\neach candidate passage, rather than just the candidate pas-\nsage; a better tuned perturbation function or more efficient\ncurvature approximation may help mitigate these costs.\nFuture Work. While the methods in this work make no\nassumptions about the models generating the samples, fu-\nture work may explore how watermarking algorithms can be\nused in conjunction with detection algorithms like Detect-\nGPT to further improve detection robustness as language\nmodels continually improve their reproductions of human\ntext. Separately, the results in Section 5.2 suggest that ex-\ntending DetectGPT to use ensembles of models for scoring,'
 'models continually improve their reproductions of human\ntext. Separately, the results in Section 5.2 suggest that ex-\ntending DetectGPT to use ensembles of models for scoring,\nrather than a single model, may improve detection in the\nblack box setting. Another topic that remains unexplored\nis the relationship between prompting and detection; that\nis, can a clever prompt successfully prevent a model’s gen-\nerations from being detected by existing methods? Finally,\nfuture work may explore whether the local log probabil-\nity curvature property we identify is present for generative\nmodels in other domains, such as audio, video, or images.\nWe hope that the present work serves as inspiration to fu-\nture work developing effective, general-purpose methods\nfor mitigating potential harms of machine-generated media.\nAcknowledgements\nEM gratefully acknowledges funding from a Knight-\nHennessy Graduate Fellowship. CF and CM are CIFAR\nFellows. The Stanford Center for Research on Foundation\nModels (CRFM) provided part of the compute resources\nused for the experiments in this work.\n9']",DetectGPT exploits the characteristic that LLM-generated text tends to lie in regions of negative curvature of the model's log probability function.,0.999999999975,1.0,0.9043659174325476,1.0,0.8401822405967527,0.9607289623870103
What empirical validation does DetectGPT provide for its hypothesis regarding log probability curvature?,"DetectGPT empirically validates its hypothesis by demonstrating that text generated from large language models tends to occupy regions of negative curvature in the model's log probability function. Specifically, it shows that passages generated by models exhibit a significantly higher average drop in log probability (perturbation discrepancy) after rephrasing, compared to human-written texts. This consistent pattern supports the effectiveness of using perturbation discrepancies for detecting machine-generated text.","['Zero-Shot Machine-Generated Text Detection using Probability Curvature\n0.0 0.1 0.20204060XSum\n0.1\n 0.0 0.1 0.2WritingPrompts\n0.0 0.1 0.20204060SQuAD\nHuman\nModel\n0.1\n 0.0 0.1 0.2 0.3PubMed\n0.0 0.2 0.4 0.6 0.8 1.0\nLog Probability Change (Perturbation Discrepancy)0.00.20.40.60.81.0Frequency\nFigure 9. Perturbation discrepancy distributions for GPT-Neo\n(2.7B) and humans across domains. A threshold of 0.1 gener-\nally separates model- and human-generated text well, which is\nimportant for practical scenarios where the domain is unknown.\nidentify a property of the log probability function computed\nby a wide variety of large language models, showing that a\ntractable approximation to the trace of the Hessian of the\nmodel’s log probability function provides a useful signal\nfor detecting model samples. Our experiments find that\nthis signal is more discriminative than existing zero-shot\ndetection methods and is competitive with bespoke detection\nmodels trained with millions of model samples.\nDetectGPT and Watermarking. One interpretation of\nthe perturbation function is producing semantically similar\nrephrasings of the original passage . If these rephrasings\nare systematically lower-probability than the original pas-\nsage, the model is exposing its bias toward the specific (and\nroughly arbitrary, by human standards) phrasing used. In\nother words, LLMs that do not perfectly imitate human\nwriting essentially watermark themselves implicitly. Under\nthis interpretation, efforts to manually add watermarking bi-\nases to model outputs (Aaronson, 2022; Kirchenbauer et al.,\n2023) may further improve the effectiveness of methods\nsuch as DetectGPT, even as LLMs continue to improve.\nLimitations. One limitation of probability-based methods\nfor zero-shot machine-generated text detection (like Detect-\nGPT) is the white-box assumption that we can evaluate log\nprobabilities of the model(s) in question. For models be-\nhind APIs that do provide probabilities (such as GPT-3),\nevaluating probabilities nonetheless costs money. Another\nassumption of DetectGPT is access to a reasonable pertur-\nbation function. While in this work, we use off-the-shelf\nmask-filling models such as T5 and mT5 (for non-English\nlanguages), some domains may see reduced performance\nif existing mask-filling models do not well represent the\nspace of meaningful rephrases, reducing the quality of the\ncurvature estimate. While DetectGPT provides the best\navailable detection performance for PubMedQA, its drop\nin performance compared to other datasets may be a result\nAverage length0.9850.9900.995AUROC\ngpt-2\nAverage length0.960.970.980.99AUROC\nopt-2.7\nXSum\nSQuAD\nWritingPrompts\n130 140 150 160 170\nAverage length0.8750.9000.9250.9500.975AUROC\nEleutherAI/gpt-j-6b\n130 140 150 160 170\nAverage length0.70.80.9AUROC\nEleutherAI/gpt-neox-20bFigure 10. DetectGPT AUROC vs passage length. The relation-\nship between detection performance and passage length generally\ndepends on the dataset and model (or tokenizer). Decreases in\ndetection quality with increasing length may be due to T5 failing\nto track many (20+) masks to fill at once; this problem may be\nmitigated by applying mask-fills in a sequence of smaller batches.\nof lower quality perturbations. Finally, DetectGPT is more\ncompute-intensive than other methods for detection, as it\nrequires sampling and scoring the set of perturbations for\neach candidate passage, rather than just the candidate pas-\nsage; a better tuned perturbation function or more efficient\ncurvature approximation may help mitigate these costs.\nFuture Work. While the methods in this work make no\nassumptions about the models generating the samples, fu-\nture work may explore how watermarking algorithms can be\nused in conjunction with detection algorithms like Detect-\nGPT to further improve detection robustness as language\nmodels continually improve their reproductions of human\ntext. Separately, the results in Section 5.2 suggest that ex-\ntending DetectGPT to use ensembles of models for scoring,'
 'Zero-Shot Machine-Generated Text Detection using Probability Curvature\nXSum SQuAD WritingPrompts\nMethod GPT-2 OPT-2.7 Neo-2.7 GPT-J NeoX Avg. GPT-2 OPT-2.7 Neo-2.7 GPT-J NeoX Avg. GPT-2 OPT-2.7 Neo-2.7 GPT-J NeoX Avg.\nlogp(x) 0.86 0.86 0.86 0.82 0.77 0.83 0.91 0.88 0.84 0.78 0.71 0.82 0.97 0.95 0.95 0.94 0.93* 0.95\nRank 0.79 0.76 0.77 0.75 0.73 0.76 0.83 0.82 0.80 0.79 0.74 0.80 0.87 0.83 0.82 0.83 0.81 0.83\nLogRank 0.89* 0.88* 0.90* 0.86* 0.81* 0.87* 0.94* 0.92* 0.90* 0.83* 0.76* 0.87* 0.98* 0.96* 0.97* 0.96* 0.95 0.96*\nEntropy 0.60 0.50 0.58 0.58 0.61 0.57 0.58 0.53 0.58 0.58 0.59 0.57 0.37 0.42 0.34 0.36 0.39 0.38\nDetectGPT 0.99 0.97 0.99 0.97 0.95 0.97 0.99 0.97 0.97 0.90 0.79 0.92 0.99 0.99 0.99 0.97 0.93* 0.97\nDiff 0.10 0.09 0.09 0.11 0.14 0.10 0.05 0.05 0.07 0.07 0.03 0.05 0.01 0.03 0.02 0.01 -0.02 0.01\nTable 1. AUROC for detecting samples from the given model on the given dataset for DetectGPT and four previously proposed criteria\n(500 samples used for evaluation). From 1.5B parameter GPT-2 to 20B parameter GPT-NeoX, DetectGPT consistently provides the most\naccurate detections. Bold shows the best AUROC within each column (model-dataset combination); asterisk (*) denotes the second-best\nAUROC. Values in the final row show DetectGPT’s AUROC over the strongest baseline method in that column.\ncompact semantic space. Since the mask-filling model sam-\nples sentences similar to xwith minimal changes to seman-\ntic meaning, we can think of the mask-filling model as first\nsampling a similar semantic embedding ( ˜z∼qz) and then\nmapping this to a token sequence ( ˜z7→˜x). Sampling in\nsemantic space ensures that all samples stay near the data\nmanifold, which is useful because we would expect the log\nprobability to always drop if we randomly perturb tokens.\nWe can therefore interpret our objective as approximating\nthe curvature restricted to the data manifold.\n5. Experiments\nWe conduct experiments to better understand multiple facets\nof machine-generated text detection; we study the effective-\nness of DetectGPT for zero-shot machine-generated text de-\ntection compared to prior zero-shot approaches, the impact\nof distribution shift on zero-shot and supervised detectors,\nand detection accuracy for the largest publicly-available\nmodels. To further characterize factors that impact detec-\ntion accuracy, we also study the robustness of zero-shot\nmethods to machine-generated text that has been partially\nrevised, the impact of alternative decoding strategies on\ndetection accuracy, and a black-box variant of the detec-\ntion task. Finally, we analyze more closely DetectGPT’s\nbehavior as the choice of perturbation function, the number\nof samples used to estimate d(x, pθ, q), the length of the\npassage, and the data distribution is varied.\nComparisons. We compare DetectGPT with various exist-\ning zero-shot methods for machine-generated text detection\nthat also leverage the predicted token-wise conditional dis-\ntributions of the source model for detection. These methods\ncorrespond to statistical tests based on token log probabil-\nities, token ranks, or predictive entropy (Gehrmann et al.,\n2019; Solaiman et al., 2019; Ippolito et al., 2020). The\nfirst method uses the source model’s average token-wise log\nprobability to determine if a candidate passage is machine-\ngenerated or not; passages with high average log probability\nare likely to be generated by the model. The second and\nthird methods use the average observed rank or log-rank of\nthe tokens in the candidate passage according to the model’s\nconditional distributions. Passages with smaller average(log-)rank are likely machine-generated. We also evalu-\nate an entropy-based approach inspired by the hypothesis\nin Gehrmann et al. (2019) that model-generated texts will\nbe more ‘in-distribution’ for the model, leading to more\nover-confident (thus lower entropy) predictive distributions.\nEmpirically, we find predictive entropy to be positively cor-'
 'Zero-Shot Machine-Generated Text Detection using Probability Curvature\n0.0 0.1 0.2 0.30204060gpt2-xl\nHuman\nModel\n0.0 0.1 0.2 0.3EleutherAI/gpt-neo-2.7B\n0.0 0.1 0.2 0.30204060EleutherAI/gpt-j-6B\n0.0 0.1 0.2 0.3EleutherAI/gpt-neox-20b\n0.0 0.2 0.4 0.6 0.8 1.0\nLog Probability Change (Perturbation Discrepancy)0.00.20.40.60.81.0Frequency\nFigure 3. The average drop in log probability (perturbation discrep-\nancy) after rephrasing a passage is consistently higher for model-\ngenerated passages than for human-written passages. Each plot\nshows the distribution of the perturbation discrepancy d(x, pθ, q)\nforhuman-written news articles andmachine-generated arti-\nclesof equal word length. Human-written articles are a sample\nof 500 XSum articles; machine-generated text, generated from\nmodels GPT-2 (1.5B), GPT-Neo-2.7B (Black et al., 2021), GPT-J\n(6B; Wang & Komatsuzaki (2021)) and GPT-NeoX (20B; Black\net al. (2022)), is generated by prompting each model with the first\n30 tokens of each XSum article, sampling from the raw conditional\ndistribution. Discrepancies are estimated with 100 T5-3B samples.\nIf we define q(· |x)to be samples from a mask-filling model\nsuch as T5 (Raffel et al., 2020), rather than human rewrites,\nwe can empirically test the Perturbation Discrepancy Gap\nHypothesis in an automated, scalable manner. For real data,\nwe use 500 news articles from the XSum dataset (Narayan\net al., 2018); for model samples, we use the output of four\ndifferent LLMs when prompted with the first 30 tokens of\neach article in XSum. We use T5-3B to apply perturbations,\nmasking out randomly-sampled 2-word spans until 15% of\nthe words in the article are masked. We approximate the\nexpectation in Eq. 1 with 100 samples from T5.1Figure 3\nshows the result of this experiment. We find the distribution\nof perturbation discrepancies is significantly different for\nhuman-written articles and model samples; model samples\ntend to have a larger perturbation discrepancy. Section 5.3\nexplores a relaxation of the assumption that qonly produces\nsamples on the data manifold, finding that a gap, although\nreduced, still exists in this case.\nGiven these results, we can detect if a piece of text was\ngenerated by a model pθby simply thresholding the pertur-\nbation discrepancy. In practice, we find that normalizing the\nperturbation discrepancy by the standard deviation of the ob-\nserved values used to estimate E˜x∼q(·|x)logpθ(˜x)provides\na slightly better signal for detection, typically increasing\n1We later show in Figure 8 that varying the number of samples\nused to estimate the expectation effectively allows for trading off\nbetween accuracy and speed.AUROC by around 0.020, so we use this normalized version\nof the perturbation discrepancy in our experiments. The\nresulting method, DetectGPT, is summarized in Alg. 1. Hav-\ning described an application of the perturbation discrepancy\nto machine-generated text detection, we next provide an\ninterpretation of this quantity.\nInterpretation of perturbation discrepancy as curvature\nWhile Figure 3 suggests that the perturbation discrepancy\nmay be useful, it is not immediately obvious what it mea-\nsures. In this section, we show that the perturbation dis-\ncrepancy approximates a measure of the local curvature\nof the log probability function near the candidate passage,\nmore specifically, that it is proportional to the negative trace\nof the Hessian of the log probability function.2To han-\ndle the non-differentiability of discrete data, we consider\ncandidate passages in a latent semantic space, where small\ndisplacements correspond to valid edits that retain similar\nmeaning to the original. Because our perturbation function\n(T5) models natural text, we expect our perturbations to\nroughly capture such meaningful variations of the original\npassage, rather than arbitrary edits.\nWe first invoke Hutchinson’s trace estimator (Hutchinson,\n1990), giving an unbiased estimate of the trace of matrix A:\ntr(A) =Ezz⊤Az (2)'
 'DetectGPT: Zero-Shot Machine-Generated Text Detection\nusing Probability Curvature\nEric Mitchell1Yoonho Lee1Alexander Khazatsky1Christopher D. Manning1Chelsea Finn1\nAbstract\nThe increasing fluency and widespread usage of\nlarge language models (LLMs) highlight the de-\nsirability of corresponding tools aiding detection\nof LLM-generated text. In this paper, we identify\na property of the structure of an LLM’s proba-\nbility function that is useful for such detection.\nSpecifically, we demonstrate that text sampled\nfrom an LLM tends to occupy negative curva-\nture regions of the model’s log probability func-\ntion. Leveraging this observation, we then define\na new curvature-based criterion for judging if a\npassage is generated from a given LLM. This\napproach, which we call DetectGPT, does not re-\nquire training a separate classifier, collecting a\ndataset of real or generated passages, or explic-\nitly watermarking generated text. It uses only\nlog probabilities computed by the model of in-\nterest and random perturbations of the passage\nfrom another generic pre-trained language model\n(e.g., T5). We find DetectGPT is more discrimi-\nnative than existing zero-shot methods for model\nsample detection, notably improving detection of\nfake news articles generated by 20B parameter\nGPT-NeoX from 0.81 AUROC for the strongest\nzero-shot baseline to 0.95 AUROC for Detect-\nGPT. See ericmitchell.ai/detectgpt\nfor code, data, and other project information.\n1. Introduction\nLarge language models (LLMs) have proven able to gen-\nerate remarkably fluent responses to a wide variety of user\nqueries. Models such as GPT-3 (Brown et al., 2020), PaLM\n(Chowdhery et al., 2022), and ChatGPT (OpenAI, 2022)\ncan convincingly answer complex questions about science,\nmathematics, historical and current events, and social trends.\n1Stanford University. Correspondence to: Eric Mitchell\n<eric.mitchell@cs.stanford.edu >.\nProceedings of the 40thInternational Conference on Machine\nLearning , Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).\n Candidate passage : “Joe Biden recently made a move to the White House that included bringing along his pet German Shepherd…”\nDetectGPT\nx\n...GPT-3(1)Perturb(2) Score(3) Compare\n🤖  from GPT-3\nYes(reword with T5)\n“made a move”  “moved”→“pet”  “dog”→Delete “bringing along”\n...\n🤔  from other source\nNo\nFigure 1. We aim to determine whether a piece of text was gener-\nated by a particular LLM p, such as GPT-3. To classify a candidate\npassage x, DetectGPT first generates minor perturbations of the\npassage ˜xiusing a generic pre-trained model such as T5. Then\nDetectGPT compares the log probability under pof the original\nsample xwith each perturbed sample ˜xi. If the average log ratio\nis high, the sample is likely from the source model.\nWhile recent work has found that cogent-sounding LLM-\ngenerated responses are often simply wrong (Lin et al.,\n2022), the articulate nature of such generated text may still\nmake LLMs attractive for replacing human labor in some\ncontexts, notably student essay writing and journalism. At\nleast one major news source has released AI-written content\nwith limited human review, leading to substantial factual er-\nrors in some articles (Christian, 2023). Such applications of\nLLMs are problematic for a variety of reasons, making fair\nstudent assessment difficult, impairing student learning, and\nproliferating convincing-but-inaccurate news articles. Un-\nfortunately, humans perform only slightly better than chance\nwhen classifying machine-generated vs human-written text\n(Gehrmann et al., 2019), leading researchers to consider\nautomated detection methods that may identify signals dif-\nficult for humans to recognize. Such methods might give\nteachers and news-readers more confidence in the human\norigin of the text that they consume.\nAs in prior work (Jawahar et al., 2020), we study the\n1arXiv:2301.11305v2  [cs.CL]  23 Jul 2023']","DetectGPT's hypothesis is empirically validated across a diverse body of LLMs, showing consistent results even when perturbations are sourced from different language models.",0.999999999975,0.8571428571428571,0.9004760254611032,1.0,0.873613410700234,0.9230250713723649
What datasets were used for BERT's pre-training and why?,"BERT's pre-training utilized a combination of the BookCorpus and English Wikipedia datasets, totaling 16GB of uncompressed text. These datasets were chosen to provide a diverse and comprehensive range of natural language data, which is essential for effective language model training. The use of large, unlabeled text corpora helps improve the model's ability to generalize and perform well on downstream tasks.","['alternatives that lead to better downstream task\nperformance; (2) We use a novel dataset, CC-\nNEWS, and conﬁrm that using more data for pre-\ntraining further improves performance on down-\nstream tasks; (3) Our training improvements show\nthat masked language model pretraining, under\nthe right design choices, is competitive with all\nother recently published methods. We release our\nmodel, pretraining and ﬁne-tuning code imple-\nmented in PyTorch ( Paszke et al. ,2017 ).\n2 Background\nIn this section, we give a brief overview of the\nBERT ( Devlin et al. ,2019 ) pretraining approach\nand some of the training choices that we will ex-\namine experimentally in the following section.\n2.1 Setup\nBERT takes as input a concatenation of two\nsegments (sequences of tokens), x1,...,x N\nandy1,...,yM. Segments usually consist of\nmore than one natural sentence. The two seg-\nments are presented as a single input sequence\nto BERT with special tokens delimiting them:\n[CLS],x1,...,x N,[SEP],y1,...,yM,[EOS].\nMandNare constrained such that M+N < T ,\nwhereTis a parameter that controls the maximum\nsequence length during training.\nThe model is ﬁrst pretrained on a large unla-\nbeled text corpus and subsequently ﬁnetuned us-\ning end-task labeled data.\n2.2 Architecture\nBERT uses the now ubiquitous transformer archi-\ntecture ( Vaswani et al. ,2017 ), which we will not\nreview in detail. We use a transformer architecture\nwithLlayers. Each block uses Aself-attention\nheads and hidden dimension H.\n2.3 Training Objectives\nDuring pretraining, BERT uses two objectives:\nmasked language modeling and next sentence pre-\ndiction.\nMasked Language Model (MLM) A random\nsample of the tokens in the input sequence is\nselected and replaced with the special token\n[MASK]. The MLM objective is a cross-entropy\nloss on predicting the masked tokens. BERT uni-\nformly selects 15% of the input tokens for possi-\nble replacement. Of the selected tokens, 80% are\nreplaced with [MASK], 10% are left unchanged,and 10% are replaced by a randomly selected vo-\ncabulary token.\nIn the original implementation, random mask-\ning and replacement is performed once in the be-\nginning and saved for the duration of training, al-\nthough in practice, data is duplicated so the mask\nis not always the same for every training sentence\n(see Section 4.1).\nNext Sentence Prediction (NSP) NSP is a bi-\nnary classiﬁcation loss for predicting whether two\nsegments follow each other in the original text.\nPositive examples are created by taking consecu-\ntive sentences from the text corpus. Negative ex-\namples are created by pairing segments from dif-\nferent documents. Positive and negative examples\nare sampled with equal probability.\nThe NSP objective was designed to improve\nperformance on downstream tasks, such as Natural\nLanguage Inference ( Bowman et al. ,2015 ), which\nrequire reasoning about the relationships between\npairs of sentences.\n2.4 Optimization\nBERT is optimized with Adam ( Kingma and Ba ,\n2015 ) using the following parameters: β1= 0.9,\nβ2= 0.999,ǫ=1e-6 and L2weight de-\ncay of0.01. The learning rate is warmed up\nover the ﬁrst 10,000 steps to a peak value of\n1e-4, and then linearly decayed. BERT trains\nwith a dropout of 0.1 on all layers and at-\ntention weights, and a GELU activation func-\ntion ( Hendrycks and Gimpel ,2016 ). Models are\npretrained for S=1,000,000 updates, with mini-\nbatches containing B=256 sequences of maxi-\nmum length T=512 tokens.\n2.5 Data\nBERT is trained on a combination of B OOK COR-\nPUS (Zhu et al. ,2015 ) plus English W IKIPEDIA ,\nwhich totals 16GB of uncompressed text.3\n3 Experimental Setup\nIn this section, we describe the experimental setup\nfor our replication study of BERT.\n3.1 Implementation\nWe reimplement BERT in FAIRSEQ (Ott et al. ,\n2019 ). We primarily follow the original BERT\n3Yang et al. (2019 ) use the same dataset but report having\nonly 13GB of text after data cleaning. This is most likely due\nto subtle differences in cleaning of the Wikipedia data.'
 '2 Approach\nOur training approach is similar to the methods\ndescribed in previous work (Brown et al., 2020;\nChowdhery et al., 2022), and is inspired by the\nChinchilla scaling laws (Hoffmann et al., 2022).\nWe train large transformers on a large quantity of\ntextual data using a standard optimizer.\n2.1 Pre-training Data\nOur training dataset is a mixture of several sources,\nreported in Table 1, that cover a diverse set of do-\nmains. For the most part, we reuse data sources\nthat have been leveraged to train other LLMs, with\nthe restriction of only using data that is publicly\navailable, and compatible with open sourcing. This\nleads to the following mixture of data and the per-\ncentage they represent in the training set:\nEnglish CommonCrawl [67%]. We preprocess\nﬁve CommonCrawl dumps, ranging from 2017\nto 2020, with the CCNet pipeline (Wenzek et al.,\n2020). This process deduplicates the data at the\nline level, performs language identiﬁcation with\na fastText linear classiﬁer to remove non-English\npages and ﬁlters low quality content with an n-\ngram language model. In addition, we trained a\nlinear model to classify pages used as references\nin Wikipedia v.s.randomly sampled pages, and\ndiscarded pages not classiﬁed as references.\nC4 [15%]. During exploratory experiments, we\nobserved that using diverse pre-processed Com-\nmonCrawl datasets improves performance. We thus\nincluded the publicly available C4 dataset (Raffel\net al., 2020) in our data. The preprocessing of C4\nalso contains deduplication and language identiﬁ-\ncation steps: the main difference with CCNet is\nthe quality ﬁltering, which mostly relies on heuris-\ntics such as presence of punctuation marks or the\nnumber of words and sentences in a webpage.\nGithub [4.5%]. We use the public GitHub\ndataset available on Google BigQuery. We only\nkept projects that are distributed under the Apache,\nBSD and MIT licenses. Additionally, we ﬁltered\nlow quality ﬁles with heuristics based on the line\nlength or proportion of alphanumeric characters,\nand removed boilerplate, such as headers, with reg-\nular expressions. Finally, we deduplicate the result-\ning dataset at the ﬁle level, with exact matches.\nWikipedia [4.5%]. We add Wikipedia dumps\nfrom the June-August 2022 period, covering 20Dataset Sampling prop. Epochs Disk size\nCommonCrawl 67.0% 1.10 3.3 TB\nC4 15.0% 1.06 783 GB\nGithub 4.5% 0.64 328 GB\nWikipedia 4.5% 2.45 83 GB\nBooks 4.5% 2.23 85 GB\nArXiv 2.5% 1.06 92 GB\nStackExchange 2.0% 1.03 78 GB\nTable 1: Pre-training data. Data mixtures used for pre-\ntraining, for each subset we list the sampling propor-\ntion, number of epochs performed on the subset when\ntraining on 1.4T tokens, and disk size. The pre-training\nruns on 1T tokens have the same sampling proportion.\nlanguages, which use either the Latin or Cyrillic\nscripts: bg,ca,cs,da,de,en,es,fr,hr,hu,it,\nnl,pl,pt,ro,ru,sl,sr,sv,uk. We process the\ndata to remove hyperlinks, comments and other\nformatting boilerplate.\nGutenberg and Books3 [4.5%]. We include\ntwo book corpora in our training dataset: the Guten-\nberg Project, which contains books that are in the\npublic domain, and the Books3 section of TheP-\nile (Gao et al., 2020), a publicly available dataset\nfor training large language models. We perform\ndeduplication at the book level, removing books\nwith more than 90% content overlap.\nArXiv [2.5%]. We process arXiv Latex ﬁles\nto add scientiﬁc data to our dataset. Following\nLewkowycz et al. (2022), we removed everything\nbefore the ﬁrst section, as well as the bibliography.\nWe also removed the comments from the .tex ﬁles,\nand inline-expanded deﬁnitions and macros written\nby users to increase consistency across papers.\nStack Exchange [2%]. We include a dump of\nStack Exchange, a website of high quality ques-\ntions and answers that covers a diverse set of do-\nmains, ranging from computer science to chemistry.\nWe kept the data from the 28 largest websites, re-\nmoved the HTML tags from text and sorted the\nanswers by score (from highest to lowest).'
 'Published as a conference paper at ICLR 2023\nPre-Training. We provide the complete training notes, Tensorboard logs, and code for our pre-\ntraining in our repository (Cf. Abstract). The pre-training hyper-parameters and cluster configu-\nration are provided in Section 2.3 and Table 11. The training corpora composition and details for\nMulti-task Instruction Pre-training are provided in Section 2.2 and Appendix C.1 and C.2.\nEvaluation. We organize all the evaluation, including language benchmarks (LAMBADA, Pile,\nMMLU, BIG-bench, CLUE, and FewCLUE) and ethical benchmarks (CrowS-Pairs, StereoSet,\nETHOS, RealToxicPrompts), into one-command-to-run bash scripts in our code repository. Data\nprocessing details for language modeling benchmarks are provided in Section 5.1 and Appendix C.4,\nfor MMLU are provided in Section 5.2 and Appendix C.6, for BIG-bench are provided in Section 5.3\nand Appendix C.5, for CLUE and FewCLUE are provided in 5.4. For all ethical evaluation, please\nrefer to Appendix A for details.\nREFERENCES\nOshin Agarwal, Heming Ge, Siamak Shakeri, and Rami Al-Rfou. Knowledge graph based synthetic\ncorpus generation for knowledge-enhanced language model pre-training. In Proceedings of the\n2021 Conference of the North American Chapter of the Association for Computational Linguis-\ntics: Human Language Technologies , pp. 3554–3565, 2021.\nVamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta,\nHonglei Zhuang, Vinh Q Tran, Dara Bahri, Jianmo Ni, et al. Ext5: Towards extreme multi-task\nscaling for transfer learning. In International Conference on Learning Representations , 2022.\nMikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victo-\nria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, et al. Efficient large scale language\nmodeling with mixtures of experts. arXiv preprint arXiv:2112.10684 , 2021.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450 , 2016.\nStephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V Nayak, Ab-\nheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Févry, et al. Promptsource: An integrated\ndevelopment environment and repository for natural language prompts. In Proceedings of the\n60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations ,\npp. 93–104, 2022.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the\ndangers of stochastic parrots: Can language models be too big? In FAccT ’21: 2021 ACM Con-\nference on Fairness, Accountability, and Transparency, Virtual Event / Toronto, Canada, March\n3-10, 2021 , pp. 610–623. ACM, 2021.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from\nquestion-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural\nlanguage processing , pp. 1533–1544, 2013.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical com-\nmonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence ,\nvolume 34, pp. 7432–7439, 2020.\nSidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Ho-\nrace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source\nautoregressive language model. In Proceedings of BigScience Episode \\# 5–Workshop on Chal-\nlenges & Perspectives in Creating Large Language Models , pp. 95–136, 2022.\nSu Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu, Robert Sim, and Hanna Wallach. Stereotyping\nnorwegian salmon: An inventory of pitfalls in fairness benchmark datasets. In Proceedings of the\n59th Annual Meeting of the Association for Computational Linguistics and the 11th International\nJoint Conference on Natural Language Processing (Volume 1: Long Papers) , pp. 1004–1015,\n2021.\n11'
 'and (2) the number of training passes through the\ndata. For example, the recently proposed XLNet\narchitecture ( Yang et al. ,2019 ) is pretrained us-\ning nearly 10 times more data than the original\nBERT ( Devlin et al. ,2019 ). It is also trained with\na batch size eight times larger for half as many op-\ntimization steps, thus seeing four times as many\nsequences in pretraining compared to BERT.\nTo help disentangle the importance of these fac-\ntors from other modeling choices (e.g., the pre-\ntraining objective), we begin by training RoBERTa\nfollowing the BERT LARGE architecture ( L= 24 ,\nH= 1024 ,A= 16 , 355M parameters). We\npretrain for 100K steps over a comparable B OOK -\nCORPUS plus W IKIPEDIA dataset as was used in']","BooksCorpus (800M words) and English Wikipedia (2500M words), chosen for their document-level organization and volume of data.",0.8055555555287036,0.5,0.9725088565146507,1.0,0.474272858746287,0.897091434985148
How do the LLaMA models' parameter counts compare across the different versions?,"The LLaMA models come in various sizes, with parameter counts ranging from 7 billion (7B) to 65 billion (65B). Specifically, the versions include 7B, 13B, 33B, and 65B parameters. Each version is designed to achieve competitive performance while being more efficient in terms of resource usage compared to larger models.","['LLaMA: Open and Efﬁcient Foundation Language Models\nHugo Touvron∗, Thibaut Lavril∗, Gautier Izacard∗, Xavier Martinet\nMarie-Anne Lachaux, Timothee Lacroix, Baptiste Rozière, Naman Goyal\nEric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin\nEdouard Grave∗, Guillaume Lample∗\nMeta AI\nAbstract\nWe introduce LLaMA, a collection of founda-\ntion language models ranging from 7B to 65B\nparameters. We train our models on trillions\nof tokens, and show that it is possible to train\nstate-of-the-art models using publicly avail-\nable datasets exclusively, without resorting\nto proprietary and inaccessible datasets. In\nparticular, LLaMA-13B outperforms GPT-3\n(175B) on most benchmarks, and LLaMA-\n65B is competitive with the best models,\nChinchilla-70B and PaLM-540B. We release\nall our models to the research community1.\n1 Introduction\nLarge Languages Models (LLMs) trained on mas-\nsive corpora of texts have shown their ability to per-\nform new tasks from textual instructions or from a\nfew examples (Brown et al., 2020). These few-shot\nproperties ﬁrst appeared when scaling models to a\nsufﬁcient size (Kaplan et al., 2020), resulting in a\nline of work that focuses on further scaling these\nmodels (Chowdhery et al., 2022; Rae et al., 2021).\nThese efforts are based on the assumption that\nmore parameters will lead to better performance.\nHowever, recent work from Hoffmann et al. (2022)\nshows that, for a given compute budget, the best\nperformances are not achieved by the largest mod-\nels, but by smaller models trained on more data.\nThe objective of the scaling laws from Hoff-\nmann et al. (2022) is to determine how to best\nscale the dataset and model sizes for a particular\ntraining compute budget. However, this objective\ndisregards the inference budget, which becomes\ncritical when serving a language model at scale.\nIn this context, given a target level of performance,\nthe preferred model is not the fastest to train but the\nfastest at inference, and although it may be cheaper\nto train a large model to reach a certain level of\n∗Equal contribution. Correspondence: {htouvron,\nthibautlav,gizacard,egrave,glample}@meta.com\n1https://github.com/facebookresearch/llamaperformance, a smaller one trained longer will\nultimately be cheaper at inference. For instance,\nalthough Hoffmann et al. (2022) recommends\ntraining a 10B model on 200B tokens, we ﬁnd\nthat the performance of a 7B model continues to\nimprove even after 1T tokens.\nThe focus of this work is to train a series of\nlanguage models that achieve the best possible per-\nformance at various inference budgets, by training\non more tokens than what is typically used. The\nresulting models, called LLaMA , ranges from 7B\nto 65B parameters with competitive performance\ncompared to the best existing LLMs. For instance,\nLLaMA-13B outperforms GPT-3 on most bench-\nmarks, despite being 10 ×smaller. We believe that\nthis model will help democratize the access and\nstudy of LLMs, since it can be run on a single GPU.\nAt the higher-end of the scale, our 65B-parameter\nmodel is also competitive with the best large lan-\nguage models such as Chinchilla or PaLM-540B.\nUnlike Chinchilla, PaLM, or GPT-3, we only\nuse publicly available data, making our work com-\npatible with open-sourcing, while most existing\nmodels rely on data which is either not publicly\navailable or undocumented (e.g. “Books – 2TB” or\n“Social media conversations”). There exist some\nexceptions, notably OPT (Zhang et al., 2022),\nGPT-NeoX (Black et al., 2022), BLOOM (Scao\net al., 2022) and GLM (Zeng et al., 2022), but none\nthat are competitive with PaLM-62B or Chinchilla.\nIn the rest of this paper, we present an overview\nof the modiﬁcations we made to the transformer\narchitecture (Vaswani et al., 2017), as well as our\ntraining method. We then report the performance of\nour models and compare with others LLMs on a set\nof standard benchmarks. Finally, we expose some\nof the biases and toxicity encoded in our models,\nusing some of the most recent benchmarks from'
 'HellaSwag (Zellers et al., 2019), WinoGrande (Sak-\naguchi et al., 2021), ARC easy and challenge (Clark\net al., 2018) and OpenBookQA (Mihaylov et al.,\n2018). These datasets include Cloze and Winograd\nstyle tasks, as well as multiple choice question an-\nswering. We evaluate in the zero-shot setting as\ndone in the language modeling community.\nIn Table 3, we compare with existing models\nof various sizes and report numbers from the cor-\nresponding papers. First, LLaMA-65B outper-\nforms Chinchilla-70B on all reported benchmarks\nbut BoolQ. Similarly, this model surpasses PaLM-\n540B everywhere but on BoolQ and WinoGrande.\nLLaMA-13B model also outperforms GPT-3 on\nmost benchmarks despite being 10 ×smaller.\n3.2 Closed-book Question Answering\nWe compare LLaMA to existing large language\nmodels on two closed-book question answering\nbenchmarks: Natural Questions (Kwiatkowski\net al., 2019) and TriviaQA (Joshi et al., 2017). For\nboth benchmarks, we report exact match perfor-\nmance in a closed book setting, i.e., where the mod-\nels do not have access to documents that contain\nevidence to answer the question. In Table 4, we\nreport performance on NaturalQuestions, and in Ta-\nble 5, we report on TriviaQA. On both benchmarks,\nLLaMA-65B achieve state-of-the-arts performance\nin the zero-shot and few-shot settings. More im-\nportantly, the LLaMA-13B is also competitive on\nthese benchmarks with GPT-3 and Chinchilla, de-\nspite being 5-10 ×smaller. This model runs on a\nsingle V100 GPU during inference.\n0-shot 1-shot 5-shot 64-shot\nGopher 280B 43.5 - 57.0 57.2\nChinchilla 70B 55.4 - 64.1 64.6\nLLaMA7B 50.0 53.4 56.3 57.6\n13B 56.6 60.5 63.1 64.0\n33B 65.1 67.9 69.9 70.4\n65B 68.2 71.6 72.6 73.0\nTable 5: TriviaQA. Zero-shot and few-shot exact\nmatch performance on the ﬁltered dev set.\n3.3 Reading Comprehension\nWe evaluate our models on the RACE reading com-\nprehension benchmark (Lai et al., 2017). This\ndataset was collected from English reading com-\nprehension exams designed for middle and highRACE-middle RACE-high\nGPT-3 175B 58.4 45.5\nPaLM8B 57.9 42.3\n62B 64.3 47.5\n540B 68.1 49.1\nLLaMA7B 61.1 46.9\n13B 61.6 47.2\n33B 64.1 48.3\n65B 67.9 51.6\nTable 6: Reading Comprehension. Zero-shot accu-\nracy.\nschool Chinese students. We follow the evaluation\nsetup from Brown et al. (2020) and report results\nin Table 6. On these benchmarks, LLaMA-65B is\ncompetitive with PaLM-540B, and, LLaMA-13B\noutperforms GPT-3 by a few percents.\n3.4 Mathematical reasoning\nWe evaluate our models on two mathematical rea-\nsoning benchmarks: MATH (Hendrycks et al.,\n2021) and GSM8k (Cobbe et al., 2021). MATH\nis a dataset of 12K middle school and high school\nmathematics problems written in LaTeX. GSM8k\nis a set of middle school mathematical problems.\nIn Table 7, we compare with PaLM and Min-\nerva (Lewkowycz et al., 2022). Minerva is a series\nof PaLM models ﬁnetuned on 38.5B tokens ex-\ntracted from ArXiv and Math Web Pages, while\nneither PaLM or LLaMA are ﬁnetuned on mathe-\nmatical data. The numbers for PaLM and Minerva\nare taken from Lewkowycz et al. (2022), and we\ncompare with and without maj1@k .maj1@k de-\nnotes evaluations where we generate ksamples for\neach problem and perform a majority voting (Wang\net al., 2022). On GSM8k, we observe that LLaMA-\n65B outperforms Minerva-62B, although it has not\nbeen ﬁne-tuned on mathematical data.\n3.5 Code generation\nWe evaluate the ability of our models to write\ncode from a natural language description on two\nbenchmarks: HumanEval (Chen et al., 2021) and\nMBPP (Austin et al., 2021). For both tasks, the\nmodel receives a description of the program in a\nfew sentences, as well as a few input-output ex-\namples. In HumanEval, it also receives a function\nsignature, and the prompt is formatted as natural\ncode with the textual description and tests in a'
 'MATH +maj1@k GSM8k +maj1@k\nPaLM8B 1.5 - 4.1 -\n62B 4.4 - 33.0 -\n540B 8.8 - 56.5 -\nMinerva8B 14.1 25.4 16.2 28.4\n62B 27.6 43.4 52.4 68.5\n540B 33.6 50.3 68.5 78.5\nLLaMA7B 2.9 6.9 11.0 18.1\n13B 3.9 8.8 17.8 29.3\n33B 7.1 15.2 35.6 53.1\n65B 10.6 20.5 50.9 69.7\nTable 7: Model performance on quantitative reason-\ning datasets. For majority voting, we use the same\nsetup as Minerva, with k= 256 samples for MATH\nandk= 100 for GSM8k (Minerva 540B uses k= 64\nfor MATH and and k= 40 for GSM8k). LLaMA-65B\noutperforms Minerva 62B on GSM8k, although it has\nnot been ﬁne-tuned on mathematical data.\ndocstring. The model needs to generate a Python\nprogram that ﬁts the description and satisﬁes the\ntest cases. In Table 8, we compare the pass@1\nscores of our models with existing language mod-\nels that have not been ﬁnetuned on code, namely\nPaLM and LaMDA (Thoppilan et al., 2022). PaLM\nand LLaMA were trained on datasets that contain\na similar number of code tokens.\nAs show in Table 8, for a similar number\nof parameters, LLaMA outperforms other gen-\neral models such as LaMDA and PaLM, which\nare not trained or ﬁnetuned speciﬁcally for code.\nLLaMA with 13B parameters and more outper-\nforms LaMDA 137B on both HumanEval and\nMBPP. LLaMA 65B also outperforms PaLM 62B,\neven when it is trained longer. The pass@1 results\nreported in this table were obtained by sampling\nwith temperature 0.1. The pass@100 and pass@80\nmetrics were obtained with temperature 0.8. We\nuse the same method as Chen et al. (2021) to obtain\nunbiased estimates of the pass@k.\nIt is possible to improve the performance on code\nby ﬁnetuning on code-speciﬁc tokens. For instance,\nPaLM-Coder (Chowdhery et al., 2022) increases\nthe pass@1 score of PaLM on HumanEval from\n26.2% for PaLM to 36%. Other models trained\nspeciﬁcally for code also perform better than gen-\neral models on these tasks (Chen et al., 2021; Ni-\njkamp et al., 2022; Fried et al., 2022). Finetuning\non code tokens is beyond the scope of this paper.Params HumanEval MBPP\npass@ @1 @100 @1 @80\nLaMDA 137B 14.0 47.3 14.8 62.4\nPaLM 8B 3.6∗18.7∗5.0∗35.7∗\nPaLM 62B 15.9 46.3∗21.4 63.2∗\nPaLM-cont 62B 23.7 - 31.2 -\nPaLM 540B 26.2 76.2 36.8 75.0\nLLaMA7B 10.5 36.5 17.7 56.2\n13B 15.8 52.5 22.0 64.0\n33B 21.7 70.7 30.2 73.4\n65B 23.7 79.3 37.7 76.8\nTable 8: Model performance for code generation.\nWe report the pass@ score on HumanEval and MBPP.\nHumanEval generations are done in zero-shot and\nMBBP with 3-shot prompts similar to Austin et al.\n(2021). The values marked with∗are read from ﬁgures\nin Chowdhery et al. (2022).\n3.6 Massive Multitask Language\nUnderstanding\nThe massive multitask language understanding\nbenchmark, or MMLU, introduced by Hendrycks\net al. (2020) consists of multiple choice questions\ncovering various domains of knowledge, includ-\ning humanities, STEM and social sciences. We\nevaluate our models in the 5-shot setting, using the\nexamples provided by the benchmark, and report\nresults in Table 9. On this benchmark, we observe\nthat the LLaMA-65B is behind both Chinchilla-\n70B and PaLM-540B by a few percent in average,\nand across most domains. A potential explanation\nis that we have used a limited amount of books\nand academic papers in our pre-training data, i.e.,\nArXiv, Gutenberg and Books3, that sums up to only\n177GB, while these models were trained on up to\n2TB of books. This large quantity of books used\nby Gopher, Chinchilla and PaLM may also explain\nwhy Gopher outperforms GPT-3 on this benchmark,\nwhile it is comparable on other benchmarks.\n3.7 Evolution of performance during training\nDuring training, we tracked the performance of our\nmodels on a few question answering and common\nsense benchmarks, and report them in Figure 2.\nOn most benchmarks, the performance improves\nsteadily, and correlates with the training perplexity\nof the model (see Figure 1). The exceptions are\nSIQA and WinoGrande. Most notably, on SIQA,\nwe observe a lot of variance in performance,'
 'our models and compare with others LLMs on a set\nof standard benchmarks. Finally, we expose some\nof the biases and toxicity encoded in our models,\nusing some of the most recent benchmarks from\nthe responsible AI community.arXiv:2302.13971v1  [cs.CL]  27 Feb 2023']","The LLaMA modelsvary in size from 7 billion parameters to 65 billion parameters, with intermediate sizes of 13 billion and 33 billion parameters.",0.9999999999666667,0.8,0.934692953514625,1.0,0.739508901089245,0.9580356043569801
"What are the significant benchmarks LLaMA models were evaluated on, and how does their performance relate to other foundation models?","LLaMA models were evaluated on several significant benchmarks, including HellaSwag, WinoGrande, ARC, OpenBookQA, Natural Questions, TriviaQA, RACE, and mathematical reasoning tasks like MATH and GSM8k. The LLaMA-13B model outperforms GPT-3 on most benchmarks despite being 10 times smaller, and LLaMA-65B achieves competitive performance compared to larger models like Chinchilla and PaLM. Overall, LLaMA models demonstrate strong capabilities while being trained exclusively on publicly available datasets.","['our models and compare with others LLMs on a set\nof standard benchmarks. Finally, we expose some\nof the biases and toxicity encoded in our models,\nusing some of the most recent benchmarks from\nthe responsible AI community.arXiv:2302.13971v1  [cs.CL]  27 Feb 2023'
 'LLaMA: Open and Efﬁcient Foundation Language Models\nHugo Touvron∗, Thibaut Lavril∗, Gautier Izacard∗, Xavier Martinet\nMarie-Anne Lachaux, Timothee Lacroix, Baptiste Rozière, Naman Goyal\nEric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin\nEdouard Grave∗, Guillaume Lample∗\nMeta AI\nAbstract\nWe introduce LLaMA, a collection of founda-\ntion language models ranging from 7B to 65B\nparameters. We train our models on trillions\nof tokens, and show that it is possible to train\nstate-of-the-art models using publicly avail-\nable datasets exclusively, without resorting\nto proprietary and inaccessible datasets. In\nparticular, LLaMA-13B outperforms GPT-3\n(175B) on most benchmarks, and LLaMA-\n65B is competitive with the best models,\nChinchilla-70B and PaLM-540B. We release\nall our models to the research community1.\n1 Introduction\nLarge Languages Models (LLMs) trained on mas-\nsive corpora of texts have shown their ability to per-\nform new tasks from textual instructions or from a\nfew examples (Brown et al., 2020). These few-shot\nproperties ﬁrst appeared when scaling models to a\nsufﬁcient size (Kaplan et al., 2020), resulting in a\nline of work that focuses on further scaling these\nmodels (Chowdhery et al., 2022; Rae et al., 2021).\nThese efforts are based on the assumption that\nmore parameters will lead to better performance.\nHowever, recent work from Hoffmann et al. (2022)\nshows that, for a given compute budget, the best\nperformances are not achieved by the largest mod-\nels, but by smaller models trained on more data.\nThe objective of the scaling laws from Hoff-\nmann et al. (2022) is to determine how to best\nscale the dataset and model sizes for a particular\ntraining compute budget. However, this objective\ndisregards the inference budget, which becomes\ncritical when serving a language model at scale.\nIn this context, given a target level of performance,\nthe preferred model is not the fastest to train but the\nfastest at inference, and although it may be cheaper\nto train a large model to reach a certain level of\n∗Equal contribution. Correspondence: {htouvron,\nthibautlav,gizacard,egrave,glample}@meta.com\n1https://github.com/facebookresearch/llamaperformance, a smaller one trained longer will\nultimately be cheaper at inference. For instance,\nalthough Hoffmann et al. (2022) recommends\ntraining a 10B model on 200B tokens, we ﬁnd\nthat the performance of a 7B model continues to\nimprove even after 1T tokens.\nThe focus of this work is to train a series of\nlanguage models that achieve the best possible per-\nformance at various inference budgets, by training\non more tokens than what is typically used. The\nresulting models, called LLaMA , ranges from 7B\nto 65B parameters with competitive performance\ncompared to the best existing LLMs. For instance,\nLLaMA-13B outperforms GPT-3 on most bench-\nmarks, despite being 10 ×smaller. We believe that\nthis model will help democratize the access and\nstudy of LLMs, since it can be run on a single GPU.\nAt the higher-end of the scale, our 65B-parameter\nmodel is also competitive with the best large lan-\nguage models such as Chinchilla or PaLM-540B.\nUnlike Chinchilla, PaLM, or GPT-3, we only\nuse publicly available data, making our work com-\npatible with open-sourcing, while most existing\nmodels rely on data which is either not publicly\navailable or undocumented (e.g. “Books – 2TB” or\n“Social media conversations”). There exist some\nexceptions, notably OPT (Zhang et al., 2022),\nGPT-NeoX (Black et al., 2022), BLOOM (Scao\net al., 2022) and GLM (Zeng et al., 2022), but none\nthat are competitive with PaLM-62B or Chinchilla.\nIn the rest of this paper, we present an overview\nof the modiﬁcations we made to the transformer\narchitecture (Vaswani et al., 2017), as well as our\ntraining method. We then report the performance of\nour models and compare with others LLMs on a set\nof standard benchmarks. Finally, we expose some\nof the biases and toxicity encoded in our models,\nusing some of the most recent benchmarks from'
 'HellaSwag (Zellers et al., 2019), WinoGrande (Sak-\naguchi et al., 2021), ARC easy and challenge (Clark\net al., 2018) and OpenBookQA (Mihaylov et al.,\n2018). These datasets include Cloze and Winograd\nstyle tasks, as well as multiple choice question an-\nswering. We evaluate in the zero-shot setting as\ndone in the language modeling community.\nIn Table 3, we compare with existing models\nof various sizes and report numbers from the cor-\nresponding papers. First, LLaMA-65B outper-\nforms Chinchilla-70B on all reported benchmarks\nbut BoolQ. Similarly, this model surpasses PaLM-\n540B everywhere but on BoolQ and WinoGrande.\nLLaMA-13B model also outperforms GPT-3 on\nmost benchmarks despite being 10 ×smaller.\n3.2 Closed-book Question Answering\nWe compare LLaMA to existing large language\nmodels on two closed-book question answering\nbenchmarks: Natural Questions (Kwiatkowski\net al., 2019) and TriviaQA (Joshi et al., 2017). For\nboth benchmarks, we report exact match perfor-\nmance in a closed book setting, i.e., where the mod-\nels do not have access to documents that contain\nevidence to answer the question. In Table 4, we\nreport performance on NaturalQuestions, and in Ta-\nble 5, we report on TriviaQA. On both benchmarks,\nLLaMA-65B achieve state-of-the-arts performance\nin the zero-shot and few-shot settings. More im-\nportantly, the LLaMA-13B is also competitive on\nthese benchmarks with GPT-3 and Chinchilla, de-\nspite being 5-10 ×smaller. This model runs on a\nsingle V100 GPU during inference.\n0-shot 1-shot 5-shot 64-shot\nGopher 280B 43.5 - 57.0 57.2\nChinchilla 70B 55.4 - 64.1 64.6\nLLaMA7B 50.0 53.4 56.3 57.6\n13B 56.6 60.5 63.1 64.0\n33B 65.1 67.9 69.9 70.4\n65B 68.2 71.6 72.6 73.0\nTable 5: TriviaQA. Zero-shot and few-shot exact\nmatch performance on the ﬁltered dev set.\n3.3 Reading Comprehension\nWe evaluate our models on the RACE reading com-\nprehension benchmark (Lai et al., 2017). This\ndataset was collected from English reading com-\nprehension exams designed for middle and highRACE-middle RACE-high\nGPT-3 175B 58.4 45.5\nPaLM8B 57.9 42.3\n62B 64.3 47.5\n540B 68.1 49.1\nLLaMA7B 61.1 46.9\n13B 61.6 47.2\n33B 64.1 48.3\n65B 67.9 51.6\nTable 6: Reading Comprehension. Zero-shot accu-\nracy.\nschool Chinese students. We follow the evaluation\nsetup from Brown et al. (2020) and report results\nin Table 6. On these benchmarks, LLaMA-65B is\ncompetitive with PaLM-540B, and, LLaMA-13B\noutperforms GPT-3 by a few percents.\n3.4 Mathematical reasoning\nWe evaluate our models on two mathematical rea-\nsoning benchmarks: MATH (Hendrycks et al.,\n2021) and GSM8k (Cobbe et al., 2021). MATH\nis a dataset of 12K middle school and high school\nmathematics problems written in LaTeX. GSM8k\nis a set of middle school mathematical problems.\nIn Table 7, we compare with PaLM and Min-\nerva (Lewkowycz et al., 2022). Minerva is a series\nof PaLM models ﬁnetuned on 38.5B tokens ex-\ntracted from ArXiv and Math Web Pages, while\nneither PaLM or LLaMA are ﬁnetuned on mathe-\nmatical data. The numbers for PaLM and Minerva\nare taken from Lewkowycz et al. (2022), and we\ncompare with and without maj1@k .maj1@k de-\nnotes evaluations where we generate ksamples for\neach problem and perform a majority voting (Wang\net al., 2022). On GSM8k, we observe that LLaMA-\n65B outperforms Minerva-62B, although it has not\nbeen ﬁne-tuned on mathematical data.\n3.5 Code generation\nWe evaluate the ability of our models to write\ncode from a natural language description on two\nbenchmarks: HumanEval (Chen et al., 2021) and\nMBPP (Austin et al., 2021). For both tasks, the\nmodel receives a description of the program in a\nfew sentences, as well as a few input-output ex-\namples. In HumanEval, it also receives a function\nsignature, and the prompt is formatted as natural\ncode with the textual description and tests in a'
 'Humanities STEM Social Sciences Other Average\nGPT-NeoX 20B 29.8 34.9 33.7 37.7 33.6\nGPT-3 175B 40.8 36.7 50.4 48.8 43.9\nGopher 280B 56.2 47.4 71.9 66.1 60.0\nChinchilla 70B 63.6 54.9 79.3 73.9 67.5\nPaLM8B 25.6 23.8 24.1 27.8 25.4\n62B 59.5 41.9 62.7 55.8 53.7\n540B 77.0 55.6 81.0 69.6 69.3\nLLaMA7B 34.0 30.5 38.3 38.1 35.1\n13B 45.0 35.8 53.8 53.3 46.9\n33B 55.8 46.0 66.7 63.4 57.8\n65B 61.8 51.7 72.9 67.4 63.4\nTable 9: Massive Multitask Language Understanding (MMLU). Five-shot accuracy.\nthat may indicate that this benchmark is not\nreliable. On WinoGrande, the performance does\nnot correlate as well with training perplexity:\nthe LLaMA-33B and LLaMA-65B have similar\nperformance during the training.\n4 Instruction Finetuning\nIn this section, we show that brieﬂy ﬁnetuning on\ninstructions data rapidly leads to improvements\non MMLU. Although the non-ﬁnetuned version\nof LLaMA-65B is already able to follow basic in-\nstructions, we observe that a very small amount of\nﬁnetuning improves the performance on MMLU,\nand further improves the ability of the model to\nfollow instructions. Since this is not the focus of\nthis paper, we only conducted a single experiment\nfollowing the same protocol as Chung et al. (2022)\nto train an instruct model, LLaMA-I.\nOPT 30B 26.1\nGLM 120B 44.8\nPaLM 62B 55.1\nPaLM-cont 62B 62.8\nChinchilla 70B 67.5\nLLaMA 65B 63.4\nOPT-IML-Max 30B 43.2\nFlan-T5-XXL 11B 55.1\nFlan-PaLM 62B 59.6\nFlan-PaLM-cont 62B 66.1\nLLaMA-I 65B 68.9\nTable 10: Instruction ﬁnetuning – MMLU (5-shot).\nComparison of models of moderate size with and with-\nout instruction ﬁnetuning on MMLU.In Table 10, we report the results of our instruct\nmodel LLaMA-I on MMLU and compare with ex-\nisting instruction ﬁnetuned models of moderate\nsizes, namely, OPT-IML (Iyer et al., 2022) and the\nFlan-PaLM series (Chung et al., 2022). All the re-\nported numbers are from the corresponding papers.\nDespite the simplicity of the instruction ﬁnetuning\napproach used here, we reach 68.9% on MMLU.\nLLaMA-I (65B) outperforms on MMLU existing\ninstruction ﬁnetuned models of moderate sizes, but\nare still far from the state-of-the-art, that is 77.4\nfor GPT code-davinci-002 on MMLU (numbers\ntaken from Iyer et al. (2022)). The details of the\nperformance on MMLU on the 57 tasks can be\nfound in Table 16 of the appendix.\n5 Bias, Toxicity and Misinformation\nLarge language models have been showed to re-\nproduce and amplify biases that are existing in\nthe training data (Sheng et al., 2019; Kurita et al.,\n2019), and to generate toxic or offensive con-\ntent (Gehman et al., 2020). As our training dataset\ncontains a large proportion of data from the Web,\nwe believe that it is crucial to determine the po-\ntential for our models to generate such content.\nTo understand the potential harm of LLaMA-65B,\nwe evaluate on different benchmarks that measure\ntoxic content production and stereotypes detection.\nWhile we have selected some of the standard bench-\nmarks that are used by the language model com-\nmunity to indicate some of the issues with these\nmodels, these evaluations are not sufﬁcient to fully\nunderstand the risks associated with these models.']"," LLaMA models were evaluated on benchmarks such as Common Sense Reasoning, Closed-book Question Answering, Reading Comprehension, Mathematical Reasoning, and Code Generation, showing superior or competitive performance compared to existing foundation models.",0.6388888888675925,1.0,0.9633846128753484,1.0,0.5760783092553172,0.8925485311389163
What is the primary goal of introducing the massive multitask test in language understanding models?,"The primary goal of introducing the massive multitask test is to measure a text model’s multitask accuracy across a diverse set of subjects, thereby evaluating its world knowledge and problem-solving ability. This test aims to bridge the gap between the extensive knowledge models acquire during pretraining and the existing benchmarks that fail to capture the full spectrum of language understanding. By assessing models in zero-shot and few-shot settings, the benchmark highlights important shortcomings and areas for improvement in language understanding capabilities.","['Published as a conference paper at ICLR 2021\nMEASURING MASSIVE MULTITASK\nLANGUAGE UNDERSTANDING\nDan Hendrycks\nUC BerkeleyCollin Burns\nColumbia UniversitySteven Basart\nUChicagoAndy Zou\nUC Berkeley\nMantas Mazeika\nUIUCDawn Song\nUC BerkeleyJacob Steinhardt\nUC Berkeley\nABSTRACT\nWe propose a new test to measure a text model’s multitask accuracy. The test\ncovers 57 tasks including elementary mathematics, US history, computer science,\nlaw, and more. To attain high accuracy on this test, models must possess extensive\nworld knowledge and problem solving ability. We ﬁnd that while most recent\nmodels have near random-chance accuracy, the very largest GPT-3 model improves\nover random chance by almost 20 percentage points on average. However, on every\none of the 57 tasks, the best models still need substantial improvements before\nthey can reach expert-level accuracy. Models also have lopsided performance\nand frequently do not know when they are wrong. Worse, they still have near-\nrandom accuracy on some socially important subjects such as morality and law.\nBy comprehensively evaluating the breadth and depth of a model’s academic and\nprofessional understanding, our test can be used to analyze models across many\ntasks and to identify important shortcomings.\n1 I NTRODUCTION\nNatural Language Processing (NLP) models have achieved superhuman performance on a number of\nrecently proposed benchmarks. However, these models are still well below human level performance\nfor language understanding as a whole, suggesting a disconnect between our benchmarks and the\nactual capabilities of these models. The General Language Understanding Evaluation benchmark\n(GLUE) (Wang et al., 2018) was introduced in 2018 to evaluate performance on a wide range of NLP\ntasks, and top models achieved superhuman performance within a year. To address the shortcomings\nof GLUE, researchers designed the SuperGLUE benchmark with more difﬁcult tasks (Wang et al.,\n2019). About a year since the release of SuperGLUE, performance is again essentially human-level\n(Raffel et al., 2019). While these benchmarks evaluate linguistic skills more than overall language\nunderstanding, an array of commonsense benchmarks have been proposed to measure basic reasoning\nand everyday knowledge (Zellers et al., 2019; Huang et al., 2019; Bisk et al., 2019). However, these\nrecent benchmarks have similarly seen rapid progress (Khashabi et al., 2020). Overall, the near\nhuman-level performance on these benchmarks suggests that they are not capturing important facets\nof language understanding.\nTransformer models have driven this recent progress by pretraining on massive text corpora, including\nall of Wikipedia, thousands of books, and numerous websites. These models consequently see\nextensive information about specialized topics, most of which is not assessed by existing NLP\nbenchmarks. It consequently remains an open question just how capable current language models are\nat learning and applying knowledge from many domains.\nTo bridge the gap between the wide-ranging knowledge that models see during pretraining and the\nexisting measures of success, we introduce a new benchmark for assessing models across a diverse\nset of subjects that humans learn. We design the benchmark to measure knowledge acquired during\npretraining by evaluating models exclusively in zero-shot and few-shot settings. This makes the\nbenchmark more challenging and more similar to how we evaluate humans. The benchmark covers\n57subjects across STEM, the humanities, the social sciences, and more. It ranges in difﬁculty from\nan elementary level to an advanced professional level, and it tests both world knowledge and problem\nsolving ability. Subjects range from traditional areas, such as mathematics and history, to more\n1arXiv:2009.03300v3  [cs.CY]  12 Jan 2021'
 'parameter language models, data may also become a bottleneck, as there is far less written about\nesoteric branches of knowledge than about everyday situations.\n6 C ONCLUSION\nWe introduced a new test that measures how well text models can learn and apply knowledge\nencountered during pretraining. By covering 57 subjects at varying levels of difﬁculty, the test\nassesses language understanding in greater breadth and depth than previous benchmarks. We found\nthat it has recently become possible for models to make meaningful progress on the test, but that\nstate-of-the-art models have lopsided performance and rarely excel at any individual task. We also\nshowed that current models are uncalibrated and have difﬁculty with tasks that require calculations.\nWorryingly, models also perform especially poorly on socially relevant subjects including morality\nand law. Our expansive test can help researchers pinpoint important shortcomings of models, making\nit easier to gain a clearer picture of state-of-the-art capabilities.\n8'
 'part of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to beneﬁt from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these ﬁndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.'
 'Large Language Models Struggle to Learn Long-Tail Knowledge\nKassner, N., Krojer, B., and Sch ¨utze, H. Are pretrained\nlanguage models symbolic reasoners over knowledge? In\nCoNLL , 2020.\nKwiatkowski, T., Palomaki, J., Rhinehart, O., Collins, M.,\nParikh, A., Alberti, C., Epstein, D., Polosukhin, I., Kelcey,\nM., Devlin, J., et al. Natural Questions: A benchmark for\nquestion answering research. In TACL , 2019.\nLauren c ¸on, H., Saulnier, L., Wang, T., Akiki, C., del Moral,\nA. V ., Scao, T. L., Werra, L. V ., Mou, C., Ponferrada,\nE. G., Nguyen, H., Frohberg, J., ˇSaˇsko, M., Lhoest, Q.,\nMcMillan-Major, A., et al. The BigScience ROOTS\ncorpus: A 1.6TB composite multilingual dataset. In\nNeurIPS , 2022.\nLee, K., Chang, M.-W., and Toutanova, K. Latent retrieval\nfor weakly supervised open domain question answering.\nInACL, 2019.\nLee, K., Ippolito, D., Nystrom, A., Zhang, C., Eck, D.,\nCallison-Burch, C., and Carlini, N. Deduplicating train-\ning data makes language models better. In ACL, 2021.\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V .,\nGoyal, N., K ¨uttler, H., Lewis, M., Yih, W.-t., Rockt ¨aschel,\nT., et al. Retrieval-augmented generation for knowledge-\nintensive NLP tasks. In NeurIPS , 2020.\nLiu, L., Lewis, P., Riedel, S., and Stenetorp, P. Challenges\nin generalization in open domain question answering. In\nFindings of NAACL , 2022.\nMallen, A., Asai, A., Zhong, V ., Das, R., Hajishirzi, H., and\nKhashabi, D. When not to trust language models: Investi-\ngating effectiveness and limitations of parametric and non-\nparametric memories. arXiv preprint arXiv:2212.10511 ,\n2022.\nMendes, P. N., Jakob, M., Garc ´ıa-Silva, A., and Bizer, C.\nDBpedia Spotlight: Shedding light on the web of docu-\nments. In International Conference on Semantic Systems ,\n2011.\nPetroni, F., Rockt ¨aschel, T., Lewis, P., Bakhtin, A., Wu,\nY ., Miller, A. H., and Riedel, S. Language models as\nknowledge bases? In EMNLP , 2019.\nPetroni, F., Lewis, P. S. H., Piktus, A., Rockt ¨aschel, T., Wu,\nY ., Miller, A. H., and Riedel, S. How context affects\nlanguage models’ factual predictions. In AKBC , 2020.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W., Liu, P. J., et al. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer. In JMLR , 2020.Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. SQuAD:\n100,000+ questions for machine comprehension of text.\nInEMNLP , 2016.\nRao, R. M., Liu, J., Verkuil, R., Meier, J., Canny, J., Abbeel,\nP., Sercu, T., and Rives, A. Msa transformer. In ICML ,\n2021.\nRazeghi, Y ., Logan IV , R. L., Gardner, M., and Singh, S.\nImpact of pretraining term frequencies on few-shot rea-\nsoning. In Findings of the Association for Computational\nLinguistics: EMNLP 2022 , 2022.\nRoberts, A., Raffel, C., and Shazeer, N. How much knowl-\nedge can you pack into the parameters of a language\nmodel? In EMNLP , 2020.\nRobertson, S. and Zaragoza, H. The probabilistic relevance\nframework: BM25 and beyond. Foundations and Trends\nin IR , 2009.\nScao, T. L., Fan, A., Akiki, C., Pavlick, E.-J., Ili’c, S.,\nHesslow, D., Castagn’e, R., Luccioni, A. S., Yvon, F.,\nGall´e, M., Tow, J., Rush, A. M., et al. BLOOM: A\n176b-parameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100 , 2022.\nShin, S., Lee, S.-W., Ahn, H., Kim, S., Kim, H., Kim, B.,\nCho, K., Lee, G., Park, W., Ha, J.-W., et al. On the\neffect of pretraining corpora on in-context learning by a\nlarge-scale language model. In NAACL , 2022.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. In NeurIPS , 2017.\nWang, B. and Komatsuzaki, A. GPT-J-6B: A 6 Billion\nParameter Autoregressive Language Model. https://\ngithub.com/kingoflolz/mesh-transformer-jax , 2021.\nWang, T., Roberts, A., Hesslow, D., Scao, T. L., Chung,\nH. W., Beltagy, I., Launay, J., and Raffel, C. What lan-\nguage model architecture and pretraining objective work']","The primary goal is to bridge the gap between the vast knowledge models encounter during pretraining on the internet and the existing measures of success, assessing models across a diverse set of subjects to highlight knowledge and problem-solving ability in more challenging, real-world contexts.",0.9999999999666667,1.0,0.9646846270232711,1.0,0.8556971919955989,0.8964729785087118
"What were the key findings regarding the models' performance on the multitask test, particularly concerning their knowledge application and subject-specific accuracy?","The key findings revealed that while the largest GPT-3 model improved over random chance by nearly 20 percentage points on the multitask test, overall performance remained lopsided, with models struggling to achieve expert-level accuracy across the 57 subjects tested. Models exhibited significant difficulties with socially important topics such as morality and law, often demonstrating near-random accuracy on these subjects. Additionally, the models were found to be uncalibrated, frequently unaware of their errors, particularly in tasks that required calculations.","['parameter language models, data may also become a bottleneck, as there is far less written about\nesoteric branches of knowledge than about everyday situations.\n6 C ONCLUSION\nWe introduced a new test that measures how well text models can learn and apply knowledge\nencountered during pretraining. By covering 57 subjects at varying levels of difﬁculty, the test\nassesses language understanding in greater breadth and depth than previous benchmarks. We found\nthat it has recently become possible for models to make meaningful progress on the test, but that\nstate-of-the-art models have lopsided performance and rarely excel at any individual task. We also\nshowed that current models are uncalibrated and have difﬁculty with tasks that require calculations.\nWorryingly, models also perform especially poorly on socially relevant subjects including morality\nand law. Our expansive test can help researchers pinpoint important shortcomings of models, making\nit easier to gain a clearer picture of state-of-the-art capabilities.\n8'
 'Published as a conference paper at ICLR 2021\nMEASURING MASSIVE MULTITASK\nLANGUAGE UNDERSTANDING\nDan Hendrycks\nUC BerkeleyCollin Burns\nColumbia UniversitySteven Basart\nUChicagoAndy Zou\nUC Berkeley\nMantas Mazeika\nUIUCDawn Song\nUC BerkeleyJacob Steinhardt\nUC Berkeley\nABSTRACT\nWe propose a new test to measure a text model’s multitask accuracy. The test\ncovers 57 tasks including elementary mathematics, US history, computer science,\nlaw, and more. To attain high accuracy on this test, models must possess extensive\nworld knowledge and problem solving ability. We ﬁnd that while most recent\nmodels have near random-chance accuracy, the very largest GPT-3 model improves\nover random chance by almost 20 percentage points on average. However, on every\none of the 57 tasks, the best models still need substantial improvements before\nthey can reach expert-level accuracy. Models also have lopsided performance\nand frequently do not know when they are wrong. Worse, they still have near-\nrandom accuracy on some socially important subjects such as morality and law.\nBy comprehensively evaluating the breadth and depth of a model’s academic and\nprofessional understanding, our test can be used to analyze models across many\ntasks and to identify important shortcomings.\n1 I NTRODUCTION\nNatural Language Processing (NLP) models have achieved superhuman performance on a number of\nrecently proposed benchmarks. However, these models are still well below human level performance\nfor language understanding as a whole, suggesting a disconnect between our benchmarks and the\nactual capabilities of these models. The General Language Understanding Evaluation benchmark\n(GLUE) (Wang et al., 2018) was introduced in 2018 to evaluate performance on a wide range of NLP\ntasks, and top models achieved superhuman performance within a year. To address the shortcomings\nof GLUE, researchers designed the SuperGLUE benchmark with more difﬁcult tasks (Wang et al.,\n2019). About a year since the release of SuperGLUE, performance is again essentially human-level\n(Raffel et al., 2019). While these benchmarks evaluate linguistic skills more than overall language\nunderstanding, an array of commonsense benchmarks have been proposed to measure basic reasoning\nand everyday knowledge (Zellers et al., 2019; Huang et al., 2019; Bisk et al., 2019). However, these\nrecent benchmarks have similarly seen rapid progress (Khashabi et al., 2020). Overall, the near\nhuman-level performance on these benchmarks suggests that they are not capturing important facets\nof language understanding.\nTransformer models have driven this recent progress by pretraining on massive text corpora, including\nall of Wikipedia, thousands of books, and numerous websites. These models consequently see\nextensive information about specialized topics, most of which is not assessed by existing NLP\nbenchmarks. It consequently remains an open question just how capable current language models are\nat learning and applying knowledge from many domains.\nTo bridge the gap between the wide-ranging knowledge that models see during pretraining and the\nexisting measures of success, we introduce a new benchmark for assessing models across a diverse\nset of subjects that humans learn. We design the benchmark to measure knowledge acquired during\npretraining by evaluating models exclusively in zero-shot and few-shot settings. This makes the\nbenchmark more challenging and more similar to how we evaluate humans. The benchmark covers\n57subjects across STEM, the humanities, the social sciences, and more. It ranges in difﬁculty from\nan elementary level to an advanced professional level, and it tests both world knowledge and problem\nsolving ability. Subjects range from traditional areas, such as mathematics and history, to more\n1arXiv:2009.03300v3  [cs.CY]  12 Jan 2021'
 'Chemistry. UniﬁedQA does best on marketing, with an accuracy of 82.5%.\nOverall, models do poorly on highly procedural problems. Figure 6 shows that calculation-heavy\nSTEM subjects tend to have low accuracy compared to verbal subjects. For GPT-3, 9out of the 10\n6'
 'Large Language Models Struggle to Learn Long-Tail Knowledge\n0.30.40.50.60.70.8Recall\nBM25 Top-k\nk = 20\nk = 10k = 5\nk = 3k = 1\n100101102103104\nNumber of Relevant Knowledge Corpus Documents05000Count\nFigure 8. Retrieval systems such as BM25 have a mild dependence\non document count. Above we plot the top- krecall for BM25 on\nNatural Questions for different values of k.\n0.000.050.100.150.200.250.30QA Accuracy\nGPT-Neo Model\n20B\n6B2.7B\n1.3B125M\n100101102103104105106\nNumber of Relevant Pre-training Documents025005000CountFigure 9. Retrieval-augmented LMs no longer exhibit low accu-\nracy on rare instances. We plot GPT-Neo accuracy on Natural\nQuestions when augmented with three paragraphs from BM25.\n2022). Doing so has various limitations, as memorization\ncan be harmful or beneficial even in non-verbatim cases (Ip-\npolito et al., 2022). Our work takes studies non-verbatim\nmemorization in the context of QA—our LMs memorize\nfacts in text form and then answers questions about those\nfacts at test time.\nMemorization and Fact Learning Existing work also\nanalyzes the relationship between the pre-training data and\nthe factual knowledge of LLMs. Aky ¨urek et al. (2022) look\nto automatically identify which documents were most in-\nfluential for a language model’s QA predictions. Our work\ninstead directly identifies and estimates the number of rele-\nvant documents via entity linking large corpora. Other work\nnotices a correspondence between model accuracy and data\nfrequency for different knowledge-intensive tasks (Petroni\net al., 2019; Kassner et al., 2020; De Cao et al., 2021; Wei\net al., 2021; F ´evry et al., 2020) and for domains outside of\nNLP (Rao et al., 2021). Our paper reports similar findings,\nbut scales this analysis to massive LM pre-training datasets\nand model sizes.\nIn concurrent and independent work, Mallen et al. (2022)\nstudy how QA performance correlates with frequency in\nthe pre-training data. Unlike our work, they do not use\nentity linking methods to count occurrences and instead\nuse proxies such as entity popularity on Wikipedia. They\nalso find QA accuracy is highly correlated with pre-trainingdata frequency and show that retrieval models can improve\nlong-tail knowledge. Our work differs in that we conduct\ncausal re-training experiments and find that model scaling\nis highly beneficial to long-tail QA performance.\n6. Conclusion and Future Work\nLarge language models demonstrate impressive few-shot\nlearning capabilities that arise from simply training on large-\nscale internet text. With the open-source release of LLMs—\nand their associated pre-training datasets—the research com-\nmunity can now begin to understand the origins of these\ncapabilities. Our work is one of the first to relate an ob-\nserved phenomenon in LLMs back to the pre-training data\nitself. In our case, our results are negative: while LLMs\nachieve moderate performance on open-domain QA bench-\nmarks, they are mainly successful on questions that probe\nknowledge that appears widely in their pre-training datasets.\nOur work raises numerous directions for further inquiry,\nnamely, how to improve retention of long-tail knowledge\ngiven that simply scaling up model and dataset size will\nlikely be insufficient. We are personally excited about im-\nproving retrieval-augmented LMs, especially with regards\nto their efficiency and retrieval accuracy. Moreover, our\nwork focuses on knowledge learning as it relates to fac-\ntoid question answering, but we leave open the question\nas to whether similar relationships exist for other types of\n8']","The findings highlighted that while recent models like GPT-3 have made progress, they exhibit lopsided performance across different subjects, lack expert-level accuracy, and struggle with procedural knowledge and tasks requiring calculations.",0.999999999975,1.0,0.898828438855125,1.0,0.8944363350628801,0.9527453402515204
How does DetectGPT perform in comparison to the strongest zero-shot baseline when detecting fake news articles generated by GPT-NeoX?,"DetectGPT consistently outperforms the strongest zero-shot baseline in detecting fake news articles generated by GPT-NeoX, achieving an AUROC of 0.97 compared to the baseline's lower performance. It demonstrates superior detection accuracy across various datasets, confirming its effectiveness in identifying machine-generated text. Overall, DetectGPT provides a clearer signal for zero-shot detection.","['have evaluated the detection performance of supervised\nmethods (typically fine-tuned transformers) for detecting\nmachine-generated text. In this section, we explore several\ndomains to better understand the relative strengths of super-\nvised and zero-shot detectors. The results are presented in\nFigure 4, using 200 samples from each dataset for evalua-\ntion. We find that supervised detectors can provide similar\ndetection performance to DetectGPT on in-distribution data\nlike English news, but perform significantly worse than zero-\nshot methods in the case of English scientific writing and\nfail altogether for German writing. This finding echoes past\nwork showing that language models trained for machine-\ngenerated text detection overfit to their training data (source\nmodel, decoding strategy, topic, language, etc.; Uchendu\net al. (2020); Ippolito et al. (2020); Jawahar et al. (2020)).\nIn contrast, zero-shot methods generalize relatively easily\nto new languages and domains; DetectGPT’s performance\nin particular is mostly unaffected by the change in language\nfrom English to German.\nWhile our experiments have shown that DetectGPT is ef-\nfective on a variety of domains and models, it is natural to\nwonder if it is effective for the largest publicly-available\nLMs. Therefore, we also evaluate multiple zero-shot and su-\npervised methods on two 175B parameter models, OpenAI’s\nGPT-3 and AI21 Labs’ Jurassic-2 Jumbo. Because neither\nAPI provides access to the complete conditional distribution\n4The overall ease of detecting machine-generated fake writing\ncorroborates anecdotal reporting that machine-generated creative\nwriting tends to be noticeably generic, and therefore relatively easy\nto detect (Roose & Newton, 2022).\n6'
 'Zero-Shot Machine-Generated Text Detection using Probability Curvature\n0.00 0.08 0.16 0.04 0.12 0.24 0.20\nFraction of GPT-J-generated news article re-written0.60.70.80.91.0Detection AUROC\nRank\nDetectGPTLogRank\nLikelihoodEntropy\nFigure 5. We simulate human edits to machine-generated text by\nreplacing varying fractions of model samples with T5-3B gener-\nated text (masking out random five word spans until r% of text is\nmasked to simulate human edits to machine-generated text). The\nfour top-performing methods all generally degrade in performance\nwith heavier revision, but DetectGPT is consistently most accurate.\nExperiment is conducted on the XSum dataset.\nfor each token, we cannot compare to the rank, log rank, and\nentropy-based prior methods. We sample 150 examples5\nfrom the PubMedQA, XSum, and WritingPrompts datasets\nand compare the two pre-trained RoBERTa-based detector\nmodels with DetectGPT and the probability thresholding\nbaseline. We show in Table 2 that DetectGPT can provide\ndetection competitive with or better than the stronger of the\ntwo supervised models, and it again greatly outperforms\nprobability thresholding on average.\n5.2. Variants of Machine-Generated Text Detection\nDetecting paraphrased machine-generated text. In prac-\ntice, humans may manually edit or refine machine-generated\ntext rather than blindly use a model’s generations for their\ntask of interest. We therefore conduct an experiment to\nsimulate the detection problem for model samples that have\nbeen increasingly heavily revised. We simulate human re-\nvision by replacing 5 word spans of the text with samples\nfrom T5-3B until r% of the text has been replaced, and\nreport performance as rvaries. Figure 5 shows that De-\ntectGPT maintains detection AUROC above 0.8 even when\nnearly a quarter of the text in model samples has been re-\nplaced. Unsurprisingly, almost all methods show a gradual\ndegradation in performance as the sample is more heavily\nrevised. The entropy baseline shows surprisingly robust\nperformance in this setting (althought it is least accurate\non average), even slightly improving detection performance\nup to 24% replacement. DetectGPT shows the strongest\ndetection performance for all revision levels.\nImpact of alternative decoding strategies on detection.\nWhile Table 1 suggests that DetectGPT is effective for\n5We reduce the number of evaluation samples from 500 in our\nmain experiments to reduce the API costs of these experiments.XSum SQuAD WritingPrompts\nMethod top- ptop-ktop-ptop-ktop-p top-k\nlogp(x) 0.92 0.87 0.89 0.85 0.98 0.96\nRank 0.76 0.76 0.81 0.80 0.84 0.83\nLogRank 0.93* 0.90* 0.92* 0.90* 0.98 0.97\nEntropy 0.53 0.55 0.54 0.56 0.32 0.35\nDetectGPT 0.98 0.98 0.94 0.93 0.98 0.97\nTable 3. AUROC for zero-shot methods averaged across the five\nmodels in Table 1 for both top- kand top- psampling, with k=\n40andp= 0.96. Both settings enable slightly more accurate\ndetection, and DetectGPT consistently provides the best detection\nperformance. See Appendix Tables 4 and 5 for complete results.\ndetecting machine-generated text, prior work notes that\nthe decoding strategy (i.e., temperature sampling, top- k,\nnucleus/top- p) can impact the difficulty of detection. We re-\npeat the analysis from Section 5.1 using top- ksampling and\nnucleus sampling. Top- ksampling truncates the sampling\ndistribution to only the khighest-probability next tokens;\nnucleus sampling samples from only the smallest set of to-\nkens whose combined probability exceeds p. The results\nare summarized in Table 3; Appendix Tables 4 and 5 show\ncomplete results. We use k= 40 , andp= 0.96, in line with\nprior work (Ippolito et al., 2020). We find that both top- k\nand nucleus sampling make detection easier, on average.\nAveraging across domains, DetectGPT provides the clearest\nsignal for zero-shot detection.\nDetection when the source model is unknown. While\nour experiments have focused on the white-box setting\nfor machine-generated text detection, in this section, we'
 'signal for zero-shot detection.\nDetection when the source model is unknown. While\nour experiments have focused on the white-box setting\nfor machine-generated text detection, in this section, we\nGPT-J GPT-Neo GPT-2Scoring ModelGPT-J GPT-Neo GPT-2Base Model0.92\n(0.02)0.83\n(0.04)0.79\n(0.02)\n0.64\n(0.06)0.97\n(0.01)0.83\n(0.02)\n0.60\n(0.09)0.85\n(0.05)0.99\n(0.00)0.85\n0.81\n0.81\n0.72 0.88 0.87\nFigure 6. DetectGPT performs\nbest when scoring samples\nwith the same model that gen-\nerated them (diagonal), but\nthe column means suggest that\nsome models (GPT-Neo, GPT-\n2) may be better ‘scorers’ than\nothers (GPT-J). White values\nshow mean (standard error)\nAUROC over XSum, SQuAD,\nand WritingPrompts; black\nshows row/column mean.explore the effect of using\na different model to score a\ncandidate passage (and per-\nturbed texts) than the model\nthat generated the passage.\nIn other words, we aim\nto classify between human-\ngenerated text and text from\nmodel A, but without ac-\ncess to model Ato com-\npute log probabilities. In-\nstead, we use log probabil-\nities computed by a surro-\ngate model B. We con-\nsider three models, GPT-J,\nGPT-Neo-2.7, and GPT-2,\nevaluating all possible com-\nbinations of source model\nand surrogate model (9 to-\ntal). We average the perfor-\nmance across 200 samples\nfrom XSum, SQuAD, and\n7'
 'Zero-Shot Machine-Generated Text Detection using Probability Curvature\nXSum SQuAD WritingPrompts\nMethod GPT-2 OPT-2.7 Neo-2.7 GPT-J NeoX Avg. GPT-2 OPT-2.7 Neo-2.7 GPT-J NeoX Avg. GPT-2 OPT-2.7 Neo-2.7 GPT-J NeoX Avg.\nlogp(x) 0.86 0.86 0.86 0.82 0.77 0.83 0.91 0.88 0.84 0.78 0.71 0.82 0.97 0.95 0.95 0.94 0.93* 0.95\nRank 0.79 0.76 0.77 0.75 0.73 0.76 0.83 0.82 0.80 0.79 0.74 0.80 0.87 0.83 0.82 0.83 0.81 0.83\nLogRank 0.89* 0.88* 0.90* 0.86* 0.81* 0.87* 0.94* 0.92* 0.90* 0.83* 0.76* 0.87* 0.98* 0.96* 0.97* 0.96* 0.95 0.96*\nEntropy 0.60 0.50 0.58 0.58 0.61 0.57 0.58 0.53 0.58 0.58 0.59 0.57 0.37 0.42 0.34 0.36 0.39 0.38\nDetectGPT 0.99 0.97 0.99 0.97 0.95 0.97 0.99 0.97 0.97 0.90 0.79 0.92 0.99 0.99 0.99 0.97 0.93* 0.97\nDiff 0.10 0.09 0.09 0.11 0.14 0.10 0.05 0.05 0.07 0.07 0.03 0.05 0.01 0.03 0.02 0.01 -0.02 0.01\nTable 1. AUROC for detecting samples from the given model on the given dataset for DetectGPT and four previously proposed criteria\n(500 samples used for evaluation). From 1.5B parameter GPT-2 to 20B parameter GPT-NeoX, DetectGPT consistently provides the most\naccurate detections. Bold shows the best AUROC within each column (model-dataset combination); asterisk (*) denotes the second-best\nAUROC. Values in the final row show DetectGPT’s AUROC over the strongest baseline method in that column.\ncompact semantic space. Since the mask-filling model sam-\nples sentences similar to xwith minimal changes to seman-\ntic meaning, we can think of the mask-filling model as first\nsampling a similar semantic embedding ( ˜z∼qz) and then\nmapping this to a token sequence ( ˜z7→˜x). Sampling in\nsemantic space ensures that all samples stay near the data\nmanifold, which is useful because we would expect the log\nprobability to always drop if we randomly perturb tokens.\nWe can therefore interpret our objective as approximating\nthe curvature restricted to the data manifold.\n5. Experiments\nWe conduct experiments to better understand multiple facets\nof machine-generated text detection; we study the effective-\nness of DetectGPT for zero-shot machine-generated text de-\ntection compared to prior zero-shot approaches, the impact\nof distribution shift on zero-shot and supervised detectors,\nand detection accuracy for the largest publicly-available\nmodels. To further characterize factors that impact detec-\ntion accuracy, we also study the robustness of zero-shot\nmethods to machine-generated text that has been partially\nrevised, the impact of alternative decoding strategies on\ndetection accuracy, and a black-box variant of the detec-\ntion task. Finally, we analyze more closely DetectGPT’s\nbehavior as the choice of perturbation function, the number\nof samples used to estimate d(x, pθ, q), the length of the\npassage, and the data distribution is varied.\nComparisons. We compare DetectGPT with various exist-\ning zero-shot methods for machine-generated text detection\nthat also leverage the predicted token-wise conditional dis-\ntributions of the source model for detection. These methods\ncorrespond to statistical tests based on token log probabil-\nities, token ranks, or predictive entropy (Gehrmann et al.,\n2019; Solaiman et al., 2019; Ippolito et al., 2020). The\nfirst method uses the source model’s average token-wise log\nprobability to determine if a candidate passage is machine-\ngenerated or not; passages with high average log probability\nare likely to be generated by the model. The second and\nthird methods use the average observed rank or log-rank of\nthe tokens in the candidate passage according to the model’s\nconditional distributions. Passages with smaller average(log-)rank are likely machine-generated. We also evalu-\nate an entropy-based approach inspired by the hypothesis\nin Gehrmann et al. (2019) that model-generated texts will\nbe more ‘in-distribution’ for the model, leading to more\nover-confident (thus lower entropy) predictive distributions.\nEmpirically, we find predictive entropy to be positively cor-']",DetectGPT improves detection from an AUROC of 0.81 for the strongest zero-shot baseline to 0.95 for DetectGPT.,0.999999999975,0.6666666666666666,0.9694340369599339,1.0,0.8792737493856263,0.9456664261139339
How does DetectGPT's performance vary across different datasets and models in zero-shot detection scenarios?,"DetectGPT shows strong performance in zero-shot detection across various datasets, often outperforming supervised models, particularly in English news. However, its effectiveness decreases when applied to different domains, such as English scientific writing and German text, where it performs significantly worse compared to zero-shot methods. Overall, DetectGPT maintains high detection accuracy even with revisions to machine-generated text, indicating its robustness across different contexts.","['have evaluated the detection performance of supervised\nmethods (typically fine-tuned transformers) for detecting\nmachine-generated text. In this section, we explore several\ndomains to better understand the relative strengths of super-\nvised and zero-shot detectors. The results are presented in\nFigure 4, using 200 samples from each dataset for evalua-\ntion. We find that supervised detectors can provide similar\ndetection performance to DetectGPT on in-distribution data\nlike English news, but perform significantly worse than zero-\nshot methods in the case of English scientific writing and\nfail altogether for German writing. This finding echoes past\nwork showing that language models trained for machine-\ngenerated text detection overfit to their training data (source\nmodel, decoding strategy, topic, language, etc.; Uchendu\net al. (2020); Ippolito et al. (2020); Jawahar et al. (2020)).\nIn contrast, zero-shot methods generalize relatively easily\nto new languages and domains; DetectGPT’s performance\nin particular is mostly unaffected by the change in language\nfrom English to German.\nWhile our experiments have shown that DetectGPT is ef-\nfective on a variety of domains and models, it is natural to\nwonder if it is effective for the largest publicly-available\nLMs. Therefore, we also evaluate multiple zero-shot and su-\npervised methods on two 175B parameter models, OpenAI’s\nGPT-3 and AI21 Labs’ Jurassic-2 Jumbo. Because neither\nAPI provides access to the complete conditional distribution\n4The overall ease of detecting machine-generated fake writing\ncorroborates anecdotal reporting that machine-generated creative\nwriting tends to be noticeably generic, and therefore relatively easy\nto detect (Roose & Newton, 2022).\n6'
 'Zero-Shot Machine-Generated Text Detection using Probability Curvature\n0.00 0.08 0.16 0.04 0.12 0.24 0.20\nFraction of GPT-J-generated news article re-written0.60.70.80.91.0Detection AUROC\nRank\nDetectGPTLogRank\nLikelihoodEntropy\nFigure 5. We simulate human edits to machine-generated text by\nreplacing varying fractions of model samples with T5-3B gener-\nated text (masking out random five word spans until r% of text is\nmasked to simulate human edits to machine-generated text). The\nfour top-performing methods all generally degrade in performance\nwith heavier revision, but DetectGPT is consistently most accurate.\nExperiment is conducted on the XSum dataset.\nfor each token, we cannot compare to the rank, log rank, and\nentropy-based prior methods. We sample 150 examples5\nfrom the PubMedQA, XSum, and WritingPrompts datasets\nand compare the two pre-trained RoBERTa-based detector\nmodels with DetectGPT and the probability thresholding\nbaseline. We show in Table 2 that DetectGPT can provide\ndetection competitive with or better than the stronger of the\ntwo supervised models, and it again greatly outperforms\nprobability thresholding on average.\n5.2. Variants of Machine-Generated Text Detection\nDetecting paraphrased machine-generated text. In prac-\ntice, humans may manually edit or refine machine-generated\ntext rather than blindly use a model’s generations for their\ntask of interest. We therefore conduct an experiment to\nsimulate the detection problem for model samples that have\nbeen increasingly heavily revised. We simulate human re-\nvision by replacing 5 word spans of the text with samples\nfrom T5-3B until r% of the text has been replaced, and\nreport performance as rvaries. Figure 5 shows that De-\ntectGPT maintains detection AUROC above 0.8 even when\nnearly a quarter of the text in model samples has been re-\nplaced. Unsurprisingly, almost all methods show a gradual\ndegradation in performance as the sample is more heavily\nrevised. The entropy baseline shows surprisingly robust\nperformance in this setting (althought it is least accurate\non average), even slightly improving detection performance\nup to 24% replacement. DetectGPT shows the strongest\ndetection performance for all revision levels.\nImpact of alternative decoding strategies on detection.\nWhile Table 1 suggests that DetectGPT is effective for\n5We reduce the number of evaluation samples from 500 in our\nmain experiments to reduce the API costs of these experiments.XSum SQuAD WritingPrompts\nMethod top- ptop-ktop-ptop-ktop-p top-k\nlogp(x) 0.92 0.87 0.89 0.85 0.98 0.96\nRank 0.76 0.76 0.81 0.80 0.84 0.83\nLogRank 0.93* 0.90* 0.92* 0.90* 0.98 0.97\nEntropy 0.53 0.55 0.54 0.56 0.32 0.35\nDetectGPT 0.98 0.98 0.94 0.93 0.98 0.97\nTable 3. AUROC for zero-shot methods averaged across the five\nmodels in Table 1 for both top- kand top- psampling, with k=\n40andp= 0.96. Both settings enable slightly more accurate\ndetection, and DetectGPT consistently provides the best detection\nperformance. See Appendix Tables 4 and 5 for complete results.\ndetecting machine-generated text, prior work notes that\nthe decoding strategy (i.e., temperature sampling, top- k,\nnucleus/top- p) can impact the difficulty of detection. We re-\npeat the analysis from Section 5.1 using top- ksampling and\nnucleus sampling. Top- ksampling truncates the sampling\ndistribution to only the khighest-probability next tokens;\nnucleus sampling samples from only the smallest set of to-\nkens whose combined probability exceeds p. The results\nare summarized in Table 3; Appendix Tables 4 and 5 show\ncomplete results. We use k= 40 , andp= 0.96, in line with\nprior work (Ippolito et al., 2020). We find that both top- k\nand nucleus sampling make detection easier, on average.\nAveraging across domains, DetectGPT provides the clearest\nsignal for zero-shot detection.\nDetection when the source model is unknown. While\nour experiments have focused on the white-box setting\nfor machine-generated text detection, in this section, we'
 'signal for zero-shot detection.\nDetection when the source model is unknown. While\nour experiments have focused on the white-box setting\nfor machine-generated text detection, in this section, we\nGPT-J GPT-Neo GPT-2Scoring ModelGPT-J GPT-Neo GPT-2Base Model0.92\n(0.02)0.83\n(0.04)0.79\n(0.02)\n0.64\n(0.06)0.97\n(0.01)0.83\n(0.02)\n0.60\n(0.09)0.85\n(0.05)0.99\n(0.00)0.85\n0.81\n0.81\n0.72 0.88 0.87\nFigure 6. DetectGPT performs\nbest when scoring samples\nwith the same model that gen-\nerated them (diagonal), but\nthe column means suggest that\nsome models (GPT-Neo, GPT-\n2) may be better ‘scorers’ than\nothers (GPT-J). White values\nshow mean (standard error)\nAUROC over XSum, SQuAD,\nand WritingPrompts; black\nshows row/column mean.explore the effect of using\na different model to score a\ncandidate passage (and per-\nturbed texts) than the model\nthat generated the passage.\nIn other words, we aim\nto classify between human-\ngenerated text and text from\nmodel A, but without ac-\ncess to model Ato com-\npute log probabilities. In-\nstead, we use log probabil-\nities computed by a surro-\ngate model B. We con-\nsider three models, GPT-J,\nGPT-Neo-2.7, and GPT-2,\nevaluating all possible com-\nbinations of source model\nand surrogate model (9 to-\ntal). We average the perfor-\nmance across 200 samples\nfrom XSum, SQuAD, and\n7'
 'sequences by their average length into three bins of equal\nsize (bottom/middle/top third), and plot the AUROC within\neach bin. The relationship between detection performance\nand passage length generally depends on the dataset and\nmodel (or tokenizer). For very long sequences, DetectGPT\nmay see reduced performance because our implementation\nof DetectGPT applies all T5 mask-filling perturbations at\nonce, and T5 may fail to track many mask tokens at once.\nBy applying perturbations in multiple sequential rounds of\nsmaller numbers of masks, this effect may be mitigated.\n6. Discussion\nAs large language models continue to improve, they will\nbecome increasingly attractive tools for replacing human\nwriters in a variety of contexts, such as education, jour-\nnalism, and art. While legitimate uses of language model\ntechnologies exist in all of these settings, teachers, readers,\nand consumers are likely to demand tools for verifying the\nhuman origin of certain content with high educational, so-\ncietal, or artistic significance, particularly when factuality\n(and not just fluency) is crucial.\nIn light of these elevated stakes and the regular emergence of\nnew large language models, we study the zero-shot machine-\ngenerated text detection problem, in which we use only the\nraw log probabilities computed by a generative model to\ndetermine if a candidate passage was sampled from it. We\n8']","DetectGPT shows discriminative improvement over existing methods, notably on the XSum dataset for fake news detection, with significant AUROC improvements.",0.999999999975,1.0,0.972077856670922,1.0,0.22785389251451826,0.911415570058073
How does DistilBERT's performance on the GLUE benchmark compare to BERT and ELMo?,"DistilBERT retains 97% of BERT's performance while being 40% smaller, showing comparable results across various tasks on the GLUE benchmark. It consistently outperforms ELMo, achieving up to a 19-point accuracy improvement on some tasks. Overall, DistilBERT's scores are close to BERT's, indicating its effectiveness as a lighter alternative.","['Table 1: DistilBERT retains 97% of BERT performance. Comparison on the dev sets of the\nGLUE benchmark. ELMo results as reported by the authors. BERT and DistilBERT results are the\nmedians of 5 runs with different seeds.\nModel Score CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B WNLI\nELMo 68.7 44.1 68.6 76.6 71.1 86.2 53.4 91.5 70.4 56.3\nBERT-base 79.5 56.3 86.7 88.6 91.8 89.6 69.3 92.7 89.0 53.5\nDistilBERT 77.0 51.3 82.2 87.5 89.2 88.5 59.9 91.3 86.9 56.3\nTable 2: DistilBERT yields to comparable\nperformance on downstream tasks. Com-\nparison on downstream tasks: IMDb (test ac-\ncuracy) and SQuAD 1.1 (EM/F1 on dev set).\nD: with a second step of distillation during\nﬁne-tuning.\nModel IMDb SQuAD\n(acc.) (EM/F1)\nBERT-base 93.46 81.2/88.5\nDistilBERT 92.82 77.7/85.8\nDistilBERT (D) - 79.1/86.9Table 3: DistilBERT is signiﬁcantly smaller\nwhile being constantly faster. Inference\ntime of a full pass of GLUE task STS-B (sen-\ntiment analysis) on CPU with a batch size of\n1.\nModel # param. Inf. time\n(Millions) (seconds)\nELMo 180 895\nBERT-base 110 668\nDistilBERT 66 410\nDistillation We applied best practices for training BERT model recently proposed in Liu et al. [2019].\nAs such, DistilBERT is distilled on very large batches leveraging gradient accumulation (up to 4K\nexamples per batch) using dynamic masking and without the next sentence prediction objective.\nData and compute power We train DistilBERT on the same corpus as the original BERT model:\na concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015]. DistilBERT\nwas trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the\nRoBERTa model [Liu et al., 2019] required 1 day of training on 1024 32GB V100.\n4 Experiments\nGeneral Language Understanding We assess the language understanding and generalization ca-\npabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark\n[Wang et al., 2018], a collection of 9 datasets for evaluating natural language understanding systems.\nWe report scores on the development sets for each task by ﬁne-tuning DistilBERT without the use\nof ensembling or multi-tasking scheme for ﬁne-tuning (which are mostly orthogonal to the present\nwork). We compare the results to the baseline provided by the authors of GLUE: an ELMo (Peters\net al. [2018]) encoder followed by two BiLSTMs.4\nThe results on each of the 9 tasks are showed on Table 1 along with the macro-score (average of\nindividual scores). Among the 9 tasks, DistilBERT is always on par or improving over the ELMo\nbaseline (up to 19 points of accuracy on STS-B). DistilBERT also compares surprisingly well to\nBERT, retaining 97% of the performance with 40% fewer parameters.\n4.1 Downstream task benchmark\nDownstream tasks We further study the performances of DistilBERT on several downstream tasks\nunder efﬁcient inference constraints: a classiﬁcation task (IMDb sentiment classiﬁcation - Maas et al.\n[2011]) and a question answering task (SQuAD v1.1 - Rajpurkar et al. [2016]).\nAs shown in Table 2, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb\nbenchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.9 points of the full BERT.\nWe also studied whether we could add another step of distillation during the adaptation phase by\nﬁne-tuning DistilBERT on SQuAD using a BERT model previously ﬁne-tuned on SQuAD as a\n4We use jiant [Wang et al., 2019] to compute the baseline.\n3'
 'BiLSTM+ELMo+Attn\nOpenAI GPT\nBERT + Single-task Adapters\nBERT (Large)\nBERT on STILTs\nBERT + BAM\nSemBERT\nSnorkel MeTaL\nALICE (Large)\nMT-DNN (ensemble)\nXLNet-Large (ensemble)0.50.60.70.80.91.01.11.2\nGLUE Score\nHuman Performance\nCoLA\nSST-2MRPC\nSTS-B\nQQP\nMNLIQNLI\nRTE\nWNLIFigure 1: GLUE benchmark performance for submitted systems, rescaled to set human performance\nto 1.0, shown as a single number score, and broken down into the nine constituent task performances.\nFor tasks with multiple metrics, we use an average of the metrics. More information on the tasks\nincluded in GLUE can be found in Wang et al. (2019a) and in Warstadt et al. (2019, CoLA), Socher\net al. (2013, SST-2), Dolan and Brockett (2005, MRPC), Cer et al. (2017, STS-B), and Williams et al.\n(2018, MNLI), and Rajpurkar et al. (2016, the original data source for QNLI).\nremains substantial scope for improvement towards GLUE’s high-level goals, the original version of\nthe benchmark is no longer a suitable metric for quantifying such progress.\nIn response, we introduce SuperGLUE, a new benchmark designed to pose a more rigorous test of\nlanguage understanding. SuperGLUE has the same high-level motivation as GLUE: to provide a\nsimple, hard-to-game measure of progress toward general-purpose language understanding technolo-\ngies for English. We anticipate that signiﬁcant progress on SuperGLUE should require substantive\ninnovations in a number of core areas of machine learning, including sample-efﬁcient, transfer,\nmultitask, and unsupervised or self-supervised learning.\nSuperGLUE follows the basic design of GLUE: It consists of a public leaderboard built around\neight language understanding tasks, drawing on existing data, accompanied by a single-number\nperformance metric, and an analysis toolkit. However, it improves upon GLUE in several ways:\nMore challenging tasks: SuperGLUE retains the two hardest tasks in GLUE. The remaining tasks\nwere identiﬁed from those submitted to an open call for task proposals and were selected based on\ndifﬁculty for current NLP approaches.\nMore diverse task formats: The task formats in GLUE are limited to sentence- and sentence-pair\nclassiﬁcation. We expand the set of task formats in SuperGLUE to include coreference resolution\nand question answering (QA).\nComprehensive human baselines: We include human performance estimates for all benchmark\ntasks, which verify that substantial headroom exists between a strong BERT-based baseline and\nhuman performance.\nImproved code support: SuperGLUE is distributed with a new, modular toolkit for work on\npretraining, multi-task learning, and transfer learning in NLP, built around standard tools including\nPyTorch (Paszke et al., 2017) and AllenNLP (Gardner et al., 2017).\nReﬁned usage rules: The conditions for inclusion on the SuperGLUE leaderboard have been\nrevamped to ensure fair competition, an informative leaderboard, and full credit assignment to data\nand task creators.\nThe SuperGLUE leaderboard, data, and software tools are available at super.gluebenchmark.com .\n2 Related Work\nMuch work prior to GLUE demonstrated that training neural models with large amounts of available\nsupervision can produce representations that effectively transfer to a broad range of NLP tasks\n2'
 '•Learning rate (Adam) : 5e-5, 3e-5, 2e-5\n•Number of epochs : 2, 3, 4\nWe also observed that large data sets (e.g.,\n100k+ labeled training examples) were far less\nsensitive to hyperparameter choice than small data\nsets. Fine-tuning is typically very fast, so it is rea-\nsonable to simply run an exhaustive search over\nthe above parameters and choose the model that\nperforms best on the development set.\nA.4 Comparison of BERT, ELMo ,and\nOpenAI GPT\nHere we studies the differences in recent popular\nrepresentation learning models including ELMo,\nOpenAI GPT and BERT. The comparisons be-\ntween the model architectures are shown visually\nin Figure 3. Note that in addition to the architec-\nture differences, BERT and OpenAI GPT are ﬁne-\ntuning approaches, while ELMo is a feature-based\napproach.\nThe most comparable existing pre-training\nmethod to BERT is OpenAI GPT, which trains a\nleft-to-right Transformer LM on a large text cor-\npus. In fact, many of the design decisions in BERT\nwere intentionally made to make it as close to\nGPT as possible so that the two methods could be\nminimally compared. The core argument of this\nwork is that the bi-directionality and the two pre-\ntraining tasks presented in Section 3.1 account for\nthe majority of the empirical improvements, but\nwe do note that there are several other differences\nbetween how BERT and GPT were trained:\n• GPT is trained on the BooksCorpus (800M\nwords); BERT is trained on the BooksCor-\npus (800M words) and Wikipedia (2,500M\nwords).\n• GPT uses a sentence separator ( [SEP] ) and\nclassiﬁer token ( [CLS] ) which are only in-\ntroduced at ﬁne-tuning time; BERT learns\n[SEP] ,[CLS] and sentence A/Bembed-\ndings during pre-training.\n• GPT was trained for 1M steps with a batch\nsize of 32,000 words; BERT was trained for\n1M steps with a batch size of 128,000 words.\n• GPT used the same learning rate of 5e-5 for\nall ﬁne-tuning experiments; BERT chooses a\ntask-speciﬁc ﬁne-tuning learning rate which\nperforms the best on the development set.To isolate the effect of these differences, we per-\nform ablation experiments in Section 5.1 which\ndemonstrate that the majority of the improvements\nare in fact coming from the two pre-training tasks\nand the bidirectionality they enable.\nA.5 Illustrations of Fine-tuning on Different\nTasks\nThe illustration of ﬁne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speciﬁc\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks. In\nthe ﬁgure,Erepresents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classiﬁcation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\nB Detailed Experimental Setup\nB.1 Detailed Descriptions for the GLUE\nBenchmark Experiments.\nOur GLUE results in Table1 are obtained\nfrom https://gluebenchmark.com/\nleaderboard and https://blog.\nopenai.com/language-unsupervised .\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classiﬁ-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment ,contradiction , or\nneutral with respect to the ﬁrst one.\nQQP Quora Question Pairs is a binary classiﬁ-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classiﬁcation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct'
 'Table 4: Baseline performance on the SuperGLUE development.\nModel Avg BoolQ CB COPA MultiRC ReCoRD RTE WiC WSC\nMetrics Acc. Acc./F1 Acc. F1 a/EM F1/EM Acc. Acc. Acc.\nMost Frequent Class 47.7 62.2 50.0/22.2 55.0 59.9/ 0.8 32.4/31.5 52.7 50.0 63.5\nCBOW 47.7 62.4 71.4/49.6 63.0 20.3/ 0.3 14.4/13.8 54.2 55.3 61.5\nBERT 72.2 77.7 94.6/93.7 69.0 70.5/24.7 70.6/69.8 75.8 74.9 68.3\nBERT++ 74.6 80.1 96.4/95.0 78.0 70.5/24.7 70.6/69.8 82.3 74.9 68.3\nA Development Set Results\nIn Table 4, we present results of the baselines on the SuperGLUE tasks development sets.\nB Performance on GLUE Diagnostics\nFigure 2 shows the performance on the GLUE diagnostics dataset for systems submitted to the public\nleaderboard.\nDisjunction Downward Monotone Restrictivity Double Negation Prepositional Phrases60\n40\n20\n020406080\nChance\nBiLSTM+ELMo+Attn\nOpenAI GPT\nBERT + Single-task Adapters\nBERT (Large)\nBERT on STILTsBERT + BAM\nSemBERT\nSnorkel MeTaL\nALICE (Large)\nMT-DNN (ensemble)\nXLNet-Large (ensemble)\nFigure 2: Performance of GLUE submissions on selected diagnostic categories, reported using the\nR3metric scaled up by 100, as in Wang et al. (2019a, see paper for a description of the categories).\nSome initially difﬁcult categories, like double negation, saw gains from advances on GLUE, but\nothers remain hard (restrictivity) or even adversarial (disjunction, downward monotone).\nC Human Performance Estimation\nFor collecting data to establish human performance on the SuperGLUE tasks, we follow a two\nstep procedure where we ﬁrst provide some training to the crowd workers before they proceed to\nannotation. For both steps and all tasks, the average pay rate is $23.75/hr.7\nIn the training phase, workers are provided with instructions on the task, linked to an FAQ page, and\nare asked to annotate up to 30 examples from the development set. After answering each example,\nworkers are also asked to check their work against the provided ground truth label. After the training\nphase is complete, we provide the qualiﬁcation to work on the annotation phase to all workers\nwho annotated a minimum of ﬁve examples, i.e. completed ﬁve HITs during training and achieved\nperformance at, or above the median performance across all workers during training.\n7This estimate is taken from https://turkerview.com .\n16']","DistilBERT achieves performance close to BERT, outperforming ELMo across GLUE benchmark tasks, demonstrating significant efficiency and effectiveness.",0.8333333332916666,0.8571428571428571,0.9466446223801541,1.0,0.8797639024949162,0.9476270385510939
How does DistilBERT's performance on downstream tasks like IMDb sentiment classification and SQuAD v1.1 compare to BERT?,"DistilBERT performs slightly worse than BERT on downstream tasks, achieving 92.82% accuracy on IMDb compared to BERT's 93.46%, and scoring 77.7/85.8 on SQuAD v1.1 compared to BERT's 81.2/88.5. However, DistilBERT is 40% smaller, making it more efficient. Additionally, an enhanced version of DistilBERT achieved better scores on SQuAD with 79.1/86.9.","['Table 1: DistilBERT retains 97% of BERT performance. Comparison on the dev sets of the\nGLUE benchmark. ELMo results as reported by the authors. BERT and DistilBERT results are the\nmedians of 5 runs with different seeds.\nModel Score CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B WNLI\nELMo 68.7 44.1 68.6 76.6 71.1 86.2 53.4 91.5 70.4 56.3\nBERT-base 79.5 56.3 86.7 88.6 91.8 89.6 69.3 92.7 89.0 53.5\nDistilBERT 77.0 51.3 82.2 87.5 89.2 88.5 59.9 91.3 86.9 56.3\nTable 2: DistilBERT yields to comparable\nperformance on downstream tasks. Com-\nparison on downstream tasks: IMDb (test ac-\ncuracy) and SQuAD 1.1 (EM/F1 on dev set).\nD: with a second step of distillation during\nﬁne-tuning.\nModel IMDb SQuAD\n(acc.) (EM/F1)\nBERT-base 93.46 81.2/88.5\nDistilBERT 92.82 77.7/85.8\nDistilBERT (D) - 79.1/86.9Table 3: DistilBERT is signiﬁcantly smaller\nwhile being constantly faster. Inference\ntime of a full pass of GLUE task STS-B (sen-\ntiment analysis) on CPU with a batch size of\n1.\nModel # param. Inf. time\n(Millions) (seconds)\nELMo 180 895\nBERT-base 110 668\nDistilBERT 66 410\nDistillation We applied best practices for training BERT model recently proposed in Liu et al. [2019].\nAs such, DistilBERT is distilled on very large batches leveraging gradient accumulation (up to 4K\nexamples per batch) using dynamic masking and without the next sentence prediction objective.\nData and compute power We train DistilBERT on the same corpus as the original BERT model:\na concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015]. DistilBERT\nwas trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the\nRoBERTa model [Liu et al., 2019] required 1 day of training on 1024 32GB V100.\n4 Experiments\nGeneral Language Understanding We assess the language understanding and generalization ca-\npabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark\n[Wang et al., 2018], a collection of 9 datasets for evaluating natural language understanding systems.\nWe report scores on the development sets for each task by ﬁne-tuning DistilBERT without the use\nof ensembling or multi-tasking scheme for ﬁne-tuning (which are mostly orthogonal to the present\nwork). We compare the results to the baseline provided by the authors of GLUE: an ELMo (Peters\net al. [2018]) encoder followed by two BiLSTMs.4\nThe results on each of the 9 tasks are showed on Table 1 along with the macro-score (average of\nindividual scores). Among the 9 tasks, DistilBERT is always on par or improving over the ELMo\nbaseline (up to 19 points of accuracy on STS-B). DistilBERT also compares surprisingly well to\nBERT, retaining 97% of the performance with 40% fewer parameters.\n4.1 Downstream task benchmark\nDownstream tasks We further study the performances of DistilBERT on several downstream tasks\nunder efﬁcient inference constraints: a classiﬁcation task (IMDb sentiment classiﬁcation - Maas et al.\n[2011]) and a question answering task (SQuAD v1.1 - Rajpurkar et al. [2016]).\nAs shown in Table 2, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb\nbenchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.9 points of the full BERT.\nWe also studied whether we could add another step of distillation during the adaptation phase by\nﬁne-tuning DistilBERT on SQuAD using a BERT model previously ﬁne-tuned on SQuAD as a\n4We use jiant [Wang et al., 2019] to compute the baseline.\n3'
 '(Zellers et al., 2018), this new task of common-\nsense natural language inference seemed trivial\nfor humans (88%) and yet challenging for then-\nstate-of-the-art models ( ă60%), including ELMo\n(Peters et al., 2018). However, BERT (Devlin\net al., 2018) soon reached over 86%, almost\nhuman-level performance. One news article on\nthis development was headlined “ ﬁnally, a ma-\nchine that can ﬁnish your sentence. ”1\nIn this paper, we investigate the following ques-\ntion: How well do deep pretrained models, like\n1A New York Times article at https: //nyti.ms /2DycutY.\n1arXiv:1905.07830v1  [cs.CL]  19 May 2019'
 '6 Conclusion and future work\nWe introduced DistilBERT, a general-purpose pre-trained version of BERT, 40% smaller, 60% faster,\nthat retains 97% of the language understanding capabilities. We showed that a general-purpose\nlanguage model can be successfully trained with distillation and analyzed the various components\nwith an ablation study. We further demonstrated that DistilBERT is a compelling option for edge\napplications.\nReferences\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. In NAACL-HLT , 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\nunsupervised multitask learners. 2019.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar S. Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke S. Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv ,\nabs/1907.11692, 2019.\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green ai. ArXiv , abs/1907.10597, 2019.\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in\nnlp. In ACL, 2019.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In NIPS , 2017.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. Transformers: State-of-the-art natural language\nprocessing, 2019.\nCristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In KDD , 2006.\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. ArXiv ,\nabs/1503.02531, 2015.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading\nbooks. 2015 IEEE International Conference on Computer Vision (ICCV) , pages 19–27, 2015.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A\nmulti-task benchmark and analysis platform for natural language understanding. In ICLR , 2018.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. Deep contextualized word representations. In NAACL , 2018.\nAlex Wang, Ian F. Tenney, Yada Pruksachatkun, Katherin Yu, Jan Hula, Patrick Xia, Raghu Pappagari, Shuning\nJin, R. Thomas McCoy, Roma Patel, Yinghui Huang, Jason Phang, Edouard Grave, Najoung Kim, Phu Mon\nHtut, Thibault F’evry, Berlin Chen, Nikita Nangia, Haokun Liu, Anhad Mohananey, Shikha Bordia, Nicolas\nPatry, Ellie Pavlick, and Samuel R. Bowman. jiant 1.1: A software toolkit for research on general-purpose\ntext understanding models. http://jiant.info/ , 2019.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y . Ng, and Christopher Potts. Learning\nword vectors for sentiment analysis. In ACL, 2011.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for machine\ncomprehension of text. In EMNLP , 2016.\nRaphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. Distilling task-speciﬁc\nknowledge from bert into simple neural networks. ArXiv , abs/1903.12136, 2019.\nDebajyoti Chatterjee. Making neural machine reading comprehension faster. ArXiv , abs/1904.00796, 2019.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better: The impact\nof student initialization on knowledge distillation. ArXiv , abs/1908.08962, 2019.\nZe Yang, Linjun Shou, Ming Gong, Wutao Lin, and Daxin Jiang. Model compression with multi-task knowledge\ndistillation for web-scale question answering system. ArXiv , abs/1904.09636, 2019.'
 'DistilBERT, a distilled version of BERT: smaller,\nfaster, cheaper and lighter\nVictor SANH, Lysandre DEBUT, Julien CHAUMOND, Thomas WOLF\nHugging Face\n{victor,lysandre,julien,thomas}@huggingface.co\nAbstract\nAs Transfer Learning from large-scale pre-trained models becomes more prevalent\nin Natural Language Processing (NLP), operating these large models in on-the-\nedge and/or under constrained computational training or inference budgets remains\nchallenging. In this work, we propose a method to pre-train a smaller general-\npurpose language representation model, called DistilBERT, which can then be ﬁne-\ntuned with good performances on a wide range of tasks like its larger counterparts.\nWhile most prior work investigated the use of distillation for building task-speciﬁc\nmodels, we leverage knowledge distillation during the pre-training phase and show\nthat it is possible to reduce the size of a BERT model by 40%, while retaining 97%\nof its language understanding capabilities and being 60% faster. To leverage the\ninductive biases learned by larger models during pre-training, we introduce a triple\nloss combining language modeling, distillation and cosine-distance losses. Our\nsmaller, faster and lighter model is cheaper to pre-train and we demonstrate its\ncapabilities for on-device computations in a proof-of-concept experiment and a\ncomparative on-device study.\n1 Introduction\nFigure 1: Parameter counts of several recently released\npretrained language models.The last two years have seen the rise\nof Transfer Learning approaches in\nNatural Language Processing (NLP)\nwith large-scale pre-trained language\nmodels becoming a basic tool in\nmany NLP tasks [Devlin et al., 2018,\nRadford et al., 2019, Liu et al., 2019].\nWhile these models lead to signiﬁ-\ncant improvement, they often have\nseveral hundred million parameters\nand current research1on pre-trained\nmodels indicates that training even\nlarger models still leads to better per-\nformances on downstream tasks.\nThe trend toward bigger models\nraises several concerns. First is the\nenvironmental cost of exponentially scaling these models’ computational requirements as mentioned\nin Schwartz et al. [2019], Strubell et al. [2019]. Second, while operating these models on-device\nin real-time has the potential to enable novel and interesting language processing applications, the\ngrowing computational and memory requirements of these models may hamper wide adoption.\n1See for instance the recently released MegatronLM ( https://nv-adlr.github.io/MegatronLM )\nEMC^2: 5th Edition Co-located with NeurIPS’19arXiv:1910.01108v4  [cs.CL]  1 Mar 2020']","DistilBERT closely approaches BERT's performance on IMDb sentiment classification and SQuAD v1.1, with minimal performance loss despite its smaller size.",0.8055555555287036,0.9166666666666666,0.9259178029954684,1.0,0.874090233907506,0.9249323642014526
"What modifications to the BERT pretraining process are introduced in RoBERTa, and how do they collectively enhance model performance?","RoBERTa introduces several modifications to the BERT pretraining process: it trains the model longer with larger batches and more data, removes the next sentence prediction objective, trains on longer sequences, and dynamically changes the masking pattern applied to the training data. Collectively, these enhancements lead to improved performance on downstream tasks by optimizing the training procedure and leveraging more data effectively. As a result, RoBERTa achieves state-of-the-art results on benchmarks like GLUE, RACE, and SQuAD.","['arXiv:1907.11692v1  [cs.CL]  26 Jul 2019RoBERTa: A Robustly Optimized BERT Pretraining Approach\nYinhan Liu∗§Myle Ott∗§Naman Goyal∗§Jingfei Du∗§Mandar Joshi†\nDanqi Chen§Omer Levy§Mike Lewis§Luke Zettlemoyer†§Veselin Stoyanov§\n†Paul G. Allen School of Computer Science & Engineering,\nUniversity of Washington, Seattle, WA\n{mandar90,lsz }@cs.washington.edu\n§Facebook AI\n{yinhanliu,myleott,naman,jingfeidu,\ndanqi,omerlevy,mikelewis,lsz,ves }@fb.com\nAbstract\nLanguage model pretraining has led to sig-\nniﬁcant performance gains but careful com-\nparison between different approaches is chal-\nlenging. Training is computationally expen-\nsive, often done on private datasets of different\nsizes, and, as we will show, hyperparameter\nchoices have signiﬁcant impact on the ﬁnal re-\nsults. We present a replication study of BERT\npretraining ( Devlin et al. ,2019 ) that carefully\nmeasures the impact of many key hyperparam-\neters and training data size. We ﬁnd that BERT\nwas signiﬁcantly undertrained, and can match\nor exceed the performance of every model\npublished after it. Our best model achieves\nstate-of-the-art results on GLUE, RACE and\nSQuAD. These results highlight the impor-\ntance of previously overlooked design choices,\nand raise questions about the source of re-\ncently reported improvements. We release our\nmodels and code.1\n1 Introduction\nSelf-training methods such as ELMo ( Peters et al. ,\n2018 ), GPT ( Radford et al. ,2018 ), BERT\n(Devlin et al. ,2019 ), XLM ( Lample and Conneau ,\n2019 ), and XLNet ( Yang et al. ,2019 ) have\nbrought signiﬁcant performance gains, but it can\nbe challenging to determine which aspects of\nthe methods contribute the most. Training is\ncomputationally expensive, limiting the amount\nof tuning that can be done, and is often done with\nprivate training data of varying sizes, limiting\nour ability to measure the effects of the modeling\nadvances.\n∗Equal contribution.\n1Our models and code are available at:\nhttps://github.com/pytorch/fairseqWe present a replication study of BERT pre-\ntraining ( Devlin et al. ,2019 ), which includes a\ncareful evaluation of the effects of hyperparmeter\ntuning and training set size. We ﬁnd that BERT\nwas signiﬁcantly undertrained and propose an im-\nproved recipe for training BERT models, which\nwe call RoBERTa, that can match or exceed the\nperformance of all of the post-BERT methods.\nOur modiﬁcations are simple, they include: (1)\ntraining the model longer, with bigger batches,\nover more data; (2) removing the next sentence\nprediction objective; (3) training on longer se-\nquences; and (4) dynamically changing the mask-\ning pattern applied to the training data. We also\ncollect a large new dataset (CC-N EWS) of compa-\nrable size to other privately used datasets, to better\ncontrol for training set size effects.\nWhen controlling for training data, our im-\nproved training procedure improves upon the pub-\nlished BERT results on both GLUE and SQuAD.\nWhen trained for longer over additional data, our\nmodel achieves a score of 88.5 on the public\nGLUE leaderboard, matching the 88.4 reported\nbyYang et al. (2019 ). Our model establishes a\nnew state-of-the-art on 4/9 of the GLUE tasks:\nMNLI, QNLI, RTE and STS-B. We also match\nstate-of-the-art results on SQuAD and RACE.\nOverall, we re-establish that BERT’s masked lan-\nguage model training objective is competitive\nwith other recently proposed training objectives\nsuch as perturbed autoregressive language model-\ning (Yang et al. ,2019 ).2\nIn summary, the contributions of this paper\nare: (1) We present a set of important BERT de-\nsign choices and training strategies and introduce\n2It is possible that these other methods could also improve\nwith more tuning. We leave this exploration to future work.'
 'alternatives that lead to better downstream task\nperformance; (2) We use a novel dataset, CC-\nNEWS, and conﬁrm that using more data for pre-\ntraining further improves performance on down-\nstream tasks; (3) Our training improvements show\nthat masked language model pretraining, under\nthe right design choices, is competitive with all\nother recently published methods. We release our\nmodel, pretraining and ﬁne-tuning code imple-\nmented in PyTorch ( Paszke et al. ,2017 ).\n2 Background\nIn this section, we give a brief overview of the\nBERT ( Devlin et al. ,2019 ) pretraining approach\nand some of the training choices that we will ex-\namine experimentally in the following section.\n2.1 Setup\nBERT takes as input a concatenation of two\nsegments (sequences of tokens), x1,...,x N\nandy1,...,yM. Segments usually consist of\nmore than one natural sentence. The two seg-\nments are presented as a single input sequence\nto BERT with special tokens delimiting them:\n[CLS],x1,...,x N,[SEP],y1,...,yM,[EOS].\nMandNare constrained such that M+N < T ,\nwhereTis a parameter that controls the maximum\nsequence length during training.\nThe model is ﬁrst pretrained on a large unla-\nbeled text corpus and subsequently ﬁnetuned us-\ning end-task labeled data.\n2.2 Architecture\nBERT uses the now ubiquitous transformer archi-\ntecture ( Vaswani et al. ,2017 ), which we will not\nreview in detail. We use a transformer architecture\nwithLlayers. Each block uses Aself-attention\nheads and hidden dimension H.\n2.3 Training Objectives\nDuring pretraining, BERT uses two objectives:\nmasked language modeling and next sentence pre-\ndiction.\nMasked Language Model (MLM) A random\nsample of the tokens in the input sequence is\nselected and replaced with the special token\n[MASK]. The MLM objective is a cross-entropy\nloss on predicting the masked tokens. BERT uni-\nformly selects 15% of the input tokens for possi-\nble replacement. Of the selected tokens, 80% are\nreplaced with [MASK], 10% are left unchanged,and 10% are replaced by a randomly selected vo-\ncabulary token.\nIn the original implementation, random mask-\ning and replacement is performed once in the be-\nginning and saved for the duration of training, al-\nthough in practice, data is duplicated so the mask\nis not always the same for every training sentence\n(see Section 4.1).\nNext Sentence Prediction (NSP) NSP is a bi-\nnary classiﬁcation loss for predicting whether two\nsegments follow each other in the original text.\nPositive examples are created by taking consecu-\ntive sentences from the text corpus. Negative ex-\namples are created by pairing segments from dif-\nferent documents. Positive and negative examples\nare sampled with equal probability.\nThe NSP objective was designed to improve\nperformance on downstream tasks, such as Natural\nLanguage Inference ( Bowman et al. ,2015 ), which\nrequire reasoning about the relationships between\npairs of sentences.\n2.4 Optimization\nBERT is optimized with Adam ( Kingma and Ba ,\n2015 ) using the following parameters: β1= 0.9,\nβ2= 0.999,ǫ=1e-6 and L2weight de-\ncay of0.01. The learning rate is warmed up\nover the ﬁrst 10,000 steps to a peak value of\n1e-4, and then linearly decayed. BERT trains\nwith a dropout of 0.1 on all layers and at-\ntention weights, and a GELU activation func-\ntion ( Hendrycks and Gimpel ,2016 ). Models are\npretrained for S=1,000,000 updates, with mini-\nbatches containing B=256 sequences of maxi-\nmum length T=512 tokens.\n2.5 Data\nBERT is trained on a combination of B OOK COR-\nPUS (Zhu et al. ,2015 ) plus English W IKIPEDIA ,\nwhich totals 16GB of uncompressed text.3\n3 Experimental Setup\nIn this section, we describe the experimental setup\nfor our replication study of BERT.\n3.1 Implementation\nWe reimplement BERT in FAIRSEQ (Ott et al. ,\n2019 ). We primarily follow the original BERT\n3Yang et al. (2019 ) use the same dataset but report having\nonly 13GB of text after data cleaning. This is most likely due\nto subtle differences in cleaning of the Wikipedia data.'
 '7 Conclusion\nWe carefully evaluate a number of design de-\ncisions when pretraining BERT models. We\nﬁnd that performance can be substantially im-\nproved by training the model longer, with bigger\nbatches over more data; removing the next sen-\ntence prediction objective; training on longer se-\nquences; and dynamically changing the masking\npattern applied to the training data. Our improved\npretraining procedure, which we call RoBERTa,\nachieves state-of-the-art results on GLUE, RACE\nand SQuAD, without multi-task ﬁnetuning for\nGLUE or additional data for SQuAD. These re-\nsults illustrate the importance of these previ-\nously overlooked design decisions and suggest\nthat BERT’s pretraining objective remains com-\npetitive with recently proposed alternatives.\nWe additionally use a novel dataset,\nCC-N EWS, and release our models and\ncode for pretraining and ﬁnetuning at:\nhttps://github.com/pytorch/fairseq .\nReferences\nEneko Agirre, Llu’is M‘arquez, and Richard Wicen-\ntowski, editors. 2007. Proceedings of the Fourth\nInternational Workshop on Semantic Evaluations\n(SemEval-2007) .\nAlexei Baevski, Sergey Edunov, Yinhan Liu, Luke\nZettlemoyer, and Michael Auli. 2019. Cloze-\ndriven pretraining of self-attention networks. arXiv\npreprint arXiv:1903.07785 .\nRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,\nDanilo Giampiccolo, Bernardo Magnini, and Idan\nSzpektor. 2006. The second PASCAL recognising\ntextual entailment challenge. In Proceedings of the\nsecond PASCAL challenges workshop on recognis-\ning textual entailment .\nLuisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo\nGiampiccolo, and Bernardo Magnini. 2009. The\nﬁfth PASCAL recognizing textual entailment chal-\nlenge.\nSamuel R Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nInEmpirical Methods in Natural Language Process-\ning (EMNLP) .\nWilliam Chan, Nikita Kitaev, Kelvin Guu, Mitchell\nStern, and Jakob Uszkoreit. 2019. KERMIT: Gener-\native insertion-based modeling for sequences. arXiv\npreprint arXiv:1906.01604 .Ido Dagan, Oren Glickman, and Bernardo Magnini.\n2006. The PASCAL recognising textual entailment\nchallenge. In Machine learning challenges. evalu-\nating predictive uncertainty, visual object classiﬁca-\ntion, and recognising tectual entailment .\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in Neural Informa-\ntion Processing Systems (NIPS) .\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In North American Association for Com-\nputational Linguistics (NAACL) .\nWilliam B Dolan and Chris Brockett. 2005. Auto-\nmatically constructing a corpus of sentential para-\nphrases. In Proceedings of the International Work-\nshop on Paraphrasing .\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei,\nXiaodong Liu, Yu Wang, Jianfeng Gao, Ming\nZhou, and Hsiao-Wuen Hon. 2019. Uniﬁed\nlanguage model pre-training for natural language\nunderstanding and generation. arXiv preprint\narXiv:1905.03197 .\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand Bill Dolan. 2007. The third PASCAL recog-\nnizing textual entailment challenge. In Proceedings\nof the ACL-PASCAL workshop on textual entailment\nand paraphrasing .\nAaron Gokaslan and Vanya Cohen. 2019. Openweb-\ntext corpus. http://web.archive.org/\nsave/http://Skylion007.github.io/\nOpenWebTextCorpus .\nFelix Hamborg, Norman Meuschke, Corinna Bre-\nitinger, and Bela Gipp. 2017. news-please: A\ngeneric news crawler and extractor. In Proceedings\nof the 15th International Symposium of Information\nScience .\nDan Hendrycks and Kevin Gimpel. 2016. Gaus-\nsian error linear units (gelus). arXiv preprint\narXiv:1606.08415 .\nMatthew Honnibal and Ines Montani. 2017. spaCy 2:\nNatural language understanding with Bloom embed-\ndings, convolutional neural networks and incremen-\ntal parsing. To appear.\nJeremy Howard and Sebastian Ruder. 2018. Universal'
 'BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout }@google.com\nAbstract\nWe introduce a new language representa-\ntion model called BERT , which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be ﬁne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeciﬁc architecture modiﬁcations.\nBERT is conceptually simple and empirically\npowerful. It obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n1 Introduction\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce ﬁne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).There are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based andﬁne-tuning . The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speciﬁc architectures that\ninclude the pre-trained representations as addi-\ntional features. The ﬁne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speciﬁc parameters, and is trained on the\ndownstream tasks by simply ﬁne-tuning allpre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the ﬁne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying ﬁne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\nIn this paper, we improve the ﬁne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations from Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a “masked lan-\nguage model” (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the maskedarXiv:1810.04805v2  [cs.CL]  24 May 2019']","RoBERTa introduces several key modifications, including training with larger mini-batches, eliminating the next sentence prediction (NSP) loss, training on longer sequences, dynamically changing the masking pattern, and pretraining over more data. These improvements collectively enable RoBERTa to match or exceed the performance of all post-BERT models, achieving state-of-the-art results on benchmarks like GLUE, RACE, and SQuAD.",0.999999999975,1.0,0.9443237241374641,1.0,0.8399257497334898,0.9597441122605312
"What role does the novel dataset CC-NEWS play in RoBERTa's pretraining, and how does it compare to other datasets used?","The novel dataset CC-NEWS is used to enhance RoBERTa's pretraining by providing more data, which has been shown to improve performance on downstream tasks. In comparison to other datasets, such as CommonCrawl and C4, CC-NEWS contributes to the diversity and volume of training data, leading to better model performance. Overall, using more data, including CC-NEWS, during pretraining is beneficial for achieving competitive results.","['alternatives that lead to better downstream task\nperformance; (2) We use a novel dataset, CC-\nNEWS, and conﬁrm that using more data for pre-\ntraining further improves performance on down-\nstream tasks; (3) Our training improvements show\nthat masked language model pretraining, under\nthe right design choices, is competitive with all\nother recently published methods. We release our\nmodel, pretraining and ﬁne-tuning code imple-\nmented in PyTorch ( Paszke et al. ,2017 ).\n2 Background\nIn this section, we give a brief overview of the\nBERT ( Devlin et al. ,2019 ) pretraining approach\nand some of the training choices that we will ex-\namine experimentally in the following section.\n2.1 Setup\nBERT takes as input a concatenation of two\nsegments (sequences of tokens), x1,...,x N\nandy1,...,yM. Segments usually consist of\nmore than one natural sentence. The two seg-\nments are presented as a single input sequence\nto BERT with special tokens delimiting them:\n[CLS],x1,...,x N,[SEP],y1,...,yM,[EOS].\nMandNare constrained such that M+N < T ,\nwhereTis a parameter that controls the maximum\nsequence length during training.\nThe model is ﬁrst pretrained on a large unla-\nbeled text corpus and subsequently ﬁnetuned us-\ning end-task labeled data.\n2.2 Architecture\nBERT uses the now ubiquitous transformer archi-\ntecture ( Vaswani et al. ,2017 ), which we will not\nreview in detail. We use a transformer architecture\nwithLlayers. Each block uses Aself-attention\nheads and hidden dimension H.\n2.3 Training Objectives\nDuring pretraining, BERT uses two objectives:\nmasked language modeling and next sentence pre-\ndiction.\nMasked Language Model (MLM) A random\nsample of the tokens in the input sequence is\nselected and replaced with the special token\n[MASK]. The MLM objective is a cross-entropy\nloss on predicting the masked tokens. BERT uni-\nformly selects 15% of the input tokens for possi-\nble replacement. Of the selected tokens, 80% are\nreplaced with [MASK], 10% are left unchanged,and 10% are replaced by a randomly selected vo-\ncabulary token.\nIn the original implementation, random mask-\ning and replacement is performed once in the be-\nginning and saved for the duration of training, al-\nthough in practice, data is duplicated so the mask\nis not always the same for every training sentence\n(see Section 4.1).\nNext Sentence Prediction (NSP) NSP is a bi-\nnary classiﬁcation loss for predicting whether two\nsegments follow each other in the original text.\nPositive examples are created by taking consecu-\ntive sentences from the text corpus. Negative ex-\namples are created by pairing segments from dif-\nferent documents. Positive and negative examples\nare sampled with equal probability.\nThe NSP objective was designed to improve\nperformance on downstream tasks, such as Natural\nLanguage Inference ( Bowman et al. ,2015 ), which\nrequire reasoning about the relationships between\npairs of sentences.\n2.4 Optimization\nBERT is optimized with Adam ( Kingma and Ba ,\n2015 ) using the following parameters: β1= 0.9,\nβ2= 0.999,ǫ=1e-6 and L2weight de-\ncay of0.01. The learning rate is warmed up\nover the ﬁrst 10,000 steps to a peak value of\n1e-4, and then linearly decayed. BERT trains\nwith a dropout of 0.1 on all layers and at-\ntention weights, and a GELU activation func-\ntion ( Hendrycks and Gimpel ,2016 ). Models are\npretrained for S=1,000,000 updates, with mini-\nbatches containing B=256 sequences of maxi-\nmum length T=512 tokens.\n2.5 Data\nBERT is trained on a combination of B OOK COR-\nPUS (Zhu et al. ,2015 ) plus English W IKIPEDIA ,\nwhich totals 16GB of uncompressed text.3\n3 Experimental Setup\nIn this section, we describe the experimental setup\nfor our replication study of BERT.\n3.1 Implementation\nWe reimplement BERT in FAIRSEQ (Ott et al. ,\n2019 ). We primarily follow the original BERT\n3Yang et al. (2019 ) use the same dataset but report having\nonly 13GB of text after data cleaning. This is most likely due\nto subtle differences in cleaning of the Wikipedia data.'
 'Published as a conference paper at ICLR 2023\nPre-Training. We provide the complete training notes, Tensorboard logs, and code for our pre-\ntraining in our repository (Cf. Abstract). The pre-training hyper-parameters and cluster configu-\nration are provided in Section 2.3 and Table 11. The training corpora composition and details for\nMulti-task Instruction Pre-training are provided in Section 2.2 and Appendix C.1 and C.2.\nEvaluation. We organize all the evaluation, including language benchmarks (LAMBADA, Pile,\nMMLU, BIG-bench, CLUE, and FewCLUE) and ethical benchmarks (CrowS-Pairs, StereoSet,\nETHOS, RealToxicPrompts), into one-command-to-run bash scripts in our code repository. Data\nprocessing details for language modeling benchmarks are provided in Section 5.1 and Appendix C.4,\nfor MMLU are provided in Section 5.2 and Appendix C.6, for BIG-bench are provided in Section 5.3\nand Appendix C.5, for CLUE and FewCLUE are provided in 5.4. For all ethical evaluation, please\nrefer to Appendix A for details.\nREFERENCES\nOshin Agarwal, Heming Ge, Siamak Shakeri, and Rami Al-Rfou. Knowledge graph based synthetic\ncorpus generation for knowledge-enhanced language model pre-training. In Proceedings of the\n2021 Conference of the North American Chapter of the Association for Computational Linguis-\ntics: Human Language Technologies , pp. 3554–3565, 2021.\nVamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta,\nHonglei Zhuang, Vinh Q Tran, Dara Bahri, Jianmo Ni, et al. Ext5: Towards extreme multi-task\nscaling for transfer learning. In International Conference on Learning Representations , 2022.\nMikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victo-\nria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, et al. Efficient large scale language\nmodeling with mixtures of experts. arXiv preprint arXiv:2112.10684 , 2021.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450 , 2016.\nStephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V Nayak, Ab-\nheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Févry, et al. Promptsource: An integrated\ndevelopment environment and repository for natural language prompts. In Proceedings of the\n60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations ,\npp. 93–104, 2022.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the\ndangers of stochastic parrots: Can language models be too big? In FAccT ’21: 2021 ACM Con-\nference on Fairness, Accountability, and Transparency, Virtual Event / Toronto, Canada, March\n3-10, 2021 , pp. 610–623. ACM, 2021.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from\nquestion-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural\nlanguage processing , pp. 1533–1544, 2013.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical com-\nmonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence ,\nvolume 34, pp. 7432–7439, 2020.\nSidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Ho-\nrace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source\nautoregressive language model. In Proceedings of BigScience Episode \\# 5–Workshop on Chal-\nlenges & Perspectives in Creating Large Language Models , pp. 95–136, 2022.\nSu Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu, Robert Sim, and Hanna Wallach. Stereotyping\nnorwegian salmon: An inventory of pitfalls in fairness benchmark datasets. In Proceedings of the\n59th Annual Meeting of the Association for Computational Linguistics and the 11th International\nJoint Conference on Natural Language Processing (Volume 1: Long Papers) , pp. 1004–1015,\n2021.\n11'
 '(Zellers et al., 2018), this new task of common-\nsense natural language inference seemed trivial\nfor humans (88%) and yet challenging for then-\nstate-of-the-art models ( ă60%), including ELMo\n(Peters et al., 2018). However, BERT (Devlin\net al., 2018) soon reached over 86%, almost\nhuman-level performance. One news article on\nthis development was headlined “ ﬁnally, a ma-\nchine that can ﬁnish your sentence. ”1\nIn this paper, we investigate the following ques-\ntion: How well do deep pretrained models, like\n1A New York Times article at https: //nyti.ms /2DycutY.\n1arXiv:1905.07830v1  [cs.CL]  19 May 2019'
 '2 Approach\nOur training approach is similar to the methods\ndescribed in previous work (Brown et al., 2020;\nChowdhery et al., 2022), and is inspired by the\nChinchilla scaling laws (Hoffmann et al., 2022).\nWe train large transformers on a large quantity of\ntextual data using a standard optimizer.\n2.1 Pre-training Data\nOur training dataset is a mixture of several sources,\nreported in Table 1, that cover a diverse set of do-\nmains. For the most part, we reuse data sources\nthat have been leveraged to train other LLMs, with\nthe restriction of only using data that is publicly\navailable, and compatible with open sourcing. This\nleads to the following mixture of data and the per-\ncentage they represent in the training set:\nEnglish CommonCrawl [67%]. We preprocess\nﬁve CommonCrawl dumps, ranging from 2017\nto 2020, with the CCNet pipeline (Wenzek et al.,\n2020). This process deduplicates the data at the\nline level, performs language identiﬁcation with\na fastText linear classiﬁer to remove non-English\npages and ﬁlters low quality content with an n-\ngram language model. In addition, we trained a\nlinear model to classify pages used as references\nin Wikipedia v.s.randomly sampled pages, and\ndiscarded pages not classiﬁed as references.\nC4 [15%]. During exploratory experiments, we\nobserved that using diverse pre-processed Com-\nmonCrawl datasets improves performance. We thus\nincluded the publicly available C4 dataset (Raffel\net al., 2020) in our data. The preprocessing of C4\nalso contains deduplication and language identiﬁ-\ncation steps: the main difference with CCNet is\nthe quality ﬁltering, which mostly relies on heuris-\ntics such as presence of punctuation marks or the\nnumber of words and sentences in a webpage.\nGithub [4.5%]. We use the public GitHub\ndataset available on Google BigQuery. We only\nkept projects that are distributed under the Apache,\nBSD and MIT licenses. Additionally, we ﬁltered\nlow quality ﬁles with heuristics based on the line\nlength or proportion of alphanumeric characters,\nand removed boilerplate, such as headers, with reg-\nular expressions. Finally, we deduplicate the result-\ning dataset at the ﬁle level, with exact matches.\nWikipedia [4.5%]. We add Wikipedia dumps\nfrom the June-August 2022 period, covering 20Dataset Sampling prop. Epochs Disk size\nCommonCrawl 67.0% 1.10 3.3 TB\nC4 15.0% 1.06 783 GB\nGithub 4.5% 0.64 328 GB\nWikipedia 4.5% 2.45 83 GB\nBooks 4.5% 2.23 85 GB\nArXiv 2.5% 1.06 92 GB\nStackExchange 2.0% 1.03 78 GB\nTable 1: Pre-training data. Data mixtures used for pre-\ntraining, for each subset we list the sampling propor-\ntion, number of epochs performed on the subset when\ntraining on 1.4T tokens, and disk size. The pre-training\nruns on 1T tokens have the same sampling proportion.\nlanguages, which use either the Latin or Cyrillic\nscripts: bg,ca,cs,da,de,en,es,fr,hr,hu,it,\nnl,pl,pt,ro,ru,sl,sr,sv,uk. We process the\ndata to remove hyperlinks, comments and other\nformatting boilerplate.\nGutenberg and Books3 [4.5%]. We include\ntwo book corpora in our training dataset: the Guten-\nberg Project, which contains books that are in the\npublic domain, and the Books3 section of TheP-\nile (Gao et al., 2020), a publicly available dataset\nfor training large language models. We perform\ndeduplication at the book level, removing books\nwith more than 90% content overlap.\nArXiv [2.5%]. We process arXiv Latex ﬁles\nto add scientiﬁc data to our dataset. Following\nLewkowycz et al. (2022), we removed everything\nbefore the ﬁrst section, as well as the bibliography.\nWe also removed the comments from the .tex ﬁles,\nand inline-expanded deﬁnitions and macros written\nby users to increase consistency across papers.\nStack Exchange [2%]. We include a dump of\nStack Exchange, a website of high quality ques-\ntions and answers that covers a diverse set of do-\nmains, ranging from computer science to chemistry.\nWe kept the data from the 28 largest websites, re-\nmoved the HTML tags from text and sorted the\nanswers by score (from highest to lowest).']","CC-NEWS, a large dataset collected from English news articles, provides a significant source of diverse and recent textual content for RoBERTa's pretraining, complementing other datasets and ensuring a broad coverage of language use and topics. This dataset helps control for training set size effects and contributes to the robustness and generalization capability of the model.",0.7499999999625,0.4444444444444444,0.9325503740122846,0.5,0.665877414179754,0.9492239424333018
Describe the process and purpose of the 'Next Sentence Prediction' task in BERT's pre-training.,"The 'Next Sentence Prediction' (NSP) task in BERT's pre-training involves predicting whether a given sentence B follows sentence A in the original text. This is achieved by creating positive examples from consecutive sentences and negative examples from random sentence pairs, with each type sampled equally. The purpose of NSP is to help the model understand the relationships between sentences, which is crucial for downstream tasks like Question Answering and Natural Language Inference.","['Many important downstream tasks such as Ques-\ntion Answering (QA) and Natural Language Infer-\nence (NLI) are based on understanding the rela-\ntionship between two sentences, which is not di-\nrectly captured by language modeling. In order\nto train a model that understands sentence rela-\ntionships, we pre-train for a binarized next sen-\ntence prediction task that can be trivially gener-\nated from any monolingual corpus. Speciﬁcally,\nwhen choosing the sentences AandBfor each pre-\ntraining example, 50% of the time Bis the actual\nnext sentence that follows A(labeled as IsNext ),\nand 50% of the time it is a random sentence from\nthe corpus (labeled as NotNext ). As we show\nin Figure 1, Cis used for next sentence predic-\ntion (NSP).5Despite its simplicity, we demon-\nstrate in Section 5.1 that pre-training towards this\ntask is very beneﬁcial to both QA and NLI.6\n5The ﬁnal model achieves 97%-98% accuracy on NSP.\n6The vector Cis not a meaningful sentence representation\nwithout ﬁne-tuning, since it was trained with NSP.'
 'Input/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g.,⟨Question, Answer⟩) in one token sequence.\nThroughout this work, a “sentence” can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A “sequence” refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The ﬁrst\ntoken of every sequence is always a special clas-\nsiﬁcation token ( [CLS] ). The ﬁnal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classiﬁcation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ( [SEP] ). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence Aor sentence B. As shown in Figure 1,\nwe denote input embedding as E, the ﬁnal hidden\nvector of the special [CLS] token asC∈RH,\nand the ﬁnal hidden vector for the ithinput token\nasTi∈RH.\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n3.1 Pre-training BERT\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right orright-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly “see itself”, and the model could trivially\npredict the target word in a multi-layered context.\nformer is often referred to as a “Transformer encoder” while\nthe left-context-only version is referred to as a “Transformer\ndecoder” since it can be used for text generation.In order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a “masked\nLM” (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the ﬁnal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.\nAlthough this allows us to obtain a bidirec-\ntional pre-trained model, a downside is that we\nare creating a mismatch between pre-training and\nﬁne-tuning, since the [MASK] token does not ap-\npear during ﬁne-tuning. To mitigate this, we do\nnot always replace “masked” words with the ac-\ntual[MASK] token. The training data generator\nchooses 15% of the token positions at random for\nprediction. If the i-th token is chosen, we replace\nthei-th token with (1) the [MASK] token 80% of\nthe time (2) a random token 10% of the time (3)\nthe unchanged i-th token 10% of the time. Then,\nTiwill be used to predict the original token with\ncross entropy loss. We compare variations of this\nprocedure in Appendix C.2.\nTask #2: Next Sentence Prediction (NSP)\nMany important downstream tasks such as Ques-\ntion Answering (QA) and Natural Language Infer-\nence (NLI) are based on understanding the rela-\ntionship between two sentences, which is not di-'
 'alternatives that lead to better downstream task\nperformance; (2) We use a novel dataset, CC-\nNEWS, and conﬁrm that using more data for pre-\ntraining further improves performance on down-\nstream tasks; (3) Our training improvements show\nthat masked language model pretraining, under\nthe right design choices, is competitive with all\nother recently published methods. We release our\nmodel, pretraining and ﬁne-tuning code imple-\nmented in PyTorch ( Paszke et al. ,2017 ).\n2 Background\nIn this section, we give a brief overview of the\nBERT ( Devlin et al. ,2019 ) pretraining approach\nand some of the training choices that we will ex-\namine experimentally in the following section.\n2.1 Setup\nBERT takes as input a concatenation of two\nsegments (sequences of tokens), x1,...,x N\nandy1,...,yM. Segments usually consist of\nmore than one natural sentence. The two seg-\nments are presented as a single input sequence\nto BERT with special tokens delimiting them:\n[CLS],x1,...,x N,[SEP],y1,...,yM,[EOS].\nMandNare constrained such that M+N < T ,\nwhereTis a parameter that controls the maximum\nsequence length during training.\nThe model is ﬁrst pretrained on a large unla-\nbeled text corpus and subsequently ﬁnetuned us-\ning end-task labeled data.\n2.2 Architecture\nBERT uses the now ubiquitous transformer archi-\ntecture ( Vaswani et al. ,2017 ), which we will not\nreview in detail. We use a transformer architecture\nwithLlayers. Each block uses Aself-attention\nheads and hidden dimension H.\n2.3 Training Objectives\nDuring pretraining, BERT uses two objectives:\nmasked language modeling and next sentence pre-\ndiction.\nMasked Language Model (MLM) A random\nsample of the tokens in the input sequence is\nselected and replaced with the special token\n[MASK]. The MLM objective is a cross-entropy\nloss on predicting the masked tokens. BERT uni-\nformly selects 15% of the input tokens for possi-\nble replacement. Of the selected tokens, 80% are\nreplaced with [MASK], 10% are left unchanged,and 10% are replaced by a randomly selected vo-\ncabulary token.\nIn the original implementation, random mask-\ning and replacement is performed once in the be-\nginning and saved for the duration of training, al-\nthough in practice, data is duplicated so the mask\nis not always the same for every training sentence\n(see Section 4.1).\nNext Sentence Prediction (NSP) NSP is a bi-\nnary classiﬁcation loss for predicting whether two\nsegments follow each other in the original text.\nPositive examples are created by taking consecu-\ntive sentences from the text corpus. Negative ex-\namples are created by pairing segments from dif-\nferent documents. Positive and negative examples\nare sampled with equal probability.\nThe NSP objective was designed to improve\nperformance on downstream tasks, such as Natural\nLanguage Inference ( Bowman et al. ,2015 ), which\nrequire reasoning about the relationships between\npairs of sentences.\n2.4 Optimization\nBERT is optimized with Adam ( Kingma and Ba ,\n2015 ) using the following parameters: β1= 0.9,\nβ2= 0.999,ǫ=1e-6 and L2weight de-\ncay of0.01. The learning rate is warmed up\nover the ﬁrst 10,000 steps to a peak value of\n1e-4, and then linearly decayed. BERT trains\nwith a dropout of 0.1 on all layers and at-\ntention weights, and a GELU activation func-\ntion ( Hendrycks and Gimpel ,2016 ). Models are\npretrained for S=1,000,000 updates, with mini-\nbatches containing B=256 sequences of maxi-\nmum length T=512 tokens.\n2.5 Data\nBERT is trained on a combination of B OOK COR-\nPUS (Zhu et al. ,2015 ) plus English W IKIPEDIA ,\nwhich totals 16GB of uncompressed text.3\n3 Experimental Setup\nIn this section, we describe the experimental setup\nfor our replication study of BERT.\n3.1 Implementation\nWe reimplement BERT in FAIRSEQ (Ott et al. ,\n2019 ). We primarily follow the original BERT\n3Yang et al. (2019 ) use the same dataset but report having\nonly 13GB of text after data cleaning. This is most likely due\nto subtle differences in cleaning of the Wikipedia data.'
 'BERT (Ours) \nTrm Trm Trm\nTrm Trm Trm...\n...Trm Trm Trm\nTrm Trm Trm...\n...OpenAI GPT \nLstm ELMo \nLstm Lstm \nLstm Lstm Lstm Lstm Lstm Lstm \nLstm Lstm Lstm  T1 T2 TN...\n...\n......\n...\n E1 E2 EN... T1 T2TN...\n E1 E2 EN ... T1 T2 TN...\n E1 E2 EN...Figure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT\nuses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to-\nleft LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly\nconditioned on both left and right context in all layers. In addition to the architecture differences, BERT and\nOpenAI GPT are ﬁne-tuning approaches, while ELMo is a feature-based approach.\nto converge. In Section C.1 we demonstrate that\nMLM does converge marginally slower than a left-\nto-right model (which predicts every token), but\nthe empirical improvements of the MLM model\nfar outweigh the increased training cost.\nNext Sentence Prediction The next sentence\nprediction task can be illustrated in the following\nexamples.\nInput =[CLS] the man went to [MASK] store [SEP]\nhe bought a gallon [MASK] milk [SEP]\nLabel =IsNext\nInput =[CLS] the man [MASK] to the store [SEP]\npenguin [MASK] are flight ##less birds [SEP]\nLabel =NotNext\nA.2 Pre-training Procedure\nTo generate each training input sequence, we sam-\nple two spans of text from the corpus, which we\nrefer to as “sentences” even though they are typ-\nically much longer than single sentences (but can\nbe shorter also). The ﬁrst sentence receives the A\nembedding and the second receives the Bembed-\nding. 50% of the time Bis the actual next sentence\nthat follows Aand 50% of the time it is a random\nsentence, which is done for the “next sentence pre-\ndiction” task. They are sampled such that the com-\nbined length is≤512 tokens. The LM masking is\napplied after WordPiece tokenization with a uni-\nform masking rate of 15%, and no special consid-\neration given to partial word pieces.\nWe train with batch size of 256 sequences (256\nsequences * 512 tokens = 128,000 tokens/batch)\nfor 1,000,000 steps, which is approximately 40epochs over the 3.3 billion word corpus. We\nuse Adam with learning rate of 1e-4, β1= 0.9,\nβ2= 0.999, L2 weight decay of 0.01, learning\nrate warmup over the ﬁrst 10,000 steps, and linear\ndecay of the learning rate. We use a dropout prob-\nability of 0.1 on all layers. We use a gelu acti-\nvation (Hendrycks and Gimpel, 2016) rather than\nthe standard relu , following OpenAI GPT. The\ntraining loss is the sum of the mean masked LM\nlikelihood and the mean next sentence prediction\nlikelihood.\nTraining of BERT BASE was performed on 4\nCloud TPUs in Pod conﬁguration (16 TPU chips\ntotal).13Training of BERT LARGE was performed\non 16 Cloud TPUs (64 TPU chips total). Each pre-\ntraining took 4 days to complete.\nLonger sequences are disproportionately expen-\nsive because attention is quadratic to the sequence\nlength. To speed up pretraing in our experiments,\nwe pre-train the model with sequence length of\n128 for 90% of the steps. Then, we train the rest\n10% of the steps of sequence of 512 to learn the\npositional embeddings.\nA.3 Fine-tuning Procedure\nFor ﬁne-tuning, most model hyperparameters are\nthe same as in pre-training, with the exception of\nthe batch size, learning rate, and number of train-\ning epochs. The dropout probability was always\nkept at 0.1. The optimal hyperparameter values\nare task-speciﬁc, but we found the following range\nof possible values to work well across all tasks:\n•Batch size : 16, 32\n13https://cloudplatform.googleblog.com/2018/06/Cloud-\nTPU-now-offers-preemptible-pricing-and-global-\navailability.html']","NSP involves predicting whether a sentence B is the actual next sentence that follows sentence A, facilitating understanding of sentence relationships.",0.999999999975,1.0,0.9339493464519023,1.0,0.6058324558242244,0.9233298232968977
"What performance improvements does LLaMA-13B show over GPT-3, and how does LLaMA-65B stand in comparison to Chinchilla-70B and PaLM-540B?","LLaMA-13B outperforms GPT-3 on most benchmarks despite being more than 10 times smaller. Additionally, LLaMA-65B achieves competitive performance with both Chinchilla-70B and PaLM-540B, surpassing them in several areas. Overall, LLaMA models demonstrate significant improvements in efficiency and effectiveness compared to their larger counterparts.","['HellaSwag (Zellers et al., 2019), WinoGrande (Sak-\naguchi et al., 2021), ARC easy and challenge (Clark\net al., 2018) and OpenBookQA (Mihaylov et al.,\n2018). These datasets include Cloze and Winograd\nstyle tasks, as well as multiple choice question an-\nswering. We evaluate in the zero-shot setting as\ndone in the language modeling community.\nIn Table 3, we compare with existing models\nof various sizes and report numbers from the cor-\nresponding papers. First, LLaMA-65B outper-\nforms Chinchilla-70B on all reported benchmarks\nbut BoolQ. Similarly, this model surpasses PaLM-\n540B everywhere but on BoolQ and WinoGrande.\nLLaMA-13B model also outperforms GPT-3 on\nmost benchmarks despite being 10 ×smaller.\n3.2 Closed-book Question Answering\nWe compare LLaMA to existing large language\nmodels on two closed-book question answering\nbenchmarks: Natural Questions (Kwiatkowski\net al., 2019) and TriviaQA (Joshi et al., 2017). For\nboth benchmarks, we report exact match perfor-\nmance in a closed book setting, i.e., where the mod-\nels do not have access to documents that contain\nevidence to answer the question. In Table 4, we\nreport performance on NaturalQuestions, and in Ta-\nble 5, we report on TriviaQA. On both benchmarks,\nLLaMA-65B achieve state-of-the-arts performance\nin the zero-shot and few-shot settings. More im-\nportantly, the LLaMA-13B is also competitive on\nthese benchmarks with GPT-3 and Chinchilla, de-\nspite being 5-10 ×smaller. This model runs on a\nsingle V100 GPU during inference.\n0-shot 1-shot 5-shot 64-shot\nGopher 280B 43.5 - 57.0 57.2\nChinchilla 70B 55.4 - 64.1 64.6\nLLaMA7B 50.0 53.4 56.3 57.6\n13B 56.6 60.5 63.1 64.0\n33B 65.1 67.9 69.9 70.4\n65B 68.2 71.6 72.6 73.0\nTable 5: TriviaQA. Zero-shot and few-shot exact\nmatch performance on the ﬁltered dev set.\n3.3 Reading Comprehension\nWe evaluate our models on the RACE reading com-\nprehension benchmark (Lai et al., 2017). This\ndataset was collected from English reading com-\nprehension exams designed for middle and highRACE-middle RACE-high\nGPT-3 175B 58.4 45.5\nPaLM8B 57.9 42.3\n62B 64.3 47.5\n540B 68.1 49.1\nLLaMA7B 61.1 46.9\n13B 61.6 47.2\n33B 64.1 48.3\n65B 67.9 51.6\nTable 6: Reading Comprehension. Zero-shot accu-\nracy.\nschool Chinese students. We follow the evaluation\nsetup from Brown et al. (2020) and report results\nin Table 6. On these benchmarks, LLaMA-65B is\ncompetitive with PaLM-540B, and, LLaMA-13B\noutperforms GPT-3 by a few percents.\n3.4 Mathematical reasoning\nWe evaluate our models on two mathematical rea-\nsoning benchmarks: MATH (Hendrycks et al.,\n2021) and GSM8k (Cobbe et al., 2021). MATH\nis a dataset of 12K middle school and high school\nmathematics problems written in LaTeX. GSM8k\nis a set of middle school mathematical problems.\nIn Table 7, we compare with PaLM and Min-\nerva (Lewkowycz et al., 2022). Minerva is a series\nof PaLM models ﬁnetuned on 38.5B tokens ex-\ntracted from ArXiv and Math Web Pages, while\nneither PaLM or LLaMA are ﬁnetuned on mathe-\nmatical data. The numbers for PaLM and Minerva\nare taken from Lewkowycz et al. (2022), and we\ncompare with and without maj1@k .maj1@k de-\nnotes evaluations where we generate ksamples for\neach problem and perform a majority voting (Wang\net al., 2022). On GSM8k, we observe that LLaMA-\n65B outperforms Minerva-62B, although it has not\nbeen ﬁne-tuned on mathematical data.\n3.5 Code generation\nWe evaluate the ability of our models to write\ncode from a natural language description on two\nbenchmarks: HumanEval (Chen et al., 2021) and\nMBPP (Austin et al., 2021). For both tasks, the\nmodel receives a description of the program in a\nfew sentences, as well as a few input-output ex-\namples. In HumanEval, it also receives a function\nsignature, and the prompt is formatted as natural\ncode with the textual description and tests in a'
 'Humanities STEM Social Sciences Other Average\nGPT-NeoX 20B 29.8 34.9 33.7 37.7 33.6\nGPT-3 175B 40.8 36.7 50.4 48.8 43.9\nGopher 280B 56.2 47.4 71.9 66.1 60.0\nChinchilla 70B 63.6 54.9 79.3 73.9 67.5\nPaLM8B 25.6 23.8 24.1 27.8 25.4\n62B 59.5 41.9 62.7 55.8 53.7\n540B 77.0 55.6 81.0 69.6 69.3\nLLaMA7B 34.0 30.5 38.3 38.1 35.1\n13B 45.0 35.8 53.8 53.3 46.9\n33B 55.8 46.0 66.7 63.4 57.8\n65B 61.8 51.7 72.9 67.4 63.4\nTable 9: Massive Multitask Language Understanding (MMLU). Five-shot accuracy.\nthat may indicate that this benchmark is not\nreliable. On WinoGrande, the performance does\nnot correlate as well with training perplexity:\nthe LLaMA-33B and LLaMA-65B have similar\nperformance during the training.\n4 Instruction Finetuning\nIn this section, we show that brieﬂy ﬁnetuning on\ninstructions data rapidly leads to improvements\non MMLU. Although the non-ﬁnetuned version\nof LLaMA-65B is already able to follow basic in-\nstructions, we observe that a very small amount of\nﬁnetuning improves the performance on MMLU,\nand further improves the ability of the model to\nfollow instructions. Since this is not the focus of\nthis paper, we only conducted a single experiment\nfollowing the same protocol as Chung et al. (2022)\nto train an instruct model, LLaMA-I.\nOPT 30B 26.1\nGLM 120B 44.8\nPaLM 62B 55.1\nPaLM-cont 62B 62.8\nChinchilla 70B 67.5\nLLaMA 65B 63.4\nOPT-IML-Max 30B 43.2\nFlan-T5-XXL 11B 55.1\nFlan-PaLM 62B 59.6\nFlan-PaLM-cont 62B 66.1\nLLaMA-I 65B 68.9\nTable 10: Instruction ﬁnetuning – MMLU (5-shot).\nComparison of models of moderate size with and with-\nout instruction ﬁnetuning on MMLU.In Table 10, we report the results of our instruct\nmodel LLaMA-I on MMLU and compare with ex-\nisting instruction ﬁnetuned models of moderate\nsizes, namely, OPT-IML (Iyer et al., 2022) and the\nFlan-PaLM series (Chung et al., 2022). All the re-\nported numbers are from the corresponding papers.\nDespite the simplicity of the instruction ﬁnetuning\napproach used here, we reach 68.9% on MMLU.\nLLaMA-I (65B) outperforms on MMLU existing\ninstruction ﬁnetuned models of moderate sizes, but\nare still far from the state-of-the-art, that is 77.4\nfor GPT code-davinci-002 on MMLU (numbers\ntaken from Iyer et al. (2022)). The details of the\nperformance on MMLU on the 57 tasks can be\nfound in Table 16 of the appendix.\n5 Bias, Toxicity and Misinformation\nLarge language models have been showed to re-\nproduce and amplify biases that are existing in\nthe training data (Sheng et al., 2019; Kurita et al.,\n2019), and to generate toxic or offensive con-\ntent (Gehman et al., 2020). As our training dataset\ncontains a large proportion of data from the Web,\nwe believe that it is crucial to determine the po-\ntential for our models to generate such content.\nTo understand the potential harm of LLaMA-65B,\nwe evaluate on different benchmarks that measure\ntoxic content production and stereotypes detection.\nWhile we have selected some of the standard bench-\nmarks that are used by the language model com-\nmunity to indicate some of the issues with these\nmodels, these evaluations are not sufﬁcient to fully\nunderstand the risks associated with these models.'
 'MATH +maj1@k GSM8k +maj1@k\nPaLM8B 1.5 - 4.1 -\n62B 4.4 - 33.0 -\n540B 8.8 - 56.5 -\nMinerva8B 14.1 25.4 16.2 28.4\n62B 27.6 43.4 52.4 68.5\n540B 33.6 50.3 68.5 78.5\nLLaMA7B 2.9 6.9 11.0 18.1\n13B 3.9 8.8 17.8 29.3\n33B 7.1 15.2 35.6 53.1\n65B 10.6 20.5 50.9 69.7\nTable 7: Model performance on quantitative reason-\ning datasets. For majority voting, we use the same\nsetup as Minerva, with k= 256 samples for MATH\nandk= 100 for GSM8k (Minerva 540B uses k= 64\nfor MATH and and k= 40 for GSM8k). LLaMA-65B\noutperforms Minerva 62B on GSM8k, although it has\nnot been ﬁne-tuned on mathematical data.\ndocstring. The model needs to generate a Python\nprogram that ﬁts the description and satisﬁes the\ntest cases. In Table 8, we compare the pass@1\nscores of our models with existing language mod-\nels that have not been ﬁnetuned on code, namely\nPaLM and LaMDA (Thoppilan et al., 2022). PaLM\nand LLaMA were trained on datasets that contain\na similar number of code tokens.\nAs show in Table 8, for a similar number\nof parameters, LLaMA outperforms other gen-\neral models such as LaMDA and PaLM, which\nare not trained or ﬁnetuned speciﬁcally for code.\nLLaMA with 13B parameters and more outper-\nforms LaMDA 137B on both HumanEval and\nMBPP. LLaMA 65B also outperforms PaLM 62B,\neven when it is trained longer. The pass@1 results\nreported in this table were obtained by sampling\nwith temperature 0.1. The pass@100 and pass@80\nmetrics were obtained with temperature 0.8. We\nuse the same method as Chen et al. (2021) to obtain\nunbiased estimates of the pass@k.\nIt is possible to improve the performance on code\nby ﬁnetuning on code-speciﬁc tokens. For instance,\nPaLM-Coder (Chowdhery et al., 2022) increases\nthe pass@1 score of PaLM on HumanEval from\n26.2% for PaLM to 36%. Other models trained\nspeciﬁcally for code also perform better than gen-\neral models on these tasks (Chen et al., 2021; Ni-\njkamp et al., 2022; Fried et al., 2022). Finetuning\non code tokens is beyond the scope of this paper.Params HumanEval MBPP\npass@ @1 @100 @1 @80\nLaMDA 137B 14.0 47.3 14.8 62.4\nPaLM 8B 3.6∗18.7∗5.0∗35.7∗\nPaLM 62B 15.9 46.3∗21.4 63.2∗\nPaLM-cont 62B 23.7 - 31.2 -\nPaLM 540B 26.2 76.2 36.8 75.0\nLLaMA7B 10.5 36.5 17.7 56.2\n13B 15.8 52.5 22.0 64.0\n33B 21.7 70.7 30.2 73.4\n65B 23.7 79.3 37.7 76.8\nTable 8: Model performance for code generation.\nWe report the pass@ score on HumanEval and MBPP.\nHumanEval generations are done in zero-shot and\nMBBP with 3-shot prompts similar to Austin et al.\n(2021). The values marked with∗are read from ﬁgures\nin Chowdhery et al. (2022).\n3.6 Massive Multitask Language\nUnderstanding\nThe massive multitask language understanding\nbenchmark, or MMLU, introduced by Hendrycks\net al. (2020) consists of multiple choice questions\ncovering various domains of knowledge, includ-\ning humanities, STEM and social sciences. We\nevaluate our models in the 5-shot setting, using the\nexamples provided by the benchmark, and report\nresults in Table 9. On this benchmark, we observe\nthat the LLaMA-65B is behind both Chinchilla-\n70B and PaLM-540B by a few percent in average,\nand across most domains. A potential explanation\nis that we have used a limited amount of books\nand academic papers in our pre-training data, i.e.,\nArXiv, Gutenberg and Books3, that sums up to only\n177GB, while these models were trained on up to\n2TB of books. This large quantity of books used\nby Gopher, Chinchilla and PaLM may also explain\nwhy Gopher outperforms GPT-3 on this benchmark,\nwhile it is comparable on other benchmarks.\n3.7 Evolution of performance during training\nDuring training, we tracked the performance of our\nmodels on a few question answering and common\nsense benchmarks, and report them in Figure 2.\nOn most benchmarks, the performance improves\nsteadily, and correlates with the training perplexity\nof the model (see Figure 1). The exceptions are\nSIQA and WinoGrande. Most notably, on SIQA,\nwe observe a lot of variance in performance,'
 'GPU TypeGPU PowerGPU-hoursTotal power Carbon emitted\nconsumption consumption (tCO 2eq)\nOPT-175B A100-80GB 400W 809,472 356 MWh 137\nBLOOM-175B A100-80GB 400W 1,082,880 475 MWh 183\nLLaMA-7B A100-80GB 400W 82,432 36 MWh 14\nLLaMA-13B A100-80GB 400W 135,168 59 MWh 23\nLLaMA-33B A100-80GB 400W 530,432 233 MWh 90\nLLaMA-65B A100-80GB 400W 1,022,362 449 MWh 173\nTable 15: Carbon footprint of training different models in the same data center. We follow Wu et al. (2022)\nto compute carbon emission of training OPT, BLOOM and our models in the same data center. For the power\nconsumption of a A100-80GB, we take the thermal design power for NVLink systems, that is 400W. We take a\nPUE of 1.1 and a carbon intensity factor set at the national US average of 0.385 kg CO 2e per KWh.\nstarting from feed forward models (Bengio et al.,\n2000), recurrent neural networks (Elman, 1990;\nMikolov et al., 2010) and LSTMs (Hochreiter and\nSchmidhuber, 1997; Graves, 2013). More recently,\ntransformer networks, based on self-attention, have\nled to important improvements, especially for cap-\nturing long range dependencies (Vaswani et al.,\n2017; Radford et al., 2018; Dai et al., 2019).\nScaling. There is a long history of scaling for\nlanguage models, for both the model and dataset\nsizes. Brants et al. (2007) showed the beneﬁts of\nusing language models trained on 2 trillion tokens,\nresulting in 300 billion n-grams, on the quality of\nmachine translation. While this work relied on a\nsimple smoothing technique, called Stupid Backoff ,\nHeaﬁeld et al. (2013) later showed how to scale\nKneser-Ney smoothing to Web-scale data. This\nallowed to train a 5-gram model on 975 billions to-\nkens from CommonCrawl, resulting in a model\nwith 500 billions n-grams (Buck et al., 2014).\nChelba et al. (2013) introduced the One Billion\nWord benchmark, a large scale training dataset to\nmeasure the progress of language models.\nIn the context of neural language models, Joze-\nfowicz et al. (2016) obtained state-of-the-art re-\nsults on the Billion Word benchmark by scaling\nLSTMs to 1 billion parameters. Later, scaling\ntransformers lead to improvement on many NLP\ntasks. Notable models include BERT (Devlin et al.,\n2018), GPT-2 (Radford et al., 2019), Megatron-\nLM (Shoeybi et al., 2019), and T5 (Raffel et al.,\n2020). A signiﬁcant breakthrough was obtained\nwith GPT-3 (Brown et al., 2020), a model with\n175 billion parameters. This lead to a series of\nLarge Language Models , such as Jurassic-1 (Lieber\net al., 2021), Megatron-Turing NLG (Smith et al.,2022), Gopher (Rae et al., 2021), Chinchilla (Hoff-\nmann et al., 2022), PaLM (Chowdhery et al., 2022),\nOPT (Zhang et al., 2022), and GLM (Zeng et al.,\n2022). Hestness et al. (2017) and Rosenfeld et al.\n(2019) studied the impact of scaling on the perfor-\nmance of deep learning models, showing the exis-\ntence of power laws between the model and dataset\nsizes and the performance of the system. Kaplan\net al. (2020) derived power laws speciﬁcally for\ntransformer based language models, which were\nlater reﬁned by Hoffmann et al. (2022), by adapting\nthe learning rate schedule when scaling datasets.\nFinally, Wei et al. (2022) studied the effect of scal-\ning on the abilities of large language models.\n8 Conclusion\nIn this paper, we presented a series of language\nmodels that are released openly, and competitive\nwith state-of-the-art foundation models. Most\nnotably, LLaMA-13B outperforms GPT-3 while\nbeing more than 10 ×smaller, and LLaMA-65B is\ncompetitive with Chinchilla-70B and PaLM-540B.\nUnlike previous studies, we show that it is possible\nto achieve state-of-the-art performance by training\nexclusively on publicly available data, without\nresorting to proprietary datasets. We hope that\nreleasing these models to the research community\nwill accelerate the development of large language\nmodels, and help efforts to improve their robust-\nness and mitigate known issues such as toxicity and\nbias. Additionally, we observed like Chung et al.']","LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with Chinchilla-70B and PaLM-540B, showing that it can achieve comparable or better performance.",0.999999999975,0.6666666666666666,0.8837205274814043,1.0,0.6206236031674552,0.982494412669821
How does LLaMA's training data preprocessing and mixture differ from other large language models?,"LLaMA is trained exclusively on publicly available datasets, contrasting with other models that often rely on proprietary or undocumented data sources. The training process focuses on utilizing a larger volume of tokens, which enhances performance while remaining efficient at inference. This approach aims to democratize access to language models and allows LLaMA to run on a single GPU.","['LLaMA: Open and Efﬁcient Foundation Language Models\nHugo Touvron∗, Thibaut Lavril∗, Gautier Izacard∗, Xavier Martinet\nMarie-Anne Lachaux, Timothee Lacroix, Baptiste Rozière, Naman Goyal\nEric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin\nEdouard Grave∗, Guillaume Lample∗\nMeta AI\nAbstract\nWe introduce LLaMA, a collection of founda-\ntion language models ranging from 7B to 65B\nparameters. We train our models on trillions\nof tokens, and show that it is possible to train\nstate-of-the-art models using publicly avail-\nable datasets exclusively, without resorting\nto proprietary and inaccessible datasets. In\nparticular, LLaMA-13B outperforms GPT-3\n(175B) on most benchmarks, and LLaMA-\n65B is competitive with the best models,\nChinchilla-70B and PaLM-540B. We release\nall our models to the research community1.\n1 Introduction\nLarge Languages Models (LLMs) trained on mas-\nsive corpora of texts have shown their ability to per-\nform new tasks from textual instructions or from a\nfew examples (Brown et al., 2020). These few-shot\nproperties ﬁrst appeared when scaling models to a\nsufﬁcient size (Kaplan et al., 2020), resulting in a\nline of work that focuses on further scaling these\nmodels (Chowdhery et al., 2022; Rae et al., 2021).\nThese efforts are based on the assumption that\nmore parameters will lead to better performance.\nHowever, recent work from Hoffmann et al. (2022)\nshows that, for a given compute budget, the best\nperformances are not achieved by the largest mod-\nels, but by smaller models trained on more data.\nThe objective of the scaling laws from Hoff-\nmann et al. (2022) is to determine how to best\nscale the dataset and model sizes for a particular\ntraining compute budget. However, this objective\ndisregards the inference budget, which becomes\ncritical when serving a language model at scale.\nIn this context, given a target level of performance,\nthe preferred model is not the fastest to train but the\nfastest at inference, and although it may be cheaper\nto train a large model to reach a certain level of\n∗Equal contribution. Correspondence: {htouvron,\nthibautlav,gizacard,egrave,glample}@meta.com\n1https://github.com/facebookresearch/llamaperformance, a smaller one trained longer will\nultimately be cheaper at inference. For instance,\nalthough Hoffmann et al. (2022) recommends\ntraining a 10B model on 200B tokens, we ﬁnd\nthat the performance of a 7B model continues to\nimprove even after 1T tokens.\nThe focus of this work is to train a series of\nlanguage models that achieve the best possible per-\nformance at various inference budgets, by training\non more tokens than what is typically used. The\nresulting models, called LLaMA , ranges from 7B\nto 65B parameters with competitive performance\ncompared to the best existing LLMs. For instance,\nLLaMA-13B outperforms GPT-3 on most bench-\nmarks, despite being 10 ×smaller. We believe that\nthis model will help democratize the access and\nstudy of LLMs, since it can be run on a single GPU.\nAt the higher-end of the scale, our 65B-parameter\nmodel is also competitive with the best large lan-\nguage models such as Chinchilla or PaLM-540B.\nUnlike Chinchilla, PaLM, or GPT-3, we only\nuse publicly available data, making our work com-\npatible with open-sourcing, while most existing\nmodels rely on data which is either not publicly\navailable or undocumented (e.g. “Books – 2TB” or\n“Social media conversations”). There exist some\nexceptions, notably OPT (Zhang et al., 2022),\nGPT-NeoX (Black et al., 2022), BLOOM (Scao\net al., 2022) and GLM (Zeng et al., 2022), but none\nthat are competitive with PaLM-62B or Chinchilla.\nIn the rest of this paper, we present an overview\nof the modiﬁcations we made to the transformer\narchitecture (Vaswani et al., 2017), as well as our\ntraining method. We then report the performance of\nour models and compare with others LLMs on a set\nof standard benchmarks. Finally, we expose some\nof the biases and toxicity encoded in our models,\nusing some of the most recent benchmarks from'
 'Humanities STEM Social Sciences Other Average\nGPT-NeoX 20B 29.8 34.9 33.7 37.7 33.6\nGPT-3 175B 40.8 36.7 50.4 48.8 43.9\nGopher 280B 56.2 47.4 71.9 66.1 60.0\nChinchilla 70B 63.6 54.9 79.3 73.9 67.5\nPaLM8B 25.6 23.8 24.1 27.8 25.4\n62B 59.5 41.9 62.7 55.8 53.7\n540B 77.0 55.6 81.0 69.6 69.3\nLLaMA7B 34.0 30.5 38.3 38.1 35.1\n13B 45.0 35.8 53.8 53.3 46.9\n33B 55.8 46.0 66.7 63.4 57.8\n65B 61.8 51.7 72.9 67.4 63.4\nTable 9: Massive Multitask Language Understanding (MMLU). Five-shot accuracy.\nthat may indicate that this benchmark is not\nreliable. On WinoGrande, the performance does\nnot correlate as well with training perplexity:\nthe LLaMA-33B and LLaMA-65B have similar\nperformance during the training.\n4 Instruction Finetuning\nIn this section, we show that brieﬂy ﬁnetuning on\ninstructions data rapidly leads to improvements\non MMLU. Although the non-ﬁnetuned version\nof LLaMA-65B is already able to follow basic in-\nstructions, we observe that a very small amount of\nﬁnetuning improves the performance on MMLU,\nand further improves the ability of the model to\nfollow instructions. Since this is not the focus of\nthis paper, we only conducted a single experiment\nfollowing the same protocol as Chung et al. (2022)\nto train an instruct model, LLaMA-I.\nOPT 30B 26.1\nGLM 120B 44.8\nPaLM 62B 55.1\nPaLM-cont 62B 62.8\nChinchilla 70B 67.5\nLLaMA 65B 63.4\nOPT-IML-Max 30B 43.2\nFlan-T5-XXL 11B 55.1\nFlan-PaLM 62B 59.6\nFlan-PaLM-cont 62B 66.1\nLLaMA-I 65B 68.9\nTable 10: Instruction ﬁnetuning – MMLU (5-shot).\nComparison of models of moderate size with and with-\nout instruction ﬁnetuning on MMLU.In Table 10, we report the results of our instruct\nmodel LLaMA-I on MMLU and compare with ex-\nisting instruction ﬁnetuned models of moderate\nsizes, namely, OPT-IML (Iyer et al., 2022) and the\nFlan-PaLM series (Chung et al., 2022). All the re-\nported numbers are from the corresponding papers.\nDespite the simplicity of the instruction ﬁnetuning\napproach used here, we reach 68.9% on MMLU.\nLLaMA-I (65B) outperforms on MMLU existing\ninstruction ﬁnetuned models of moderate sizes, but\nare still far from the state-of-the-art, that is 77.4\nfor GPT code-davinci-002 on MMLU (numbers\ntaken from Iyer et al. (2022)). The details of the\nperformance on MMLU on the 57 tasks can be\nfound in Table 16 of the appendix.\n5 Bias, Toxicity and Misinformation\nLarge language models have been showed to re-\nproduce and amplify biases that are existing in\nthe training data (Sheng et al., 2019; Kurita et al.,\n2019), and to generate toxic or offensive con-\ntent (Gehman et al., 2020). As our training dataset\ncontains a large proportion of data from the Web,\nwe believe that it is crucial to determine the po-\ntential for our models to generate such content.\nTo understand the potential harm of LLaMA-65B,\nwe evaluate on different benchmarks that measure\ntoxic content production and stereotypes detection.\nWhile we have selected some of the standard bench-\nmarks that are used by the language model com-\nmunity to indicate some of the issues with these\nmodels, these evaluations are not sufﬁcient to fully\nunderstand the risks associated with these models.'
 'HellaSwag (Zellers et al., 2019), WinoGrande (Sak-\naguchi et al., 2021), ARC easy and challenge (Clark\net al., 2018) and OpenBookQA (Mihaylov et al.,\n2018). These datasets include Cloze and Winograd\nstyle tasks, as well as multiple choice question an-\nswering. We evaluate in the zero-shot setting as\ndone in the language modeling community.\nIn Table 3, we compare with existing models\nof various sizes and report numbers from the cor-\nresponding papers. First, LLaMA-65B outper-\nforms Chinchilla-70B on all reported benchmarks\nbut BoolQ. Similarly, this model surpasses PaLM-\n540B everywhere but on BoolQ and WinoGrande.\nLLaMA-13B model also outperforms GPT-3 on\nmost benchmarks despite being 10 ×smaller.\n3.2 Closed-book Question Answering\nWe compare LLaMA to existing large language\nmodels on two closed-book question answering\nbenchmarks: Natural Questions (Kwiatkowski\net al., 2019) and TriviaQA (Joshi et al., 2017). For\nboth benchmarks, we report exact match perfor-\nmance in a closed book setting, i.e., where the mod-\nels do not have access to documents that contain\nevidence to answer the question. In Table 4, we\nreport performance on NaturalQuestions, and in Ta-\nble 5, we report on TriviaQA. On both benchmarks,\nLLaMA-65B achieve state-of-the-arts performance\nin the zero-shot and few-shot settings. More im-\nportantly, the LLaMA-13B is also competitive on\nthese benchmarks with GPT-3 and Chinchilla, de-\nspite being 5-10 ×smaller. This model runs on a\nsingle V100 GPU during inference.\n0-shot 1-shot 5-shot 64-shot\nGopher 280B 43.5 - 57.0 57.2\nChinchilla 70B 55.4 - 64.1 64.6\nLLaMA7B 50.0 53.4 56.3 57.6\n13B 56.6 60.5 63.1 64.0\n33B 65.1 67.9 69.9 70.4\n65B 68.2 71.6 72.6 73.0\nTable 5: TriviaQA. Zero-shot and few-shot exact\nmatch performance on the ﬁltered dev set.\n3.3 Reading Comprehension\nWe evaluate our models on the RACE reading com-\nprehension benchmark (Lai et al., 2017). This\ndataset was collected from English reading com-\nprehension exams designed for middle and highRACE-middle RACE-high\nGPT-3 175B 58.4 45.5\nPaLM8B 57.9 42.3\n62B 64.3 47.5\n540B 68.1 49.1\nLLaMA7B 61.1 46.9\n13B 61.6 47.2\n33B 64.1 48.3\n65B 67.9 51.6\nTable 6: Reading Comprehension. Zero-shot accu-\nracy.\nschool Chinese students. We follow the evaluation\nsetup from Brown et al. (2020) and report results\nin Table 6. On these benchmarks, LLaMA-65B is\ncompetitive with PaLM-540B, and, LLaMA-13B\noutperforms GPT-3 by a few percents.\n3.4 Mathematical reasoning\nWe evaluate our models on two mathematical rea-\nsoning benchmarks: MATH (Hendrycks et al.,\n2021) and GSM8k (Cobbe et al., 2021). MATH\nis a dataset of 12K middle school and high school\nmathematics problems written in LaTeX. GSM8k\nis a set of middle school mathematical problems.\nIn Table 7, we compare with PaLM and Min-\nerva (Lewkowycz et al., 2022). Minerva is a series\nof PaLM models ﬁnetuned on 38.5B tokens ex-\ntracted from ArXiv and Math Web Pages, while\nneither PaLM or LLaMA are ﬁnetuned on mathe-\nmatical data. The numbers for PaLM and Minerva\nare taken from Lewkowycz et al. (2022), and we\ncompare with and without maj1@k .maj1@k de-\nnotes evaluations where we generate ksamples for\neach problem and perform a majority voting (Wang\net al., 2022). On GSM8k, we observe that LLaMA-\n65B outperforms Minerva-62B, although it has not\nbeen ﬁne-tuned on mathematical data.\n3.5 Code generation\nWe evaluate the ability of our models to write\ncode from a natural language description on two\nbenchmarks: HumanEval (Chen et al., 2021) and\nMBPP (Austin et al., 2021). For both tasks, the\nmodel receives a description of the program in a\nfew sentences, as well as a few input-output ex-\namples. In HumanEval, it also receives a function\nsignature, and the prompt is formatted as natural\ncode with the textual description and tests in a'
 'Large Language Models Struggle to Learn Long-Tail Knowledge\nNikhil Kandpal1Haikang Deng1Adam Roberts2Eric Wallace3Colin Raffel1\nAbstract\nThe Internet contains a wealth of knowledge—\nfrom the birthdays of historical figures to tutorials\non how to code—all of which may be learned by\nlanguage models. However, while certain pieces\nof information are ubiquitous on the web, oth-\ners appear extremely rarely. In this paper, we\nstudy the relationship between the knowledge\nmemorized by large language models and the in-\nformation in pre-training datasets scraped from\nthe web. In particular, we show that a language\nmodel’s ability to answer a fact-based question\nrelates to how many documents associated with\nthat question were seen during pre-training. We\nidentify these relevant documents by entity link-\ning pre-training datasets and counting documents\nthat contain the same entities as a given question-\nanswer pair. Our results demonstrate strong cor-\nrelational and causal relationships between accu-\nracy and relevant document count for numerous\nquestion answering datasets (e.g., TriviaQA), pre-\ntraining corpora (e.g., ROOTS), and model sizes\n(e.g., 176B parameters). Moreover, while larger\nmodels are better at learning long-tail knowledge,\nwe estimate that today’s models must be scaled by\nmany orders of magnitude to reach competitive\nQA performance on questions with little support\nin the pre-training data. Finally, we show that\nretrieval-augmentation can reduce the dependence\non relevant pre-training information, presenting a\npromising approach for capturing the long-tail.\n1. Introduction\nLarge language models (LLMs) trained on text from the\nInternet capture many facts about the world, ranging from\nwell-known factoids to esoteric domain-specific information.\nThese models implicitly store knowledge in their parameters\n1UNC Chapel Hill2Google Research3UC Berkeley. Corre-\nspondence to: Nikhil Kandpal <nkandpa2@cs.unc.edu >.\nProceedings of the 40thInternational Conference on Machine\nLearning , Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).\n100101102103104105106\nNumber of Relevant Pre-training Documents0.00.10.20.30.40.50.6QA Accuracy\nBLOOM Model\n176B\n7.1B\n3B\n1.7B\n1.1B\n560MFigure 1. Language models struggle to capture the long-tail of\ninformation on the web. Above, we plot accuracy for the BLOOM\nmodel family on TriviaQA as a function of how many documents\nin the model’s pre-training data are relevant to each question.\n(Petroni et al., 2019; Roberts et al., 2020), and given the\nscale of today’s pre-training datasets and LLMs, one would\nhope that they can learn a huge amount of information from\nweb-sourced text. However, not all of the knowledge on\nthe Internet appears equally often—there is a long-tail of\ninformation that appears rarely or only once.\nIn this work, we explore the relationship between the knowl-\nedge learned by an LLM and the information in its pre-\ntraining dataset. Specifically, we study how an LLM’s\nability to answer a question relates to how many docu-\nments associated with that question were seen during pre-\ntraining. We focus on factoid QA datasets (Joshi et al., 2017;\nKwiatkowski et al., 2019), which lets us ground question-\nanswer pairs into concrete subject-object co-occurrences.\nAs an example, for the QA pair ( In what city was the poet\nDante born? ,Florence ), we consider documents where the\nentities Dante andFlorence co-occur as highly relevant.\nTo identify these entity co-occurrences we apply a highly-\nparallelized entity linking pipeline to trillions of tokens\nfrom datasets such as C4 (Raffel et al., 2020), The Pile (Gao\net al., 2020), ROOTS (Lauren c ¸on et al., 2022), OpenWeb-\nText (Gokaslan & Cohen, 2019), and Wikipedia.\nWe observe a strong correlation between an LM’s ability to\nanswer a question and the number of pre-training documents\n1arXiv:2211.08411v2  [cs.CL]  27 Jul 2023']","LLaMA's training data includes a mixture of sources like CommonCrawl, C4, GitHub, Wikipedia, and others, preprocessed with techniques like deduplication, language identification, and quality filtering, emphasizing the use of publicly available data.",0.9999999999,1.0,0.893622101437176,0.3333333333333333,0.4774410511138367,0.909764204455347
How does the accuracy of the largest GPT-3 model compare to human professionals across the 57 tasks?,"The largest GPT-3 model performs better than random chance, achieving an accuracy of 43.9% on the 57 tasks. However, it still falls significantly short of expert-level performance, indicating that it cannot match the capabilities of human professionals across these tasks. Overall, both GPT-3 and UniﬁedQA models exhibit lopsided performance and substantial knowledge gaps.","[""Published as a conference paper at ICLR 2021\n0 20 40 60 80 100\nAccuracy (%)World ReligionsVirologyUS Foreign PolicySociologySecurity StudiesPublic RelationsProfessional PsychologyProfessional MedicineProfessional LawProfessional AccountingPrehistoryPhilosophyNutritionMoral ScenariosMoral DisputesMiscellaneousMedical GeneticsMarketingManagementMachine LearningLogical FallaciesJurisprudenceInternational LawHuman SexualityHuman AgingHigh School World HistoryHigh School US HistoryHigh School StatisticsHigh School PsychologyHigh School PhysicsHigh School MicroeconomicsHigh School MathematicsHigh School MacroeconomicsHigh School Gov't and PoliticsHigh School GeographyHigh School European HistoryHigh School Comp SciHigh School ChemistryHigh School BiologyGlobal FactsFormal LogicElementary MathematicsElectrical EngineeringEconometricsConceptual PhysicsComputer SecurityCollege PhysicsCollege MedicineCollege MathematicsCollege Comp SciCollege ChemistryCollege BiologyClinical KnowledgeBusiness EthicsAstronomyAnatomyAbstract AlgebraGPT-3\nUnifiedQA\nRandom\nFigure 6: GPT-3 (few-shot) and UniﬁedQA results.Few-Shot Prompt. We feed GPT-3 prompts\nlike that shown in Figure 1a. We begin each\nprompt with “The following are multiple choice\nquestions (with answers) about [subject].” For\nzero-shot evaluation, we append the question to\nthe prompt. For few-shot evaluation, we add up\nto5demonstration examples with answers to\nthe prompt before appending the question. All\nprompts end with “Answer: ”. The model then\nproduces probabilities for the tokens “A,” “B,”\n“C,” and “D,” and we treat the highest probability\noption as the prediction. For consistent evalua-\ntion, we create a dev set with 5ﬁxed few-shot\nexamples for each subject.\n4.2 R ESULTS\nModel Size and Accuracy. We compare the\nfew-shot accuracy of each GPT-3 size in Table 1.\nWe ﬁnd that the three smaller GPT-3 models\nhave near random accuracy (around 25%). In\ncontrast, we ﬁnd that the X-Large 175billion\nparameter GPT-3 model performs substantially\nbetter than random, with an accuracy of 43.9%.\nWe also ﬁnd qualitatively similar results in the\nzero-shot setting. While the smaller models\nhave around 25% zero-shot accuracy, Figure 10\nin Appendix A shows that the largest GPT-3\nmodel has a much higher zero-shot accuracy of\nabout 37.7%. Brown et al. (2020) also observe\nthat larger GPT-3 models perform better, though\nprogress tends to be steadier. In Figure 1b we\nshow that non-random accuracy on the multitask\ntest emerged with recent large few-shot models\ncompared to datasets that assess commonsense\nand linguistic understanding.\nTo test the usefulness of ﬁne-tuning instead of\nfew-shot learning, we also evaluate UniﬁedQA\nmodels. UniﬁedQA has the advantage of being\nﬁne-tuned on other question answering datasets,\nunlike GPT-3. We assess UniﬁedQA by evalu-\nating its transfer performance without any ad-\nditional ﬁne-tuning. The largest UniﬁedQA\nmodel we test has 11billion parameters, which\nis slightly smaller than GPT-3 Large. Neverthe-\nless, we show in Table 1 that it attains 48.9%\naccuracy. This performs better than the few-shot GPT-3 X-Large model, despite UniﬁedQA have\nan order of magnitude fewer parameters. We also ﬁnd that even the smallest UniﬁedQA variant,\nwith just 60million parameters, has approximately 29.3%accuracy. These results suggest that while\nmodel size is a key component for achieving strong performance, ﬁne-tuning also helps.\nComparing Disciplines. Using our test, we discover that GPT-3 and UniﬁedQA have lopsided\nperformance and several substantial knowledge gaps. Figure 6 shows the accuracy of GPT-3 (few-\nshot) and UniﬁedQA for all 57tasks. It shows the both models are below expert-level performance\nfor all tasks, with GPT-3’s accuracy ranging from 69% for US Foreign Policy to 26% for College\nChemistry. UniﬁedQA does best on marketing, with an accuracy of 82.5%.\nOverall, models do poorly on highly procedural problems. Figure 6 shows that calculation-heavy""
 'Published as a conference paper at ICLR 2021\nMEASURING MASSIVE MULTITASK\nLANGUAGE UNDERSTANDING\nDan Hendrycks\nUC BerkeleyCollin Burns\nColumbia UniversitySteven Basart\nUChicagoAndy Zou\nUC Berkeley\nMantas Mazeika\nUIUCDawn Song\nUC BerkeleyJacob Steinhardt\nUC Berkeley\nABSTRACT\nWe propose a new test to measure a text model’s multitask accuracy. The test\ncovers 57 tasks including elementary mathematics, US history, computer science,\nlaw, and more. To attain high accuracy on this test, models must possess extensive\nworld knowledge and problem solving ability. We ﬁnd that while most recent\nmodels have near random-chance accuracy, the very largest GPT-3 model improves\nover random chance by almost 20 percentage points on average. However, on every\none of the 57 tasks, the best models still need substantial improvements before\nthey can reach expert-level accuracy. Models also have lopsided performance\nand frequently do not know when they are wrong. Worse, they still have near-\nrandom accuracy on some socially important subjects such as morality and law.\nBy comprehensively evaluating the breadth and depth of a model’s academic and\nprofessional understanding, our test can be used to analyze models across many\ntasks and to identify important shortcomings.\n1 I NTRODUCTION\nNatural Language Processing (NLP) models have achieved superhuman performance on a number of\nrecently proposed benchmarks. However, these models are still well below human level performance\nfor language understanding as a whole, suggesting a disconnect between our benchmarks and the\nactual capabilities of these models. The General Language Understanding Evaluation benchmark\n(GLUE) (Wang et al., 2018) was introduced in 2018 to evaluate performance on a wide range of NLP\ntasks, and top models achieved superhuman performance within a year. To address the shortcomings\nof GLUE, researchers designed the SuperGLUE benchmark with more difﬁcult tasks (Wang et al.,\n2019). About a year since the release of SuperGLUE, performance is again essentially human-level\n(Raffel et al., 2019). While these benchmarks evaluate linguistic skills more than overall language\nunderstanding, an array of commonsense benchmarks have been proposed to measure basic reasoning\nand everyday knowledge (Zellers et al., 2019; Huang et al., 2019; Bisk et al., 2019). However, these\nrecent benchmarks have similarly seen rapid progress (Khashabi et al., 2020). Overall, the near\nhuman-level performance on these benchmarks suggests that they are not capturing important facets\nof language understanding.\nTransformer models have driven this recent progress by pretraining on massive text corpora, including\nall of Wikipedia, thousands of books, and numerous websites. These models consequently see\nextensive information about specialized topics, most of which is not assessed by existing NLP\nbenchmarks. It consequently remains an open question just how capable current language models are\nat learning and applying knowledge from many domains.\nTo bridge the gap between the wide-ranging knowledge that models see during pretraining and the\nexisting measures of success, we introduce a new benchmark for assessing models across a diverse\nset of subjects that humans learn. We design the benchmark to measure knowledge acquired during\npretraining by evaluating models exclusively in zero-shot and few-shot settings. This makes the\nbenchmark more challenging and more similar to how we evaluate humans. The benchmark covers\n57subjects across STEM, the humanities, the social sciences, and more. It ranges in difﬁculty from\nan elementary level to an advanced professional level, and it tests both world knowledge and problem\nsolving ability. Subjects range from traditional areas, such as mathematics and history, to more\n1arXiv:2009.03300v3  [cs.CY]  12 Jan 2021'
 'Published as a conference paper at ICLR 2021\net al., 2019), ALBERT-xxlarge (223 million parameters) (Lan et al., 2020), and GPT-2 (1,558 million\nparameters) (Radford et al., 2019). Models are ﬁne-tuned to predict one of four classes using the\nUniﬁedQA MCQ questions and using our dev+val set. We test on our multitask test set.\nWe observe that these smaller models can attain better-than-random accuracy. RoBERTa-base attains\nan overall accuracy of 27.9%, with 27.9%accuracy for the humanities, 28.8%for social sciences,\n27.0%for STEM, and 27.7%for other. ALBERT-xxlarge attains an accuracy of 27.1%, with 27.2%\naccuracy for the humanities, 25.7%for the social sciences, 27.7%for STEM, and 27.9%for other.\nGPT-2 attains an accuracy of 32.4%, with 32.8%accuracy for the humanities, 33.3%for the social\nsciences, 30.2%for STEM, and 33.1%for other.\nCompare this to UniﬁedQA’s smallest variant, which has just 60million parameters and approximately\n29.3%accuracy. It obtains higher accuracy than RoBERTa and ALBERT, even though it has fewer\nparameters. This suggests that its larger pretraining dataset enables higher accuracy. Likewise,\nUniﬁedQA with 3billion parameters attains 43.7%, while the similarly sized GPT-2 model with 1.5\nbillion parameters attains 32.4%accuracy. This again suggests that T5’s larger pretraining dataset\nsize (and therefore UniﬁedQA’s pretraining dataset size) can increase accuracy.\nA.2 E RROR ANALYSIS\nWe qualitatively analyze when GPT-3 makes high conﬁdence mistakes. We ﬁnd that while many of\nthese mistakes were clearly wrong, many were mistakes that a human might make. For example,\none question it got wrong was “How many chromosomes do all human somatic cells contain?” The\ncorrect answer is 46, while few-shot GPT-3 predicted 23with conﬁdence 97.5%. This answer would\nhave been correct if the question asked about the number of pairs of chromosomes. Similarly, many\nof its other high conﬁdence mistakes were also correct answers to slightly different questions.\nA.3 F ORMAT SENSITIVITY\nWhile different question formatting choices often lead to similar GPT-3 accuracies, we ﬁnd that\nUniﬁedQA is more sensitive. UniﬁedQA’s input format is of the form\nQUESTION1 \\\\n (A) CHOICE1 (B) CHOICE2 (C) CHOICE3 (D) CHOICE4</s>\nwhere questions and choices are normalized and made lowercase. If we remove the </s> from the\ninput, accuracy declines by several percentage points.\n12'
 ""Published as a conference paper at ICLR 2021\nA A DDITIONAL ANALYSIS\nThis appendix includes ﬁgures with sorted results (Figure 9), few-shot examples vs. accuracy\n(Figure 10), and few-shot calibration (Figure 11). It also includes sections on ﬁne-tuning, error\nanalysis, and format sensitivity.\n0 20 40 60 80 100\nAccuracy (%)College ChemistryMoral ScenariosCollege PhysicsHigh School PhysicsHigh School MathematicsFormal LogicElementary MathematicsAbstract AlgebraHigh School StatisticsMachine LearningEconometricsHigh School ChemistryProfessional AccountingProfessional LawCollege MathematicsProfessional MedicineConceptual PhysicsGlobal FactsHigh School Comp SciMedical GeneticsHigh School MacroeconomicsHigh School MicroeconomicsMoral DisputesProfessional PsychologyCollege BiologyVirologyCollege Comp SciBusiness EthicsNutritionCollege MedicineAnatomyClinical KnowledgeLogical FallaciesHigh School BiologyPublic RelationsAstronomyElectrical EngineeringHuman AgingPhilosophySecurity StudiesPrehistoryHigh School US HistorySociologyHigh School European HistoryHuman SexualityJurisprudenceWorld ReligionsInternational LawHigh School World HistoryManagementComputer SecurityHigh School GeographyHigh School Gov't and PoliticsMarketingMiscellaneousHigh School PsychologyUS Foreign PolicyGPT-3 Results\nRandom Chance\n0102030405060708090100\nAccuracy (%)Moral ScenariosFormal LogicAbstract AlgebraEconometricsHigh School MathematicsCollege PhysicsMachine LearningHigh School StatisticsCollege ChemistryElementary MathematicsCollege MathematicsHigh School ChemistryGlobal FactsProfessional LawMedical GeneticsProfessional AccountingCollege BiologyHigh School PhysicsAnatomyCollege Comp SciConceptual PhysicsCollege MedicineVirologyProfessional MedicineAstronomyHigh School MacroeconomicsElectrical EngineeringProfessional PsychologySecurity StudiesHuman SexualityNutritionHigh School Comp SciPrehistoryClinical KnowledgeHigh School BiologyHuman AgingHigh School MicroeconomicsPhilosophyPublic RelationsWorld ReligionsMoral DisputesLogical FallaciesHigh School European HistoryHigh School US HistoryHigh School World HistoryMiscellaneousUS Foreign PolicyComputer SecuritySociologyInternational LawHigh School GeographyJurisprudenceBusiness EthicsHigh School PsychologyManagementHigh School Gov't and PoliticsMarketingUnifiedQA Results\nRandom Chance\nFigure 9: On the left are GPT-3 few shot accuracies for all of the 57tasks. On the right are UniﬁedQA\ntransfer accuracies for all of the 57tasks. For both models, capabilities are lopsided.\nA.1 A NALYSIS WITH MORE FINE-TUNED MODELS\nWe primarily analyzed models with more than 10billion parameters in the main body of the paper.\nFor this section, we analyze smaller models including RoBERTa-base (125 million parameters) (Liu\n11""]","he largest GPT-3 model achieves 43.9% accuracy on average, significantly below the expert-level performance, indicating that even the most advanced models require substantial improvements to reach human professional accuracy levels across these tasks.",0.999999999975,1.0,0.9617760723004372,1.0,0.7407605093829147,0.9630420375316588
What can you say about the calibration of models like GPT-3 in relation to their confidence and accuracy on the multitask test?,"Models like GPT-3 exhibit miscalibration, with a significant gap between accuracy and confidence that can reach up to 14%. In a few-shot setting, models are more calibrated than in a zero-shot setting, but the correlation between confidence and accuracy remains imperfect, with a correlation coefficient of 0.81 compared to 0.63 in zero-shot settings. This suggests that while models improve their predictions with more examples, they still struggle to accurately assess their confidence in those predictions.","['Published as a conference paper at ICLR 2021\nMEASURING MASSIVE MULTITASK\nLANGUAGE UNDERSTANDING\nDan Hendrycks\nUC BerkeleyCollin Burns\nColumbia UniversitySteven Basart\nUChicagoAndy Zou\nUC Berkeley\nMantas Mazeika\nUIUCDawn Song\nUC BerkeleyJacob Steinhardt\nUC Berkeley\nABSTRACT\nWe propose a new test to measure a text model’s multitask accuracy. The test\ncovers 57 tasks including elementary mathematics, US history, computer science,\nlaw, and more. To attain high accuracy on this test, models must possess extensive\nworld knowledge and problem solving ability. We ﬁnd that while most recent\nmodels have near random-chance accuracy, the very largest GPT-3 model improves\nover random chance by almost 20 percentage points on average. However, on every\none of the 57 tasks, the best models still need substantial improvements before\nthey can reach expert-level accuracy. Models also have lopsided performance\nand frequently do not know when they are wrong. Worse, they still have near-\nrandom accuracy on some socially important subjects such as morality and law.\nBy comprehensively evaluating the breadth and depth of a model’s academic and\nprofessional understanding, our test can be used to analyze models across many\ntasks and to identify important shortcomings.\n1 I NTRODUCTION\nNatural Language Processing (NLP) models have achieved superhuman performance on a number of\nrecently proposed benchmarks. However, these models are still well below human level performance\nfor language understanding as a whole, suggesting a disconnect between our benchmarks and the\nactual capabilities of these models. The General Language Understanding Evaluation benchmark\n(GLUE) (Wang et al., 2018) was introduced in 2018 to evaluate performance on a wide range of NLP\ntasks, and top models achieved superhuman performance within a year. To address the shortcomings\nof GLUE, researchers designed the SuperGLUE benchmark with more difﬁcult tasks (Wang et al.,\n2019). About a year since the release of SuperGLUE, performance is again essentially human-level\n(Raffel et al., 2019). While these benchmarks evaluate linguistic skills more than overall language\nunderstanding, an array of commonsense benchmarks have been proposed to measure basic reasoning\nand everyday knowledge (Zellers et al., 2019; Huang et al., 2019; Bisk et al., 2019). However, these\nrecent benchmarks have similarly seen rapid progress (Khashabi et al., 2020). Overall, the near\nhuman-level performance on these benchmarks suggests that they are not capturing important facets\nof language understanding.\nTransformer models have driven this recent progress by pretraining on massive text corpora, including\nall of Wikipedia, thousands of books, and numerous websites. These models consequently see\nextensive information about specialized topics, most of which is not assessed by existing NLP\nbenchmarks. It consequently remains an open question just how capable current language models are\nat learning and applying knowledge from many domains.\nTo bridge the gap between the wide-ranging knowledge that models see during pretraining and the\nexisting measures of success, we introduce a new benchmark for assessing models across a diverse\nset of subjects that humans learn. We design the benchmark to measure knowledge acquired during\npretraining by evaluating models exclusively in zero-shot and few-shot settings. This makes the\nbenchmark more challenging and more similar to how we evaluate humans. The benchmark covers\n57subjects across STEM, the humanities, the social sciences, and more. It ranges in difﬁculty from\nan elementary level to an advanced professional level, and it tests both world knowledge and problem\nsolving ability. Subjects range from traditional areas, such as mathematics and history, to more\n1arXiv:2009.03300v3  [cs.CY]  12 Jan 2021'
 'across all augmented variants. In contrast, InstructGPT\nand GLM exhibit a statistically significant degradation\nin performance, averaging approximately 5% across the\nvarious perturbation methods. Notably, all three language\nmodels exhibit a heightened level of susceptibility to\nsemantics-altering perturbations in the IMDB sentiment\nanalysis scenario, as indicated by the EM ctstmetric.\nIn addition, the model performance is evaluated on\nthe BoolQA dataset using identical experimental settings.\nThe results of this evaluation are consistent with the\naforementioned findings, with ChatGPT outperforming\nthe other two baselines on both the original and per-\nturbed subsets. Furthermore, it is observed that the three\nlanguage models exhibit a degradation in performance\nwhen subjected to contrasting semantic perturbations,\nsuggesting that equivariance of robustness may pose a\nsignificant challenge for the current advanced language\nmodels and their applications.'
 'Published as a conference paper at ICLR 2021\n0-Shot 1-Shot 2-Shot 3-Shot 4-Shot 5-Shot\nNumber of Examples in Context3035404550Accuracy (%)\nGPT-3 Multitask Accuracy vs.\nNumber of Examples in Context\nFigure 10: As the number of few-shot instruction\nexamples increases, the accuracy monotonically\nincreases. Notably, zero-shot performance is only\nsomewhat lower than 5-shot accuracy.\n20 30 40 50 60 70\nConfidence (%)203040506070Accuracy (%)\nGPT-3 Few-Shot CalibrationFigure 11: While models are more calibrated in\na few-shot setting than a zero-shot setting, they\nare still miscalibrated, with gap between accuracy\nand conﬁdence reaching up to 14%. Here the\ncorrelation between conﬁdence and accuracy is\nr= 0.81, compared to r= 0.63in the zero-shot\nsetting.\nB T ESTDETAILS\nB.1 T ASK DESCRIPTIONS AND EXAMPLES\nWe provide analysis of question length and difﬁculty in Figure 12. We list all tasks and the topics\nthey test in Table 2. We also provide an example for each task starting with Figure 14.\n0 500 1000 1500 2000 2500 3000\nQuestion Length (Characters)0.00.20.40.60.81.0Confidence of True Label\nGPT-3 Question Length and Correctness\n0 250 500 750 1000 1250\nAverage Question Length (Characters)0.00.20.40.60.81.0Subject Accuracy\nGPT-3 Average Question Length and\nAccuracy by Subject\nFigure 12: Figures on the relation between question difﬁculty and question length. For questions\nlonger than a tweet (280 characters), the correlation between question length and true label conﬁdence\nis slightly positive. This shows that longer questions are not necessarily harder.\nB.2 E XACT QUESTION AND ANSWER CONTAMINATION\nSince language models train on vast text corpora, there is some chance that they have seen the exact\nquestion and answer during pretraining. If they memorized the exact question and answer, then\nthey would attain higher accuracy than their true ability. Likewise, a question’s entropy would be\nespecially low if it were memorized. Memorized questions and answers should have low entropy and\n13'
 ""Published as a conference paper at ICLR 2021\n0 20 40 60 80 100\nAccuracy (%)World ReligionsVirologyUS Foreign PolicySociologySecurity StudiesPublic RelationsProfessional PsychologyProfessional MedicineProfessional LawProfessional AccountingPrehistoryPhilosophyNutritionMoral ScenariosMoral DisputesMiscellaneousMedical GeneticsMarketingManagementMachine LearningLogical FallaciesJurisprudenceInternational LawHuman SexualityHuman AgingHigh School World HistoryHigh School US HistoryHigh School StatisticsHigh School PsychologyHigh School PhysicsHigh School MicroeconomicsHigh School MathematicsHigh School MacroeconomicsHigh School Gov't and PoliticsHigh School GeographyHigh School European HistoryHigh School Comp SciHigh School ChemistryHigh School BiologyGlobal FactsFormal LogicElementary MathematicsElectrical EngineeringEconometricsConceptual PhysicsComputer SecurityCollege PhysicsCollege MedicineCollege MathematicsCollege Comp SciCollege ChemistryCollege BiologyClinical KnowledgeBusiness EthicsAstronomyAnatomyAbstract AlgebraGPT-3\nUnifiedQA\nRandom\nFigure 6: GPT-3 (few-shot) and UniﬁedQA results.Few-Shot Prompt. We feed GPT-3 prompts\nlike that shown in Figure 1a. We begin each\nprompt with “The following are multiple choice\nquestions (with answers) about [subject].” For\nzero-shot evaluation, we append the question to\nthe prompt. For few-shot evaluation, we add up\nto5demonstration examples with answers to\nthe prompt before appending the question. All\nprompts end with “Answer: ”. The model then\nproduces probabilities for the tokens “A,” “B,”\n“C,” and “D,” and we treat the highest probability\noption as the prediction. For consistent evalua-\ntion, we create a dev set with 5ﬁxed few-shot\nexamples for each subject.\n4.2 R ESULTS\nModel Size and Accuracy. We compare the\nfew-shot accuracy of each GPT-3 size in Table 1.\nWe ﬁnd that the three smaller GPT-3 models\nhave near random accuracy (around 25%). In\ncontrast, we ﬁnd that the X-Large 175billion\nparameter GPT-3 model performs substantially\nbetter than random, with an accuracy of 43.9%.\nWe also ﬁnd qualitatively similar results in the\nzero-shot setting. While the smaller models\nhave around 25% zero-shot accuracy, Figure 10\nin Appendix A shows that the largest GPT-3\nmodel has a much higher zero-shot accuracy of\nabout 37.7%. Brown et al. (2020) also observe\nthat larger GPT-3 models perform better, though\nprogress tends to be steadier. In Figure 1b we\nshow that non-random accuracy on the multitask\ntest emerged with recent large few-shot models\ncompared to datasets that assess commonsense\nand linguistic understanding.\nTo test the usefulness of ﬁne-tuning instead of\nfew-shot learning, we also evaluate UniﬁedQA\nmodels. UniﬁedQA has the advantage of being\nﬁne-tuned on other question answering datasets,\nunlike GPT-3. We assess UniﬁedQA by evalu-\nating its transfer performance without any ad-\nditional ﬁne-tuning. The largest UniﬁedQA\nmodel we test has 11billion parameters, which\nis slightly smaller than GPT-3 Large. Neverthe-\nless, we show in Table 1 that it attains 48.9%\naccuracy. This performs better than the few-shot GPT-3 X-Large model, despite UniﬁedQA have\nan order of magnitude fewer parameters. We also ﬁnd that even the smallest UniﬁedQA variant,\nwith just 60million parameters, has approximately 29.3%accuracy. These results suggest that while\nmodel size is a key component for achieving strong performance, ﬁne-tuning also helps.\nComparing Disciplines. Using our test, we discover that GPT-3 and UniﬁedQA have lopsided\nperformance and several substantial knowledge gaps. Figure 6 shows the accuracy of GPT-3 (few-\nshot) and UniﬁedQA for all 57tasks. It shows the both models are below expert-level performance\nfor all tasks, with GPT-3’s accuracy ranging from 69% for US Foreign Policy to 26% for College\nChemistry. UniﬁedQA does best on marketing, with an accuracy of 82.5%.\nOverall, models do poorly on highly procedural problems. Figure 6 shows that calculation-heavy""]","Models like GPT-3 are poorly calibrated, with their confidence often not accurately reflecting their actual performance across different subjects, indicating a need for improvement in model calibration techniques.",0.999999999975,1.0,0.9297196902697197,1.0,0.4802754600394664,0.9211018401578656
"Without needing training on a separate classifier, how does DetectGPT determine if a passage was generated by an LLM?","DetectGPT determines if a passage was generated by an LLM by leveraging the structure of the LLM's probability function, specifically focusing on the log probabilities of the text. It generates minor perturbations of the passage using a generic pre-trained model and compares the log probability of the original sample with those of the perturbed samples. If the average log ratio is high, the sample is likely from the source model.","['Zero-Shot Machine-Generated Text Detection using Probability Curvature\ngenerated text detection. Other work explores watermarks\nfor generated text (Kirchenbauer et al., 2023), which modify\na model’s generations to make them easier to detect. Our\nwork does not assume text is generated with the goal of easy\ndetection; DetectGPT detects text generated from publicly\navailable LLMs using standard LLM sampling strategies.\nThe widespread use of LLMs has led to much other con-\ntemporaneous work on detecting LLM output. Sadasivan\net al. (2023) show that the detection AUROC of the an de-\ntector is upper bounded by a function of the TV distance\nbetween the model and human text. However, we find that\nAUROC of DetectGPT is high even for the largest publicly-\navailable models (Table 2), suggesting that TV distance may\nnot correlate strongly with model scale and capability. This\ndisconnect may be exacerbated by new training objectives\nother than maximum likelihood, e.g., reinforcement learn-\ning with human feedback (Christiano et al., 2017; Ziegler\net al., 2020). Both Sadasivan et al. (2023) and Krishna et al.\n(2023) show the effectiveness of paraphrasing as a tool for\nevading detection, suggesting an important area of study\nfor future work. Liang et al. (2023) show that multi-lingual\ndetection is difficult, with non-DetectGPT detectors show-\ning bias against non-native speakers; this result highlights\nthe advantage of zero-shot detectors like DetectGPT, which\ngeneralize well to any data generated by the original gener-\nating model. Mireshghallah et al. (2023) study which proxy\nscoring models produce the most useful log probabilities\nfor detection when the generating model is not known (a\nlarge-scale version of our Figure 6). Surprisingly (but con-\nsistent with our findings), they find that smaller models are\nin fact better proxy models for performing detection with\nperturbation-based methods like DetectGPT.\nThe problem of machine-generated text detection echoes ear-\nlier work on detecting deepfakes, artificial images or videos\ngenerated by deep nets, which has spawned substantial ef-\nforts in detection of fake visual content (Dolhansky et al.,\n2020; Zi et al., 2020). While early works in deepfake de-\ntection used relatively general-purpose model architectures\n(G¨uera & Delp, 2018), many deepfake detection methods\nrely on the continuous nature of image data to achieve state-\nof-the-art performance (Zhao et al., 2021; Guarnera et al.,\n2020), making direct application to text difficult.\n3. The Zero-Shot Machine-Generated Text\nDetection Problem\nWe study zero-shot machine-generated text detection, the\nproblem of detecting whether a piece of text, or candidate\npassage x, is a sample from a source model pθ. The problem\nis zero-shot in the sense that we do not assume access to\nhuman-written or generated samples to perform detection.\nAs in prior work, we study a ‘white box’ setting (Gehrmann\net al., 2019) in which the detector may evaluate the log prob-Algorithm 1 DetectGPT model-generated text detection\n1:Input: passage x, source model pθ, perturbation function q,\nnumber of perturbations k, decision threshold ϵ\n2:˜xi∼q(· |x), i∈[1..k]// mask spans, sample replacements\n3:˜µ←1\nkP\nilogpθ(˜xi)// approximate expectation in Eq. 1\n4:ˆdx←logpθ(x)−˜µ // estimate d(x, pθ, q)\n5:˜σ2\nx←1\nk−1P\ni(logpθ(˜xi)−˜µ)2// variance for normalization\n6:ifˆdx√˜σx> ϵthen\n7: return true // probably model sample\n8:else\n9: return false // probably not model sample\nability of a sample logpθ(x). The white box setting does\nnotassume access to the model architecture or parameters.\nMost public APIs for LLMs (such as GPT-3) enable scoring\ntext, though some exceptions exist, notably ChatGPT. While\nmost of our experiments consider the white box setting, see\nSection 5.2 for experiments in which we score text using\nmodels other than the source model. See Mireshghallah\net al. (2023) for a comprehensive evaluation in this setting.\nThe detection criterion we propose, DetectGPT, also makes'
 'have evaluated the detection performance of supervised\nmethods (typically fine-tuned transformers) for detecting\nmachine-generated text. In this section, we explore several\ndomains to better understand the relative strengths of super-\nvised and zero-shot detectors. The results are presented in\nFigure 4, using 200 samples from each dataset for evalua-\ntion. We find that supervised detectors can provide similar\ndetection performance to DetectGPT on in-distribution data\nlike English news, but perform significantly worse than zero-\nshot methods in the case of English scientific writing and\nfail altogether for German writing. This finding echoes past\nwork showing that language models trained for machine-\ngenerated text detection overfit to their training data (source\nmodel, decoding strategy, topic, language, etc.; Uchendu\net al. (2020); Ippolito et al. (2020); Jawahar et al. (2020)).\nIn contrast, zero-shot methods generalize relatively easily\nto new languages and domains; DetectGPT’s performance\nin particular is mostly unaffected by the change in language\nfrom English to German.\nWhile our experiments have shown that DetectGPT is ef-\nfective on a variety of domains and models, it is natural to\nwonder if it is effective for the largest publicly-available\nLMs. Therefore, we also evaluate multiple zero-shot and su-\npervised methods on two 175B parameter models, OpenAI’s\nGPT-3 and AI21 Labs’ Jurassic-2 Jumbo. Because neither\nAPI provides access to the complete conditional distribution\n4The overall ease of detecting machine-generated fake writing\ncorroborates anecdotal reporting that machine-generated creative\nwriting tends to be noticeably generic, and therefore relatively easy\nto detect (Roose & Newton, 2022).\n6'
 'DetectGPT: Zero-Shot Machine-Generated Text Detection\nusing Probability Curvature\nEric Mitchell1Yoonho Lee1Alexander Khazatsky1Christopher D. Manning1Chelsea Finn1\nAbstract\nThe increasing fluency and widespread usage of\nlarge language models (LLMs) highlight the de-\nsirability of corresponding tools aiding detection\nof LLM-generated text. In this paper, we identify\na property of the structure of an LLM’s proba-\nbility function that is useful for such detection.\nSpecifically, we demonstrate that text sampled\nfrom an LLM tends to occupy negative curva-\nture regions of the model’s log probability func-\ntion. Leveraging this observation, we then define\na new curvature-based criterion for judging if a\npassage is generated from a given LLM. This\napproach, which we call DetectGPT, does not re-\nquire training a separate classifier, collecting a\ndataset of real or generated passages, or explic-\nitly watermarking generated text. It uses only\nlog probabilities computed by the model of in-\nterest and random perturbations of the passage\nfrom another generic pre-trained language model\n(e.g., T5). We find DetectGPT is more discrimi-\nnative than existing zero-shot methods for model\nsample detection, notably improving detection of\nfake news articles generated by 20B parameter\nGPT-NeoX from 0.81 AUROC for the strongest\nzero-shot baseline to 0.95 AUROC for Detect-\nGPT. See ericmitchell.ai/detectgpt\nfor code, data, and other project information.\n1. Introduction\nLarge language models (LLMs) have proven able to gen-\nerate remarkably fluent responses to a wide variety of user\nqueries. Models such as GPT-3 (Brown et al., 2020), PaLM\n(Chowdhery et al., 2022), and ChatGPT (OpenAI, 2022)\ncan convincingly answer complex questions about science,\nmathematics, historical and current events, and social trends.\n1Stanford University. Correspondence to: Eric Mitchell\n<eric.mitchell@cs.stanford.edu >.\nProceedings of the 40thInternational Conference on Machine\nLearning , Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).\n Candidate passage : “Joe Biden recently made a move to the White House that included bringing along his pet German Shepherd…”\nDetectGPT\nx\n...GPT-3(1)Perturb(2) Score(3) Compare\n🤖  from GPT-3\nYes(reword with T5)\n“made a move”  “moved”→“pet”  “dog”→Delete “bringing along”\n...\n🤔  from other source\nNo\nFigure 1. We aim to determine whether a piece of text was gener-\nated by a particular LLM p, such as GPT-3. To classify a candidate\npassage x, DetectGPT first generates minor perturbations of the\npassage ˜xiusing a generic pre-trained model such as T5. Then\nDetectGPT compares the log probability under pof the original\nsample xwith each perturbed sample ˜xi. If the average log ratio\nis high, the sample is likely from the source model.\nWhile recent work has found that cogent-sounding LLM-\ngenerated responses are often simply wrong (Lin et al.,\n2022), the articulate nature of such generated text may still\nmake LLMs attractive for replacing human labor in some\ncontexts, notably student essay writing and journalism. At\nleast one major news source has released AI-written content\nwith limited human review, leading to substantial factual er-\nrors in some articles (Christian, 2023). Such applications of\nLLMs are problematic for a variety of reasons, making fair\nstudent assessment difficult, impairing student learning, and\nproliferating convincing-but-inaccurate news articles. Un-\nfortunately, humans perform only slightly better than chance\nwhen classifying machine-generated vs human-written text\n(Gehrmann et al., 2019), leading researchers to consider\nautomated detection methods that may identify signals dif-\nficult for humans to recognize. Such methods might give\nteachers and news-readers more confidence in the human\norigin of the text that they consume.\nAs in prior work (Jawahar et al., 2020), we study the\n1arXiv:2301.11305v2  [cs.CL]  23 Jul 2023'
 'signal for zero-shot detection.\nDetection when the source model is unknown. While\nour experiments have focused on the white-box setting\nfor machine-generated text detection, in this section, we\nGPT-J GPT-Neo GPT-2Scoring ModelGPT-J GPT-Neo GPT-2Base Model0.92\n(0.02)0.83\n(0.04)0.79\n(0.02)\n0.64\n(0.06)0.97\n(0.01)0.83\n(0.02)\n0.60\n(0.09)0.85\n(0.05)0.99\n(0.00)0.85\n0.81\n0.81\n0.72 0.88 0.87\nFigure 6. DetectGPT performs\nbest when scoring samples\nwith the same model that gen-\nerated them (diagonal), but\nthe column means suggest that\nsome models (GPT-Neo, GPT-\n2) may be better ‘scorers’ than\nothers (GPT-J). White values\nshow mean (standard error)\nAUROC over XSum, SQuAD,\nand WritingPrompts; black\nshows row/column mean.explore the effect of using\na different model to score a\ncandidate passage (and per-\nturbed texts) than the model\nthat generated the passage.\nIn other words, we aim\nto classify between human-\ngenerated text and text from\nmodel A, but without ac-\ncess to model Ato com-\npute log probabilities. In-\nstead, we use log probabil-\nities computed by a surro-\ngate model B. We con-\nsider three models, GPT-J,\nGPT-Neo-2.7, and GPT-2,\nevaluating all possible com-\nbinations of source model\nand surrogate model (9 to-\ntal). We average the perfor-\nmance across 200 samples\nfrom XSum, SQuAD, and\n7']",DetectGPT uses the log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model.,0.999999999975,1.0,0.9183625364669097,1.0,,0.9417988851982861
"What role do random perturbations play in DetectGPT's methodology, and how are they applied?","In DetectGPT's methodology, random perturbations are used to generate slightly modified versions of a candidate passage, which helps analyze the differences in log probabilities between machine-generated and human-written texts. The perturbation function produces these variations while preserving the original meaning, allowing the system to estimate the perturbation discrepancy. This discrepancy serves as a key signal for distinguishing between model-generated and human-generated text.","['models other than the source model. See Mireshghallah\net al. (2023) for a comprehensive evaluation in this setting.\nThe detection criterion we propose, DetectGPT, also makes\nuse of generic pre-trained mask-filling models in order to\ngenerate passages that are ‘nearby’ the candidate passage.\nHowever, these mask-filling models are used off-the-shelf,\nwithout any fine-tuning or adaptation to the target domain.\n4. DetectGPT: Zero-shot Machine-Generated\nText Detection with Random Perturbations\nDetectGPT is based on the hypothesis that samples from a\nsource model pθtypically lie in areas of negative curvature\nof the log probability function of pθ, unlike human text. In\nother words, if we apply small perturbations to a passage\nx∼pθ, producing ˜x, the quantity logpθ(x)−logpθ(˜x)\nshould be relatively large on average for machine-generated\nsamples compared to human-written text. To leverage this\nhypothesis, first consider a perturbation function q(· |x)\nthat gives a distribution over ˜x, slightly modified versions of\nxwith similar meaning (we will generally consider roughly\nparagraph-length texts x). As an example, q(· |x)might be\nthe result of simply asking a human to rewrite one of the\nsentences of x, while preserving the meaning of x. Using\nthe notion of a perturbation function, we can define the\nperturbation discrepancy d(x, pθ, q):\nd(x, pθ, q)≜logpθ(x)−E˜x∼q(·|x)logpθ(˜x)(1)\nWe state our hypothesis more formally as the Local Pertur-\nbation Discrepancy Gap Hypothesis, which describes a gap\nin the perturbation discrepancy for model-generated text\nand human-generated text.\nPerturbation Discrepancy Gap Hypothesis. Ifqproduces\nsamples on the data manifold, d(x, pθ, q)is positive and\nlarge with high probability for samples x∼pθ. For human-\nwritten text, d(x, pθ, q)tends toward zero for all x.\n3'
 'Zero-Shot Machine-Generated Text Detection using Probability Curvature\n60M 220M 770M 2.7B0.50.60.70.80.91.0Detection AUROC\n5 perturbations\n60M 220M 770M 2.7B\n25 perturbations\nRandom\nGPT2-sm\nGPT2-md\nGPT2-lg\nGPT2-xl\nMask filling model size (# parameters)\nFigure 7. There is a clear association between capacity of mask-\nfilling model and detection performance, across source model\nscales. Random mask filling (uniform sampling from mask filling\nmodel vocabulary) performs poorly, reinforcing the idea that the\nperturbation function should produce samples on the data manifold.\nCurves show AUROC scores on 200 SQuAD contexts.\nWritingPrompts. The results are presented in Figure 6,\nshowing that when the surrogate model is different from the\nsource model, detection performance is reduced, indicating\nthat DetectGPT is most suited to the white-box setting. Yet\nwe also observe that if we fix the model used for scoring\nand average across source models whose generations are\ndetected (average within column), there is significant varia-\ntion in AUROC; GPT-2 and GPT-Neo-2.7 seem to be better\n‘scorers’ than GPT-J. These variations in cross-model scor-\ning performance suggest ensembling scoring models may\nbe a useful direction for future research; see Mireshghallah\net al. (2023) for reference.\n5.3.Other factors impacting performance of DetectGPT\nIn this section, we explore how factors such as the size of the\nmask-filling model, the number of perturbations used to es-\ntimate the expectation in Equation 1, or the data distribution\nof the text to be detected impact detection quality.\nSource and mask-filling model scale. Here we study the\nimpact of the size of the source model and mask-filling\nmodel on DetectGPT’s performance; the results are shown\nin Figure 7. In particular, the increased discrimination power\nof DetectGPT for larger mask-filling models supports the\ninterpretation that DetectGPT is estimating the curvature\nof the log probability in a latent semantic space, rather\nthan in raw token embedding space. Larger T5 models\nbetter represent this latent space, where random directions\ncorrespond to meaningful changes in the text.\nNumber of perturbations for DetectGPT. We evaluate the\nperformance of DetectGPT as a function of the number of\nperturbations used to estimate the expectation in Equation 1\non three datasets. The results are presented in Figure 8.\nDetection accuracy continues to improve until 100 pertur-\nbations, where it converges. Evaluations use 100 examples\nfrom each dataset.\nData distributional properties. We study more closely\nthe impact of the data distribution on DetectGPT, particu-\n1 10 100 10000.60.70.80.91.0Detection AUROC\nGPT-2\nXSum\nSQuAD\nWritingPrompts\n1 10 100 1000\nGPT-J\nNumber of perturbationsFigure 8. Impact of varying the number of perturbations (samples\nof mask and mask-fill) used by DetectGPT on AUROC for GPT-2\n(left) and GPT-J ( right ) to estimate the perturbation discrepancy\non detection. Averaging up to 100 perturbations greatly increases\nDetectGPT’s reliability. Perturbations sampled from T5-large.\nlarly how the domain impacts the threshold separating the\nperturbation discrepancy distributions of model-generated\nand human texts as well as the impact of passage length on\ndetection. Figure 9 shows the perturbation discrepancy dis-\ntributions for model-generated and human texts across four\ndata distributions, using GPT-Neo-2.7B to generate sam-\nples. A threshold of slightly below 0.1 separates human and\nmodel texts across data distributions, which is important for\npractical scenarios in which a passage may be analyzed with-\nout knowing its domain a priori. Finally, Figure 10 shows an\nanalysis of DetectGPT’s performance as a function of pas-\nsage length. We bin the paired human- and model-generated\nsequences by their average length into three bins of equal\nsize (bottom/middle/top third), and plot the AUROC within\neach bin. The relationship between detection performance'
 'models continually improve their reproductions of human\ntext. Separately, the results in Section 5.2 suggest that ex-\ntending DetectGPT to use ensembles of models for scoring,\nrather than a single model, may improve detection in the\nblack box setting. Another topic that remains unexplored\nis the relationship between prompting and detection; that\nis, can a clever prompt successfully prevent a model’s gen-\nerations from being detected by existing methods? Finally,\nfuture work may explore whether the local log probabil-\nity curvature property we identify is present for generative\nmodels in other domains, such as audio, video, or images.\nWe hope that the present work serves as inspiration to fu-\nture work developing effective, general-purpose methods\nfor mitigating potential harms of machine-generated media.\nAcknowledgements\nEM gratefully acknowledges funding from a Knight-\nHennessy Graduate Fellowship. CF and CM are CIFAR\nFellows. The Stanford Center for Research on Foundation\nModels (CRFM) provided part of the compute resources\nused for the experiments in this work.\n9'
 'Zero-Shot Machine-Generated Text Detection using Probability Curvature\n0.0 0.1 0.20204060XSum\n0.1\n 0.0 0.1 0.2WritingPrompts\n0.0 0.1 0.20204060SQuAD\nHuman\nModel\n0.1\n 0.0 0.1 0.2 0.3PubMed\n0.0 0.2 0.4 0.6 0.8 1.0\nLog Probability Change (Perturbation Discrepancy)0.00.20.40.60.81.0Frequency\nFigure 9. Perturbation discrepancy distributions for GPT-Neo\n(2.7B) and humans across domains. A threshold of 0.1 gener-\nally separates model- and human-generated text well, which is\nimportant for practical scenarios where the domain is unknown.\nidentify a property of the log probability function computed\nby a wide variety of large language models, showing that a\ntractable approximation to the trace of the Hessian of the\nmodel’s log probability function provides a useful signal\nfor detecting model samples. Our experiments find that\nthis signal is more discriminative than existing zero-shot\ndetection methods and is competitive with bespoke detection\nmodels trained with millions of model samples.\nDetectGPT and Watermarking. One interpretation of\nthe perturbation function is producing semantically similar\nrephrasings of the original passage . If these rephrasings\nare systematically lower-probability than the original pas-\nsage, the model is exposing its bias toward the specific (and\nroughly arbitrary, by human standards) phrasing used. In\nother words, LLMs that do not perfectly imitate human\nwriting essentially watermark themselves implicitly. Under\nthis interpretation, efforts to manually add watermarking bi-\nases to model outputs (Aaronson, 2022; Kirchenbauer et al.,\n2023) may further improve the effectiveness of methods\nsuch as DetectGPT, even as LLMs continue to improve.\nLimitations. One limitation of probability-based methods\nfor zero-shot machine-generated text detection (like Detect-\nGPT) is the white-box assumption that we can evaluate log\nprobabilities of the model(s) in question. For models be-\nhind APIs that do provide probabilities (such as GPT-3),\nevaluating probabilities nonetheless costs money. Another\nassumption of DetectGPT is access to a reasonable pertur-\nbation function. While in this work, we use off-the-shelf\nmask-filling models such as T5 and mT5 (for non-English\nlanguages), some domains may see reduced performance\nif existing mask-filling models do not well represent the\nspace of meaningful rephrases, reducing the quality of the\ncurvature estimate. While DetectGPT provides the best\navailable detection performance for PubMedQA, its drop\nin performance compared to other datasets may be a result\nAverage length0.9850.9900.995AUROC\ngpt-2\nAverage length0.960.970.980.99AUROC\nopt-2.7\nXSum\nSQuAD\nWritingPrompts\n130 140 150 160 170\nAverage length0.8750.9000.9250.9500.975AUROC\nEleutherAI/gpt-j-6b\n130 140 150 160 170\nAverage length0.70.80.9AUROC\nEleutherAI/gpt-neox-20bFigure 10. DetectGPT AUROC vs passage length. The relation-\nship between detection performance and passage length generally\ndepends on the dataset and model (or tokenizer). Decreases in\ndetection quality with increasing length may be due to T5 failing\nto track many (20+) masks to fill at once; this problem may be\nmitigated by applying mask-fills in a sequence of smaller batches.\nof lower quality perturbations. Finally, DetectGPT is more\ncompute-intensive than other methods for detection, as it\nrequires sampling and scoring the set of perturbations for\neach candidate passage, rather than just the candidate pas-\nsage; a better tuned perturbation function or more efficient\ncurvature approximation may help mitigate these costs.\nFuture Work. While the methods in this work make no\nassumptions about the models generating the samples, fu-\nture work may explore how watermarking algorithms can be\nused in conjunction with detection algorithms like Detect-\nGPT to further improve detection robustness as language\nmodels continually improve their reproductions of human\ntext. Separately, the results in Section 5.2 suggest that ex-\ntending DetectGPT to use ensembles of models for scoring,']","Random perturbations are used to slightly modify the text, keeping its meaning similar. DetectGPT evaluates the change in log probability caused by these perturbations to detect machine-generated text.",0.9166666666361111,1.0,0.8429106699479375,1.0,0.9880007698567465,0.9520030794269858
What specific architectural changes were made to develop DistilBERT from BERT?,"The specific architectural changes made to develop DistilBERT from BERT include rearranging the order of layer normalization and residual connections to improve stability during training. This modification allowed for the scaling of BERT-style models beyond the original BERT architecture. Additionally, a triple loss combining language modeling, distillation, and cosine-distance losses was introduced to leverage the inductive biases learned by larger models during pre-training.","['6 Conclusion and future work\nWe introduced DistilBERT, a general-purpose pre-trained version of BERT, 40% smaller, 60% faster,\nthat retains 97% of the language understanding capabilities. We showed that a general-purpose\nlanguage model can be successfully trained with distillation and analyzed the various components\nwith an ablation study. We further demonstrated that DistilBERT is a compelling option for edge\napplications.\nReferences\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. In NAACL-HLT , 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\nunsupervised multitask learners. 2019.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar S. Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke S. Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv ,\nabs/1907.11692, 2019.\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green ai. ArXiv , abs/1907.10597, 2019.\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in\nnlp. In ACL, 2019.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In NIPS , 2017.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. Transformers: State-of-the-art natural language\nprocessing, 2019.\nCristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In KDD , 2006.\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. ArXiv ,\nabs/1503.02531, 2015.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading\nbooks. 2015 IEEE International Conference on Computer Vision (ICCV) , pages 19–27, 2015.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A\nmulti-task benchmark and analysis platform for natural language understanding. In ICLR , 2018.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. Deep contextualized word representations. In NAACL , 2018.\nAlex Wang, Ian F. Tenney, Yada Pruksachatkun, Katherin Yu, Jan Hula, Patrick Xia, Raghu Pappagari, Shuning\nJin, R. Thomas McCoy, Roma Patel, Yinghui Huang, Jason Phang, Edouard Grave, Najoung Kim, Phu Mon\nHtut, Thibault F’evry, Berlin Chen, Nikita Nangia, Haokun Liu, Anhad Mohananey, Shikha Bordia, Nicolas\nPatry, Ellie Pavlick, and Samuel R. Bowman. jiant 1.1: A software toolkit for research on general-purpose\ntext understanding models. http://jiant.info/ , 2019.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y . Ng, and Christopher Potts. Learning\nword vectors for sentiment analysis. In ACL, 2011.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for machine\ncomprehension of text. In EMNLP , 2016.\nRaphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. Distilling task-speciﬁc\nknowledge from bert into simple neural networks. ArXiv , abs/1903.12136, 2019.\nDebajyoti Chatterjee. Making neural machine reading comprehension faster. ArXiv , abs/1904.00796, 2019.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better: The impact\nof student initialization on knowledge distillation. ArXiv , abs/1908.08962, 2019.\nZe Yang, Linjun Shou, Ming Gong, Wutao Lin, and Daxin Jiang. Model compression with multi-task knowledge\ndistillation for web-scale question answering system. ArXiv , abs/1904.09636, 2019.'
 'DistilBERT, a distilled version of BERT: smaller,\nfaster, cheaper and lighter\nVictor SANH, Lysandre DEBUT, Julien CHAUMOND, Thomas WOLF\nHugging Face\n{victor,lysandre,julien,thomas}@huggingface.co\nAbstract\nAs Transfer Learning from large-scale pre-trained models becomes more prevalent\nin Natural Language Processing (NLP), operating these large models in on-the-\nedge and/or under constrained computational training or inference budgets remains\nchallenging. In this work, we propose a method to pre-train a smaller general-\npurpose language representation model, called DistilBERT, which can then be ﬁne-\ntuned with good performances on a wide range of tasks like its larger counterparts.\nWhile most prior work investigated the use of distillation for building task-speciﬁc\nmodels, we leverage knowledge distillation during the pre-training phase and show\nthat it is possible to reduce the size of a BERT model by 40%, while retaining 97%\nof its language understanding capabilities and being 60% faster. To leverage the\ninductive biases learned by larger models during pre-training, we introduce a triple\nloss combining language modeling, distillation and cosine-distance losses. Our\nsmaller, faster and lighter model is cheaper to pre-train and we demonstrate its\ncapabilities for on-device computations in a proof-of-concept experiment and a\ncomparative on-device study.\n1 Introduction\nFigure 1: Parameter counts of several recently released\npretrained language models.The last two years have seen the rise\nof Transfer Learning approaches in\nNatural Language Processing (NLP)\nwith large-scale pre-trained language\nmodels becoming a basic tool in\nmany NLP tasks [Devlin et al., 2018,\nRadford et al., 2019, Liu et al., 2019].\nWhile these models lead to signiﬁ-\ncant improvement, they often have\nseveral hundred million parameters\nand current research1on pre-trained\nmodels indicates that training even\nlarger models still leads to better per-\nformances on downstream tasks.\nThe trend toward bigger models\nraises several concerns. First is the\nenvironmental cost of exponentially scaling these models’ computational requirements as mentioned\nin Schwartz et al. [2019], Strubell et al. [2019]. Second, while operating these models on-device\nin real-time has the potential to enable novel and interesting language processing applications, the\ngrowing computational and memory requirements of these models may hamper wide adoption.\n1See for instance the recently released MegatronLM ( https://nv-adlr.github.io/MegatronLM )\nEMC^2: 5th Edition Co-located with NeurIPS’19arXiv:1910.01108v4  [cs.CL]  1 Mar 2020'
 'demonstrated that rearranging the order of the layer nor-\nmalization and the residual connections as shown in Figure\n7 is critical to enable the scaling of the BERT-style mod-\nels beyond BERT-Large. The architecture (b) in Figure 7\neliminates instabilities observed using the original BERT\narchitecture in (a) and also has a lower training loss. To\nthe best of our knowledge, we are the ﬁrst to report such a\nchange enables training larger BERT models.'
 'Table 1: DistilBERT retains 97% of BERT performance. Comparison on the dev sets of the\nGLUE benchmark. ELMo results as reported by the authors. BERT and DistilBERT results are the\nmedians of 5 runs with different seeds.\nModel Score CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B WNLI\nELMo 68.7 44.1 68.6 76.6 71.1 86.2 53.4 91.5 70.4 56.3\nBERT-base 79.5 56.3 86.7 88.6 91.8 89.6 69.3 92.7 89.0 53.5\nDistilBERT 77.0 51.3 82.2 87.5 89.2 88.5 59.9 91.3 86.9 56.3\nTable 2: DistilBERT yields to comparable\nperformance on downstream tasks. Com-\nparison on downstream tasks: IMDb (test ac-\ncuracy) and SQuAD 1.1 (EM/F1 on dev set).\nD: with a second step of distillation during\nﬁne-tuning.\nModel IMDb SQuAD\n(acc.) (EM/F1)\nBERT-base 93.46 81.2/88.5\nDistilBERT 92.82 77.7/85.8\nDistilBERT (D) - 79.1/86.9Table 3: DistilBERT is signiﬁcantly smaller\nwhile being constantly faster. Inference\ntime of a full pass of GLUE task STS-B (sen-\ntiment analysis) on CPU with a batch size of\n1.\nModel # param. Inf. time\n(Millions) (seconds)\nELMo 180 895\nBERT-base 110 668\nDistilBERT 66 410\nDistillation We applied best practices for training BERT model recently proposed in Liu et al. [2019].\nAs such, DistilBERT is distilled on very large batches leveraging gradient accumulation (up to 4K\nexamples per batch) using dynamic masking and without the next sentence prediction objective.\nData and compute power We train DistilBERT on the same corpus as the original BERT model:\na concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015]. DistilBERT\nwas trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the\nRoBERTa model [Liu et al., 2019] required 1 day of training on 1024 32GB V100.\n4 Experiments\nGeneral Language Understanding We assess the language understanding and generalization ca-\npabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark\n[Wang et al., 2018], a collection of 9 datasets for evaluating natural language understanding systems.\nWe report scores on the development sets for each task by ﬁne-tuning DistilBERT without the use\nof ensembling or multi-tasking scheme for ﬁne-tuning (which are mostly orthogonal to the present\nwork). We compare the results to the baseline provided by the authors of GLUE: an ELMo (Peters\net al. [2018]) encoder followed by two BiLSTMs.4\nThe results on each of the 9 tasks are showed on Table 1 along with the macro-score (average of\nindividual scores). Among the 9 tasks, DistilBERT is always on par or improving over the ELMo\nbaseline (up to 19 points of accuracy on STS-B). DistilBERT also compares surprisingly well to\nBERT, retaining 97% of the performance with 40% fewer parameters.\n4.1 Downstream task benchmark\nDownstream tasks We further study the performances of DistilBERT on several downstream tasks\nunder efﬁcient inference constraints: a classiﬁcation task (IMDb sentiment classiﬁcation - Maas et al.\n[2011]) and a question answering task (SQuAD v1.1 - Rajpurkar et al. [2016]).\nAs shown in Table 2, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb\nbenchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.9 points of the full BERT.\nWe also studied whether we could add another step of distillation during the adaptation phase by\nﬁne-tuning DistilBERT on SQuAD using a BERT model previously ﬁne-tuned on SQuAD as a\n4We use jiant [Wang et al., 2019] to compute the baseline.\n3']","DistilBERT omits token-type embeddings and the pooler layer, and reduces the number of layers by half compared to BERT.",0.499999999975,1.0,0.9934057768202676,0.0,0.2143140172200794,0.8572560688803176
What core challenge does HellaSwag aim to address in the context of state-of-the-art models' capabilities in commonsense natural language inference (NLI)?,"HellaSwag aims to address the challenge that state-of-the-art models, like BERT, struggle with commonsense natural language inference despite achieving high accuracy on other benchmarks. It demonstrates that these models often rely on spurious patterns and dataset-specific biases rather than robust commonsense reasoning. The dataset's design reveals that even advanced models are easily fooled by nonsensical generated text that aligns with superficial language patterns.","['BERT, perform at commonsense natural language\ninference (NLI)? Our surprising conclusion is\nthat the underlying task remains unsolved. In-\ndeed, we ﬁnd that deep models such as BERT do\nnot demonstrate robust commonsense reasonining\nability by themselves. Instead, they operate more\nlikerapid surface learners for a particular dataset.\nTheir strong performance on SWAG is dependent\non the ﬁnetuning process, wherein they largely\nlearn to pick up on dataset-speciﬁc distributional\nbiases. When the distribution of language shifts\nslightly, performance drops drastically – even if\nthe domain remains identical.\nWe study this question by introducing Hella-\nSwag ,2a new benchmark for commonsense\nNLI. We use Adversarial Filtering (AF), a data-\ncollection paradigm in which a series of discrim-\ninators is used to select a challenging set of gen-\nerated wrong answers. AF is surprisingly e ﬀec-\ntive towards this goal: the resulting dataset of 70k\nproblems is easy for humans (95.6% accuracy),\nyet challenging for machines ( ă50%q. This result\nholds even when models are given a signiﬁcant\nnumber of training examples, and even when the\ntest data comes from the exact same distribution\nas the training data. Machine performance slips\nan additional 5% when evaluated on examples that\ncover novel concepts from the same domain.\nTo make this dataset robust to deep pre-\ntrained models, we use a trifecta of state-of-the-\nart generators (Radford et al., 2018), state-of-\nthe-art discriminators (BERT), and high quality\nsource text. We expand on the SWAG’s origi-\nnal video-captioning domain by using WikiHow\narticles, greatly increasing the context diversity\nand generation length. Our investigation reveals\na Goldilocks zone – roughly three sentences of\ncontext, and two generated sentences – wherein\ngenerations are largely nonsensical, even though\nstate-of-the-art discriminators cannot reliably tell\nthe di ﬀerence between these generations and the\nground truth.\nMore broadly, our paper presents a case-study\ntowards a future of veriﬁed progress in NLP, via it-\nerative rounds of building and breaking datasets. If\nour ultimate goal is to provide reliable benchmarks\nfor challenging tasks, such as commonsense NLI,\nthese benchmarks cannot be static. Instead, they\nmust evolve together with the evolving state-of-\n2Short for Harder Endings, Longer contexts, and Low-\nshot Activities for Situations WithAdversarial Generations.\nDataset and code at https: //rowanzellers.com /hellaswag.\nContext 2Context 1\nContext N…\nContext 1Context M…Real ending…Real ending(N instances)(M instances)DtrainReal ending…Real endingReal ending\nGen’d ending K Gen’d ending K Gen’d ending K ……………Gen’d ending2 …Gen’d ending2 Gen’d ending2 Gen’d ending 1 …Gen’d ending 1 Gen’d ending 1 DtestGen’d ending2 …Gen’d ending 1…………Gen’d ending 2Gen’d ending 1 Gen’d ending K Gen’d ending K …\nf\nTrain f to discriminate real vs. generated\nReplace easily-classiﬁed generations with adversarial ones that currently aren’t includedGenerated Ending (context M)Generated Ending (context 2)New!New!Figure 2: An overview of Adversarial Filtering. On\neach iteration, a new classiﬁer is trained on a dummy\ntraining set Dtrainto replace easily-classiﬁed negative\nendings on the dummy test set Dtestwith adversarial\nendings. This process is repeated iteratively, to obtain\na challenging dataset regardless of the ﬁnal split.\nthe-art. Continued evolution in turn requires prin-\ncipled dataset creation algorithms. Whenever a\nnew iteration of a dataset is created, these algo-\nrithms must leverage existing modeling advance-\nments to ﬁlter out spurious biases. Only once this\ncycle becomes impossible can we say that the un-\nderlying task – as opposed an individual dataset –\nis solved.\n2 Background\nSWAG is a dataset for commonsense NLI. For\neach question, a model is given a context from a\nvideo caption and four ending choices for what\nmight happen next. Only one choice is right – the\nactual next caption of the video.'
 'lems of reporting bias (Gordon and Van Durme,\n2013). Alternatively, future models might answer\ncorrectly only by picking up on spurious patterns,\nin which case a new development of the bench-\nmark – using these models as adversaries – would\nplace us in the same position as we are right now.\nPut another way, for humans to answer Hella-\nSwag questions requires abstracting away from\nlanguage and modeling world states instead. We\npostulate that this is what separates solving the\ntask of commonsense NLI, as opposed to a par-\nticular dataset. Indeed, we ﬁnd that existing deep\nmethods often get fooled by lexical false friends.\nFor example, in the WikiHow example from Fig-\nure 10, BERT chooses an ending that matches\nthetechnology words in the context, rather than\nmatching the deeper topic: using technology as an\nexcuse for not doing homework.\n6.3 Towards a future of evolving benchmarks\nWhat happens when HellaSwag gets solved? We\nbelieve the answer is simple: crowdsource another\ndataset, with the same exact format, and see where\nmodels fail. Indeed, in our work we found this to\nbe straightforward from an algorithmic perspec-\ntive: by throwing in the best known generator\n(GPT) and the best known discriminator (BERT-\nLarge), we made a dataset that is adversarial - not\njust to BERT, but to all models we have access to.\nWhile this was easy algorithmically, care must\nbe taken from a data curation standpoint. Indeed,\nwe ﬁnd success exists within a Goldilocks zone:\nthe data source must be complex enough that state-\nof-the-art generators often make mistakes, while\nsimple enough such that discriminators often fail\nto catch them. This ties the future of SWAG-\nstyle benchmarks to progress on language gener-\nation: until generation is solved, commonsense\nNLI will remain unsolved. Even recent promis-\ning results on scaling up language models (Rad-\nford et al., 2019) ﬁnd problems in terms of consis-\ntency, with the best curated examples requiring 25\nrandom seeds.\n7 Conclusion\nIn this paper, we presented HellaSwag , a new\ndataset for physically situated commonsense rea-\nsoning. By constructing the dataset through ad-\nversarial ﬁltering, combined with state-of-the-art\nmodels for language generation and discrimina-\ntion, we produced a dataset that is adversarial tothe most robust models available – even when\nmodels are evaluated on items from the train-\ning distribution. In turn, we provided insight\ninto the inner workings of pretrained models, and\nsuggest a path for NLP progress going forward:\ntowards benchmarks that adversarially co-evolve\nwith evolving state-of-the-art models.\nAcknowledgments\nWe thank the reviewers, as well as Jesse Thoma-\nson, for their helpful feedback. We thank the\nMechanical Turk workers for their great work\nduring dataset collection. Thanks also to Zak\nStone and the Google Cloud TPU team for help\nwith the computing infrastructure. This work\nwas supported by the National Science Foundation\nthrough a Graduate Research Fellowship (DGE-\n1256082) and NSF grants (IIS-1524371, 1637479,\n165205, 1703166), the DARPA CwC program\nthrough ARO (W911NF-15-1-0543), the IARPA\nDIV A program through D17PC00343, the Sloan\nResearch Foundation through a Sloan Fellowship,\nthe Allen Institute for Artiﬁcial Intelligence, the\nNVIDIA Artiﬁcial Intelligence Lab, and gifts by\nGoogle and Facebook. The views and conclu-\nsions contained herein are those of the authors and\nshould not be interpreted as representing endorse-\nments of IARPA, DOI /IBC, or the U.S. Govern-\nment.\nReferences\nYonatan Belinkov and Yonatan Bisk. 2018. Synthetic\nand natural noise both break neural machine transla-\ntion. In ICLR . ICLR.\nQian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui\nJiang, and Diana Inkpen. 2017. Enhanced lstm for\nnatural language inference. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers) , vol-\nume 1, pages 1657–1668.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and'
 ""HellaSwag : Can a Machine Really Finish Your Sentence?\nRowan Zellers♠Ari Holtzman♠Yonatan Bisk♠Ali Farhadi♠♥Yejin Choi♠♥\n♠Paul G. Allen School of Computer Science & Engineering, University of Washington\n♥Allen Institute for Artiﬁcial Intelligence\nhttps://rowanzellers.com/hellaswag\nAbstract\nRecent work by Zellers et al. (2018) intro-\nduced a new task of commonsense natural lan-\nguage inference : given an event description\nsuch as “A woman sits at a piano,” a machine\nmust select the most likely followup: “She\nsets her ﬁngers on the keys.” With the intro-\nduction of BERT (Devlin et al., 2018), near\nhuman-level performance was reached. Does\nthis mean that machines can perform human\nlevel commonsense inference?\nIn this paper, we show that commonsense in-\nference still proves di ﬃcult for even state-\nof-the-art models, by presenting HellaSwag ,\na new challenge dataset. Though its ques-\ntions are trivial for humans ( ą95% accuracy),\nstate-of-the-art models struggle ( ă48%). We\nachieve this via Adversarial Filtering (AF), a\ndata collection paradigm wherein a series of\ndiscriminators iteratively select an adversarial\nset of machine-generated wrong answers. AF\nproves to be surprisingly robust. The key in-\nsight is to scale up the length and complex-\nity of the dataset examples towards a critical\n‘Goldilocks’ zone wherein generated text is\nridiculous to humans, yet often misclassiﬁed\nby state-of-the-art models.\nOur construction of HellaSwag , and its result-\ning di ﬃculty, sheds light on the inner work-\nings of deep pretrained models. More broadly,\nit suggests a new path forward for NLP re-\nsearch, in which benchmarks co-evolve with\nthe evolving state-of-the-art in an adversarial\nway, so as to present ever-harder challenges.\n1 Introduction\nImagine a woman chasing a dog around outside,\ntrying to give it a bath. What might happen next?\nHumans can read a narrative like this, shown in\nFigure 1, and connect it to a rich model of the\nworld: the dog is currently dry and not soapy, and\nit actively doesn’t want to be bathed. Thus, one\nA woman is outside with a bucket and a dog. The dog is running around trying to avoid a bath. She…A. rinses the bucket off with soap and blow dry the dog’s head.B. uses a hose to keep it from getting soapy.C. gets the dog wet, then it runs away again.D. gets into a bath tub with the dog.Come to a complete halt at a stop sign or red light. At a stop sign, come to a complete halt for about 2 seconds or until vehicles that arrived before you clear the intersection. If you're stopped at a red light, proceed when the light has turned green. …A. Stop for no more than two seconds, or until the light turns yellow. A red light in front of you indicates that you should stop.B. After you come to a complete stop, turn off your turn signal. Allow vehicles to move in different directions before moving onto the sidewalk.C. Stay out of the oncoming traﬃc. People coming in from behind may elect to stay left or right.D. If the intersection has a white stripe in your lane, stop before this line. Wait until all traﬃc has cleared before crossing the intersection.\nOpenAIGPT\nHow to determine who has right of way. \neasy!???+\nAdversarial Filtering+\nAdversarial FilteringFigure 1: Models like BERT struggle to ﬁnish the sen-\ntences in HellaSwag , even when they come from the\nsame distribution as the training set. While the wrong\nendings are on-topic, with words that relate to the con-\ntext, humans consistently judge their meanings to be\neither incorrect or implausible. For example, option A\nof the WikiHow passage suggests that a driver should\nstop at a red light for no more than two seconds .\nplausible next event is option C—that she’ll get\nthe dog wet and it will run away again.\nWhen the SWAG dataset was ﬁrst announced\n(Zellers et al., 2018), this new task of common-\nsense natural language inference seemed trivial\nfor humans (88%) and yet challenging for then-\nstate-of-the-art models ( ă60%), including ELMo""
 'Stylistic\nEnsembleELMo+\nLSTMGPT BERTBase BERTLarge30405060708090100 Accuracy (%)\n48.2%53.7%64.8%71.4%83.0%\n28.0% 28.2% 28.4%32.0%41.1%78.5%77.4%\n71.3%\n63.0%\n41.1%Accuracy of the filtering model before AF\nAccuracy of the filtering model after AF\nBERT-Large accuracy after AFFigure 11: Performance on the WikiHow subset of al-\nternative variations of HellaSwag , where di ﬀerent Ad-\nversarial Filters are used (but without human valida-\ntion). We consider the shallow stylistic adversaries\nused by Zellers et al. (2018) (Stylistic Ensemble),\nas well as an LSTM with ELMo embeddings, GPT,\nBERT-Base, and BERT-Large. For each adversarial ﬁl-\ntering model, we record the accuracy of that model be-\nfore and after AF is used. We also evaluate each al-\nternative dataset using BERT-Large. The results sug-\ngest that using a a stronger model at test time (over the\nmodel used for AF) improves performance, but is not\nenough to solve the task.\n6 Discussion\nOur results suggest that HellaSwag is a challenging\ntestbed for state-of-the-art NLI models, even those\nbuilt on extensive pretraining. The question still\nremains, though, of where will the ﬁeld go next?\n6.1 How easy might HellaSwag be for future\ndiscriminators?\nIn this paper, we showed the existence of a\nGoldilocks zone of text complexity – in which\ngenerations are nonsensical, but existing state-\nof-the-art NLP models cannot tell the di ﬀerence.\nHow hard will the dataset be for future, even more\npowerful, models?\nAnswering this question is challenging because\nthese models don’t exist (or are unavailable) at\nthe time of writing . However, one remedy is to\nperform an ablation study on the Adversarial Fil-\ntering model used, comparing weaker ﬁlters with\nstronger discriminators. We present our results\nin Figure 11, and ﬁnd that while weak discrim-\ninators (like the stylistic ensemble used to make\nSWAG) only marginally reduce the accuracy of\nBERT-Large, increasing the gap between the ﬁlter\nand the ﬁnal discriminator is not enough to solve\nthe task. For instance, using a discriminator with\n3x the parameters as the adversarial ﬁlter (BERT-\nLarge vs. BERT-Base) results in 63% machine ac-\ncuracy.\n30 40 50 60 70 80 90 100\nOverall Accuracy on HellaSwag1021041061081010Pretraining Hours (Estimate)\nHuman performance?\nELMoGPTBERT-BaseBERT-LargeFigure 12: Estimated pretraining hours required to\nreach a desired accuracy on HellaSwag . We estimate\nperfomance with respect to a RTX 2080 Ti - a modern,\nfast GPU, and ﬁt a log-linear regression line. An ex-\ntrapolation suggests that to reach human-level perfor-\nmance on HellaSwag , without algorithmic or computa-\ntional improvements, would require 109GPU-hours of\npretraining (over 100k GPU years).\n6.2 How well does pretraining scale?\nOverall, the current paradigm of pretraining large\nmodels on lots of data has made immense progress\non NLP benchmarks. Though we expect this\ntrend to continue, it also behooves us to con-\nsider its limits. If more compute is indeed the\nanswer for human-level commonsense inference,\nwhat would the compute requirements of this hy-\npothetical massive model look like?\nWe investigate this in Figure 12 by compar-\ning the accuracies of known models on Hella-\nSwag with their computational needs. This estima-\ntion is a rough estimate: we convert reported TPU\nruntimes to our benchmark RTX 2080 Ti GPU us-\ning the Rooﬂine model (Williams et al., 2009),\nwhich focuses primarily on the bottleneck of load-\ning tensors into GPU memory. Extrapolating from\nan exponential ﬁt suggests that reaching human-\nlevel performance on our dataset would require\n109GPU hours, or 100k years – unless algorith-\nmic improvements are made.\nWhat might these algorithmic improvements\nlook like? These could include architectural ad-\nvances, better pretraining objectives, and beyond.\nHowever, these improvements share the bottle-\nneck of the data source. To answer some Hella-\nSwag questions correctly without reasoning deeply\n– like knowing that it is a bad idea to stop at a']","HellaSwag challenges the notion that state-of-the-art models have solved commonsense NLI by introducing adversarially crafted dataset examples that are easy for humans but difficult for machines, highlighting the gap in commonsense reasoning.",0.999999999975,0.8333333333333334,0.9098375690705552,1.0,0.5063906944158201,0.9346536867541897
"How does the dynamic masking strategy in RoBERTa differ from BERT's static masking, and what advantage does it offer?","Dynamic masking in RoBERTa generates a new masking pattern for each training instance every time the sequence is fed to the model, whereas BERT's static masking uses the same mask for every training instance throughout the training process. The advantage of dynamic masking is that it provides a more diverse set of masked inputs, which can enhance the model's ability to learn contextual representations and improve performance on downstream tasks. This approach helps avoid overfitting to a fixed mask and allows the model to see each sequence in multiple ways during training.","['V2.0 some questions are not answered in the pro-\nvided context, making the task more challenging.\nFor SQuAD V1.1 we adopt the same span pre-\ndiction method as BERT ( Devlin et al. ,2019 ). For\nSQuAD V2.0, we add an additional binary classi-\nﬁer to predict whether the question is answerable,\nwhich we train jointly by summing the classiﬁca-\ntion and span loss terms. During evaluation, we\nonly predict span indices on pairs that are classi-\nﬁed as answerable.\nRACE The ReAding Comprehension from Ex-\naminations (RACE) ( Lai et al. ,2017 ) task is a\nlarge-scale reading comprehension dataset with\nmore than 28,000 passages and nearly 100,000\nquestions. The dataset is collected from English\nexaminations in China, which are designed for\nmiddle and high school students. In RACE, each\npassage is associated with multiple questions. For\nevery question, the task is to select one correct an-\nswer from four options. RACE has signiﬁcantly\nlonger context than other popular reading compre-\nhension datasets and the proportion of questions\nthat requires reasoning is very large.\n4 Training Procedure Analysis\nThis section explores and quantiﬁes which choices\nare important for successfully pretraining BERT\nmodels. We keep the model architecture ﬁxed.7\nSpeciﬁcally, we begin by training BERT models\nwith the same conﬁguration as BERT BASE (L=\n12,H= 768 ,A= 12 , 110M params).\n4.1 Static vs. Dynamic Masking\nAs discussed in Section 2, BERT relies on ran-\ndomly masking and predicting tokens. The orig-\ninal BERT implementation performed masking\nonce during data preprocessing, resulting in a sin-\nglestatic mask. To avoid using the same mask for\neach training instance in every epoch, training data\nwas duplicated 10 times so that each sequence is\nmasked in 10 different ways over the 40 epochs of\ntraining. Thus, each training sequence was seen\nwith the same mask four times during training.\nWe compare this strategy with dynamic mask-\ningwhere we generate the masking pattern every\ntime we feed a sequence to the model. This be-\ncomes crucial when pretraining for more steps or\nwith larger datasets.\n7Studying architectural changes, including larger archi-\ntectures, is an important area for future work.Masking SQuAD 2.0 MNLI-m SST-2\nreference 76.3 84.3 92.8\nOur reimplementation:\nstatic 78.3 84.3 92.5\ndynamic 78.7 84.0 92.9\nTable 1: Comparison between static and dynamic\nmasking for BERT BASE. We report F1 for SQuAD and\naccuracy for MNLI-m and SST-2. Reported results are\nmedians over 5 random initializations (seeds). Refer-\nence results are from Yang et al. (2019 ).\nResults Table 1compares the published\nBERT BASE results from Devlin et al. (2019 ) to our\nreimplementation with either static or dynamic\nmasking. We ﬁnd that our reimplementation\nwith static masking performs similar to the\noriginal BERT model, and dynamic masking is\ncomparable or slightly better than static masking.\nGiven these results and the additional efﬁciency\nbeneﬁts of dynamic masking, we use dynamic\nmasking in the remainder of the experiments.\n4.2 Model Input Format and Next Sentence\nPrediction\nIn the original BERT pretraining procedure, the\nmodel observes two concatenated document seg-\nments, which are either sampled contiguously\nfrom the same document (with p= 0.5) or from\ndistinct documents. In addition to the masked lan-\nguage modeling objective, the model is trained to\npredict whether the observed document segments\ncome from the same or distinct documents via an\nauxiliary Next Sentence Prediction (NSP) loss.\nThe NSP loss was hypothesized to be an impor-\ntant factor in training the original BERT model.\nDevlin et al. (2019 ) observe that removing NSP\nhurts performance, with signiﬁcant performance\ndegradation on QNLI, MNLI, and SQuAD 1.1.\nHowever, some recent work has questioned the\nnecessity of the NSP loss ( Lample and Conneau ,\n2019 ;Yang et al. ,2019 ;Joshi et al. ,2019 ).\nTo better understand this discrepancy, we com-\npare several alternative training formats:'
 'jority class.\nC Additional Ablation Studies\nC.1 Effect of Number of Training Steps\nFigure 5 presents MNLI Dev accuracy after ﬁne-\ntuning from a checkpoint that has been pre-trained\nforksteps. This allows us to answer the following\nquestions:\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh ﬁne-tuning accuracy?\nAnswer: Yes, BERT BASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\nC.2 Ablation for Different Masking\nProcedures\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n200 400 600 800 1,0007678808284\nPre-training Steps (Thousands)MNLI Dev Accuracy\nBERT BASE (Masked LM)\nBERT BASE (Left-to-Right)\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after ﬁne-tuning, starting\nfrom model parameters that have been pre-trained for\nksteps. The x-axis is the value of k.Note that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand ﬁne-tuning, as the [MASK] symbol never ap-\npears during the ﬁne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both ﬁne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npliﬁed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\nMasking Rates Dev Set Results\nMASK SAME RND MNLI NER\nFine-tune Fine-tune Feature-based\n80% 10% 10% 84.2 95.4 94.9\n100% 0% 0% 84.3 94.9 94.0\n80% 0% 20% 84.1 95.2 94.6\n80% 20% 0% 84.4 95.2 94.7\n0% 20% 80% 83.7 94.8 94.6\n0% 0% 100% 83.6 94.9 94.6\nTable 8: Ablation over different masking strategies.\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe[MASK] symbol for MLM; S AME means that\nwe keep the target token as is; R NDmeans that\nwe replace the target token with another random\ntoken.\nThe numbers in the left part of the table repre-\nsent the probabilities of the speciﬁc strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\nFrom the table it can be seen that ﬁne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the M ASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe R NDstrategy performs much worse than our\nstrategy as well.'
 'for natural language understanding. In Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: An-\nalyzing and Interpreting Neural Networks for NLP ,\npages 353–355.\nWei Wang, Ming Yan, and Chen Wu. 2018b. Multi-\ngranularity hierarchical attention fusion networks\nfor reading comprehension and question answering.\nInProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers) . Association for Computational Lin-\nguistics.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2018. Neural network acceptability judg-\nments. arXiv preprint arXiv:1805.12471 .\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2018. A broad-coverage challenge corpus\nfor sentence understanding through inference. In\nNAACL .\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural ma-\nchine translation system: Bridging the gap between\nhuman and machine translation. arXiv preprint\narXiv:1609.08144 .\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod\nLipson. 2014. How transferable are features in deep\nneural networks? In Advances in neural information\nprocessing systems , pages 3320–3328.\nAdams Wei Yu, David Dohan, Minh-Thang Luong, Rui\nZhao, Kai Chen, Mohammad Norouzi, and Quoc V\nLe. 2018. QANet: Combining local convolution\nwith global self-attention for reading comprehen-\nsion. In ICLR .\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin\nChoi. 2018. Swag: A large-scale adversarial dataset\nfor grounded commonsense inference. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) .\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE\ninternational conference on computer vision , pages\n19–27.\nAppendix for “BERT: Pre-training of\nDeep Bidirectional Transformers for\nLanguage Understanding”\nWe organize the appendix into three sections:\n• Additional implementation details for BERT\nare presented in Appendix A;• Additional details for our experiments are\npresented in Appendix B; and\n• Additional ablation studies are presented in\nAppendix C.\nWe present additional ablation studies for\nBERT including:\n–Effect of Number of Training Steps; and\n–Ablation for Different Masking Proce-\ndures.\nA Additional Details for BERT\nA.1 Illustration of the Pre-training Tasks\nWe provide examples of the pre-training tasks in\nthe following.\nMasked LM and the Masking Procedure As-\nsuming the unlabeled sentence is my dog is\nhairy , and during the random masking procedure\nwe chose the 4-th token (which corresponding to\nhairy ), our masking procedure can be further il-\nlustrated by\n• 80% of the time: Replace the word with the\n[MASK] token, e.g., my dog is hairy →\nmy dog is [MASK]\n• 10% of the time: Replace the word with a\nrandom word, e.g., my dog is hairy →my\ndog is apple\n• 10% of the time: Keep the word un-\nchanged, e.g., my dog is hairy →my dog\nis hairy . The purpose of this is to bias the\nrepresentation towards the actual observed\nword.\nThe advantage of this procedure is that the\nTransformer encoder does not know which words\nit will be asked to predict or which have been re-\nplaced by random words, so it is forced to keep\na distributional contextual representation of ev-\neryinput token. Additionally, because random\nreplacement only occurs for 1.5% of all tokens\n(i.e., 10% of 15%), this does not seem to harm\nthe model’s language understanding capability. In\nSection C.2, we evaluate the impact this proce-\ndure.\nCompared to standard langauge model training,\nthe masked LM only make predictions on 15% of\ntokens in each batch, which suggests that more\npre-training steps may be required for the model'
 'alternatives that lead to better downstream task\nperformance; (2) We use a novel dataset, CC-\nNEWS, and conﬁrm that using more data for pre-\ntraining further improves performance on down-\nstream tasks; (3) Our training improvements show\nthat masked language model pretraining, under\nthe right design choices, is competitive with all\nother recently published methods. We release our\nmodel, pretraining and ﬁne-tuning code imple-\nmented in PyTorch ( Paszke et al. ,2017 ).\n2 Background\nIn this section, we give a brief overview of the\nBERT ( Devlin et al. ,2019 ) pretraining approach\nand some of the training choices that we will ex-\namine experimentally in the following section.\n2.1 Setup\nBERT takes as input a concatenation of two\nsegments (sequences of tokens), x1,...,x N\nandy1,...,yM. Segments usually consist of\nmore than one natural sentence. The two seg-\nments are presented as a single input sequence\nto BERT with special tokens delimiting them:\n[CLS],x1,...,x N,[SEP],y1,...,yM,[EOS].\nMandNare constrained such that M+N < T ,\nwhereTis a parameter that controls the maximum\nsequence length during training.\nThe model is ﬁrst pretrained on a large unla-\nbeled text corpus and subsequently ﬁnetuned us-\ning end-task labeled data.\n2.2 Architecture\nBERT uses the now ubiquitous transformer archi-\ntecture ( Vaswani et al. ,2017 ), which we will not\nreview in detail. We use a transformer architecture\nwithLlayers. Each block uses Aself-attention\nheads and hidden dimension H.\n2.3 Training Objectives\nDuring pretraining, BERT uses two objectives:\nmasked language modeling and next sentence pre-\ndiction.\nMasked Language Model (MLM) A random\nsample of the tokens in the input sequence is\nselected and replaced with the special token\n[MASK]. The MLM objective is a cross-entropy\nloss on predicting the masked tokens. BERT uni-\nformly selects 15% of the input tokens for possi-\nble replacement. Of the selected tokens, 80% are\nreplaced with [MASK], 10% are left unchanged,and 10% are replaced by a randomly selected vo-\ncabulary token.\nIn the original implementation, random mask-\ning and replacement is performed once in the be-\nginning and saved for the duration of training, al-\nthough in practice, data is duplicated so the mask\nis not always the same for every training sentence\n(see Section 4.1).\nNext Sentence Prediction (NSP) NSP is a bi-\nnary classiﬁcation loss for predicting whether two\nsegments follow each other in the original text.\nPositive examples are created by taking consecu-\ntive sentences from the text corpus. Negative ex-\namples are created by pairing segments from dif-\nferent documents. Positive and negative examples\nare sampled with equal probability.\nThe NSP objective was designed to improve\nperformance on downstream tasks, such as Natural\nLanguage Inference ( Bowman et al. ,2015 ), which\nrequire reasoning about the relationships between\npairs of sentences.\n2.4 Optimization\nBERT is optimized with Adam ( Kingma and Ba ,\n2015 ) using the following parameters: β1= 0.9,\nβ2= 0.999,ǫ=1e-6 and L2weight de-\ncay of0.01. The learning rate is warmed up\nover the ﬁrst 10,000 steps to a peak value of\n1e-4, and then linearly decayed. BERT trains\nwith a dropout of 0.1 on all layers and at-\ntention weights, and a GELU activation func-\ntion ( Hendrycks and Gimpel ,2016 ). Models are\npretrained for S=1,000,000 updates, with mini-\nbatches containing B=256 sequences of maxi-\nmum length T=512 tokens.\n2.5 Data\nBERT is trained on a combination of B OOK COR-\nPUS (Zhu et al. ,2015 ) plus English W IKIPEDIA ,\nwhich totals 16GB of uncompressed text.3\n3 Experimental Setup\nIn this section, we describe the experimental setup\nfor our replication study of BERT.\n3.1 Implementation\nWe reimplement BERT in FAIRSEQ (Ott et al. ,\n2019 ). We primarily follow the original BERT\n3Yang et al. (2019 ) use the same dataset but report having\nonly 13GB of text after data cleaning. This is most likely due\nto subtle differences in cleaning of the Wikipedia data.']","Unlike BERT's static masking, which applies the same mask to each training instance throughout epochs, RoBERTa employs dynamic masking, where the mask is generated anew for each instance every time it is fed into the model. This strategy ensures a richer variety of training examples and prevents the model from overfitting to a specific masked pattern.",0.999999999975,0.8571428571428571,0.9480792291027346,0.5,0.8651299811304602,0.960519924521841
"How does the performance of RoBERTa compare to BERT and other models on the GLUE benchmark, particularly in terms of state-of-the-art achievements?","RoBERTa outperforms the originally reported results of BERT LARGE on the GLUE benchmark, achieving significant improvements across all tasks when trained on larger and more diverse datasets. Specifically, RoBERTa's best models demonstrate superior performance compared to BERT LARGE and other models such as XLNet LARGE, often surpassing their scores on various downstream tasks. Overall, RoBERTa's enhancements in training data size and duration contribute to its state-of-the-art achievements on GLUE.","['BiLSTM+ELMo+Attn\nOpenAI GPT\nBERT + Single-task Adapters\nBERT (Large)\nBERT on STILTs\nBERT + BAM\nSemBERT\nSnorkel MeTaL\nALICE (Large)\nMT-DNN (ensemble)\nXLNet-Large (ensemble)0.50.60.70.80.91.01.11.2\nGLUE Score\nHuman Performance\nCoLA\nSST-2MRPC\nSTS-B\nQQP\nMNLIQNLI\nRTE\nWNLIFigure 1: GLUE benchmark performance for submitted systems, rescaled to set human performance\nto 1.0, shown as a single number score, and broken down into the nine constituent task performances.\nFor tasks with multiple metrics, we use an average of the metrics. More information on the tasks\nincluded in GLUE can be found in Wang et al. (2019a) and in Warstadt et al. (2019, CoLA), Socher\net al. (2013, SST-2), Dolan and Brockett (2005, MRPC), Cer et al. (2017, STS-B), and Williams et al.\n(2018, MNLI), and Rajpurkar et al. (2016, the original data source for QNLI).\nremains substantial scope for improvement towards GLUE’s high-level goals, the original version of\nthe benchmark is no longer a suitable metric for quantifying such progress.\nIn response, we introduce SuperGLUE, a new benchmark designed to pose a more rigorous test of\nlanguage understanding. SuperGLUE has the same high-level motivation as GLUE: to provide a\nsimple, hard-to-game measure of progress toward general-purpose language understanding technolo-\ngies for English. We anticipate that signiﬁcant progress on SuperGLUE should require substantive\ninnovations in a number of core areas of machine learning, including sample-efﬁcient, transfer,\nmultitask, and unsupervised or self-supervised learning.\nSuperGLUE follows the basic design of GLUE: It consists of a public leaderboard built around\neight language understanding tasks, drawing on existing data, accompanied by a single-number\nperformance metric, and an analysis toolkit. However, it improves upon GLUE in several ways:\nMore challenging tasks: SuperGLUE retains the two hardest tasks in GLUE. The remaining tasks\nwere identiﬁed from those submitted to an open call for task proposals and were selected based on\ndifﬁculty for current NLP approaches.\nMore diverse task formats: The task formats in GLUE are limited to sentence- and sentence-pair\nclassiﬁcation. We expand the set of task formats in SuperGLUE to include coreference resolution\nand question answering (QA).\nComprehensive human baselines: We include human performance estimates for all benchmark\ntasks, which verify that substantial headroom exists between a strong BERT-based baseline and\nhuman performance.\nImproved code support: SuperGLUE is distributed with a new, modular toolkit for work on\npretraining, multi-task learning, and transfer learning in NLP, built around standard tools including\nPyTorch (Paszke et al., 2017) and AllenNLP (Gardner et al., 2017).\nReﬁned usage rules: The conditions for inclusion on the SuperGLUE leaderboard have been\nrevamped to ensure fair competition, an informative leaderboard, and full credit assignment to data\nand task creators.\nThe SuperGLUE leaderboard, data, and software tools are available at super.gluebenchmark.com .\n2 Related Work\nMuch work prior to GLUE demonstrated that training neural models with large amounts of available\nsupervision can produce representations that effectively transfer to a broad range of NLP tasks\n2'
 'Table 4: Baseline performance on the SuperGLUE development.\nModel Avg BoolQ CB COPA MultiRC ReCoRD RTE WiC WSC\nMetrics Acc. Acc./F1 Acc. F1 a/EM F1/EM Acc. Acc. Acc.\nMost Frequent Class 47.7 62.2 50.0/22.2 55.0 59.9/ 0.8 32.4/31.5 52.7 50.0 63.5\nCBOW 47.7 62.4 71.4/49.6 63.0 20.3/ 0.3 14.4/13.8 54.2 55.3 61.5\nBERT 72.2 77.7 94.6/93.7 69.0 70.5/24.7 70.6/69.8 75.8 74.9 68.3\nBERT++ 74.6 80.1 96.4/95.0 78.0 70.5/24.7 70.6/69.8 82.3 74.9 68.3\nA Development Set Results\nIn Table 4, we present results of the baselines on the SuperGLUE tasks development sets.\nB Performance on GLUE Diagnostics\nFigure 2 shows the performance on the GLUE diagnostics dataset for systems submitted to the public\nleaderboard.\nDisjunction Downward Monotone Restrictivity Double Negation Prepositional Phrases60\n40\n20\n020406080\nChance\nBiLSTM+ELMo+Attn\nOpenAI GPT\nBERT + Single-task Adapters\nBERT (Large)\nBERT on STILTsBERT + BAM\nSemBERT\nSnorkel MeTaL\nALICE (Large)\nMT-DNN (ensemble)\nXLNet-Large (ensemble)\nFigure 2: Performance of GLUE submissions on selected diagnostic categories, reported using the\nR3metric scaled up by 100, as in Wang et al. (2019a, see paper for a description of the categories).\nSome initially difﬁcult categories, like double negation, saw gains from advances on GLUE, but\nothers remain hard (restrictivity) or even adversarial (disjunction, downward monotone).\nC Human Performance Estimation\nFor collecting data to establish human performance on the SuperGLUE tasks, we follow a two\nstep procedure where we ﬁrst provide some training to the crowd workers before they proceed to\nannotation. For both steps and all tasks, the average pay rate is $23.75/hr.7\nIn the training phase, workers are provided with instructions on the task, linked to an FAQ page, and\nare asked to annotate up to 30 examples from the development set. After answering each example,\nworkers are also asked to check their work against the provided ground truth label. After the training\nphase is complete, we provide the qualiﬁcation to work on the annotation phase to all workers\nwho annotated a minimum of ﬁve examples, i.e. completed ﬁve HITs during training and achieved\nperformance at, or above the median performance across all workers during training.\n7This estimate is taken from https://turkerview.com .\n16'
 'System MNLI-(m/mm) QQP QNLI SST-2 CoLA STS-B MRPC RTE Average\n392k 363k 108k 67k 8.5k 5.7k 3.5k 2.5k -\nPre-OpenAI SOTA 80.6/80.1 66.1 82.3 93.2 35.0 81.0 86.0 61.7 74.0\nBiLSTM+ELMo+Attn 76.4/76.1 64.8 79.8 90.4 36.0 73.3 84.9 56.8 71.0\nOpenAI GPT 82.1/81.4 70.3 87.4 91.3 45.4 80.0 82.3 56.0 75.1\nBERT BASE 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6\nBERT LARGE 86.7/85.9 72.1 92.7 94.9 60.5 86.5 89.3 70.1 82.1\nTable 1: GLUE Test results, scored by the evaluation server ( https://gluebenchmark.com/leaderboard ).\nThe number below each task denotes the number of training examples. The “Average” column is slightly different\nthan the ofﬁcial GLUE score, since we exclude the problematic WNLI set.8BERT and OpenAI GPT are single-\nmodel, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and\naccuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.\nWe use a batch size of 32 and ﬁne-tune for 3\nepochs over the data for all GLUE tasks. For each\ntask, we selected the best ﬁne-tuning learning rate\n(among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set.\nAdditionally, for BERT LARGE we found that ﬁne-\ntuning was sometimes unstable on small datasets,\nso we ran several random restarts and selected the\nbest model on the Dev set. With random restarts,\nwe use the same pre-trained checkpoint but per-\nform different ﬁne-tuning data shufﬂing and clas-\nsiﬁer layer initialization.9\nResults are presented in Table 1. Both\nBERT BASE and BERT LARGE outperform all sys-\ntems on all tasks by a substantial margin, obtaining\n4.5% and 7.0% respective average accuracy im-\nprovement over the prior state of the art. Note that\nBERT BASE and OpenAI GPT are nearly identical\nin terms of model architecture apart from the at-\ntention masking. For the largest and most widely\nreported GLUE task, MNLI, BERT obtains a 4.6%\nabsolute accuracy improvement. On the ofﬁcial\nGLUE leaderboard10, BERT LARGE obtains a score\nof 80.5, compared to OpenAI GPT, which obtains\n72.8 as of the date of writing.\nWe ﬁnd that BERT LARGE signiﬁcantly outper-\nforms BERT BASE across all tasks, especially those\nwith very little training data. The effect of model\nsize is explored more thoroughly in Section 5.2.\n4.2 SQuAD v1.1\nThe Stanford Question Answering Dataset\n(SQuAD v1.1) is a collection of 100k crowd-\nsourced question/answer pairs (Rajpurkar et al.,\n2016). Given a question and a passage from\n9The GLUE data set distribution does not include the Test\nlabels, and we only made a single GLUE evaluation server\nsubmission for each of BERT BASE and BERT LARGE .\n10https://gluebenchmark.com/leaderboardWikipedia containing the answer, the task is to\npredict the answer text span in the passage.\nAs shown in Figure 1, in the question answer-\ning task, we represent the input question and pas-\nsage as a single packed sequence, with the ques-\ntion using the Aembedding and the passage using\ntheBembedding. We only introduce a start vec-\ntorS∈RHand an end vector E∈RHduring\nﬁne-tuning. The probability of word ibeing the\nstart of the answer span is computed as a dot prod-\nuct between TiandSfollowed by a softmax over\nall of the words in the paragraph: Pi=eS·Ti∑\njeS·Tj.\nThe analogous formula is used for the end of the\nanswer span. The score of a candidate span from\npositionito positionjis deﬁned as S·Ti+E·Tj,\nand the maximum scoring span where j≥iis\nused as a prediction. The training objective is the\nsum of the log-likelihoods of the correct start and\nend positions. We ﬁne-tune for 3 epochs with a\nlearning rate of 5e-5 and a batch size of 32.\nTable 2 shows top leaderboard entries as well\nas results from top published systems (Seo et al.,\n2017; Clark and Gardner, 2018; Peters et al.,\n2018a; Hu et al., 2018). The top results from the\nSQuAD leaderboard do not have up-to-date public\nsystem descriptions available,11and are allowed to\nuse any public data when training their systems.'
 'Model data bsz stepsSQuADMNLI-m SST-2(v1.1/2.0)\nRoBERTa\nwith B OOKS + W IKI 16GB 8K 100K 93.6/87.3 89.0 95.3\n+ additional data ( §3.2) 160GB 8K 100K 94.0/87.7 89.3 95.6\n+ pretrain longer 160GB 8K 300K 94.4/88.7 90.0 96.1\n+ pretrain even longer 160GB 8K 500K 94.6/89.4 90.2 96.4\nBERT LARGE\nwith B OOKS + W IKI 13GB 256 1M 90.9/81.8 86.6 93.7\nXLNet LARGE\nwith B OOKS + W IKI 13GB 256 1M 94.0/87.8 88.4 94.4\n+ additional data 126GB 2K 500K 94.5/88.8 89.8 95.6\nTable 4: Development set results for RoBERTa as we pretrain o ver more data (16GB →160GB of text) and pretrain\nfor longer (100K →300K→500K steps). Each row accumulates improvements from the row s above. RoBERTa\nmatches the architecture and training objective of BERT LARGE . Results for BERT LARGE and XLNet LARGE are from\nDevlin et al. (2019 ) and Yang et al. (2019 ), respectively. Complete results on all GLUE tasks can be fo und in the\nAppendix.\nDevlin et al. (2019 ). We pretrain our model using\n1024 V100 GPUs for approximately one day.\nResults We present our results in Table 4. When\ncontrolling for training data, we observe that\nRoBERTa provides a large improvement over the\noriginally reported BERT LARGE results, reafﬁrming\nthe importance of the design choices we explored\nin Section 4.\nNext, we combine this data with the three ad-\nditional datasets described in Section 3.2. We\ntrain RoBERTa over the combined data with the\nsame number of training steps as before (100K).\nIn total, we pretrain over 160GB of text. We ob-\nserve further improvements in performance across\nall downstream tasks, validating the importance of\ndata size and diversity in pretraining.9\nFinally, we pretrain RoBERTa for signiﬁcantly\nlonger, increasing the number of pretraining steps\nfrom 100K to 300K, and then further to 500K. We\nagain observe signiﬁcant gains in downstream task\nperformance, and the 300K and 500K step mod-\nels outperform XLNet LARGE across most tasks. We\nnote that even our longest-trained model does not\nappear to overﬁt our data and would likely beneﬁt\nfrom additional training.\nIn the rest of the paper, we evaluate our best\nRoBERTa model on the three different bench-\nmarks: GLUE, SQuaD and RACE. Speciﬁcally\n9Our experiments conﬂate increases in data size and di-\nversity. We leave a more careful analysis of these two dimen-\nsions to future work.we consider RoBERTa trained for 500K steps over\nall ﬁve of the datasets introduced in Section 3.2.\n5.1 GLUE Results\nFor GLUE we consider two ﬁnetuning settings.\nIn the ﬁrst setting ( single-task, dev ) we ﬁnetune\nRoBERTa separately for each of the GLUE tasks,\nusing only the training data for the correspond-\ning task. We consider a limited hyperparameter\nsweep for each task, with batch sizes ∈ {16,32}\nand learning rates ∈ {1e−5,2e−5,3e−5}, with a\nlinear warmup for the ﬁrst 6% of steps followed by\na linear decay to 0. We ﬁnetune for 10 epochs and\nperform early stopping based on each task’s eval-\nuation metric on the dev set. The rest of the hyper-\nparameters remain the same as during pretraining.\nIn this setting, we report the median development\nset results for each task over ﬁve random initial-\nizations, without model ensembling.\nIn the second setting ( ensembles, test ), we com-\npare RoBERTa to other approaches on the test set\nvia the GLUE leaderboard. While many submis-\nsions to the GLUE leaderboard depend on multi-\ntask ﬁnetuning, our submission depends only on\nsingle-task ﬁnetuning . For RTE, STS and MRPC\nwe found it helpful to ﬁnetune starting from the\nMNLI single-task model, rather than the baseline\npretrained RoBERTa. We explore a slightly wider\nhyperparameter space, described in the Appendix,\nand ensemble between 5 and 7 models per task.']","RoBERTa surpasses BERT and establishes new state-of-the-art performance on the GLUE benchmark, particularly excelling in tasks like MNLI, QNLI, RTE, and STS-B. This achievement underscores the effectiveness of RoBERTa's optimizations and its capacity to better understand and process natural language.",0.8055555555287036,1.0,0.9181398181631647,0.5,0.8165216277783387,0.9327531777800211
How should future language model benchmarks be structured to ensure a holistic assessment of models' capabilities and knowledge breadth?,"Future language model benchmarks should be structured to cover a diverse set of subjects and tasks, similar to the proposed new benchmark that evaluates 57 subjects across various difficulty levels. They should include challenging zero-shot and few-shot settings to better reflect human evaluation methods, emphasizing both world knowledge and problem-solving abilities. Additionally, benchmarks should adapt over time by incorporating adversarial elements to continuously assess and identify the shortcomings of evolving language models.","['parameter language models, data may also become a bottleneck, as there is far less written about\nesoteric branches of knowledge than about everyday situations.\n6 C ONCLUSION\nWe introduced a new test that measures how well text models can learn and apply knowledge\nencountered during pretraining. By covering 57 subjects at varying levels of difﬁculty, the test\nassesses language understanding in greater breadth and depth than previous benchmarks. We found\nthat it has recently become possible for models to make meaningful progress on the test, but that\nstate-of-the-art models have lopsided performance and rarely excel at any individual task. We also\nshowed that current models are uncalibrated and have difﬁculty with tasks that require calculations.\nWorryingly, models also perform especially poorly on socially relevant subjects including morality\nand law. Our expansive test can help researchers pinpoint important shortcomings of models, making\nit easier to gain a clearer picture of state-of-the-art capabilities.\n8'
 'Published as a conference paper at ICLR 2021\nMEASURING MASSIVE MULTITASK\nLANGUAGE UNDERSTANDING\nDan Hendrycks\nUC BerkeleyCollin Burns\nColumbia UniversitySteven Basart\nUChicagoAndy Zou\nUC Berkeley\nMantas Mazeika\nUIUCDawn Song\nUC BerkeleyJacob Steinhardt\nUC Berkeley\nABSTRACT\nWe propose a new test to measure a text model’s multitask accuracy. The test\ncovers 57 tasks including elementary mathematics, US history, computer science,\nlaw, and more. To attain high accuracy on this test, models must possess extensive\nworld knowledge and problem solving ability. We ﬁnd that while most recent\nmodels have near random-chance accuracy, the very largest GPT-3 model improves\nover random chance by almost 20 percentage points on average. However, on every\none of the 57 tasks, the best models still need substantial improvements before\nthey can reach expert-level accuracy. Models also have lopsided performance\nand frequently do not know when they are wrong. Worse, they still have near-\nrandom accuracy on some socially important subjects such as morality and law.\nBy comprehensively evaluating the breadth and depth of a model’s academic and\nprofessional understanding, our test can be used to analyze models across many\ntasks and to identify important shortcomings.\n1 I NTRODUCTION\nNatural Language Processing (NLP) models have achieved superhuman performance on a number of\nrecently proposed benchmarks. However, these models are still well below human level performance\nfor language understanding as a whole, suggesting a disconnect between our benchmarks and the\nactual capabilities of these models. The General Language Understanding Evaluation benchmark\n(GLUE) (Wang et al., 2018) was introduced in 2018 to evaluate performance on a wide range of NLP\ntasks, and top models achieved superhuman performance within a year. To address the shortcomings\nof GLUE, researchers designed the SuperGLUE benchmark with more difﬁcult tasks (Wang et al.,\n2019). About a year since the release of SuperGLUE, performance is again essentially human-level\n(Raffel et al., 2019). While these benchmarks evaluate linguistic skills more than overall language\nunderstanding, an array of commonsense benchmarks have been proposed to measure basic reasoning\nand everyday knowledge (Zellers et al., 2019; Huang et al., 2019; Bisk et al., 2019). However, these\nrecent benchmarks have similarly seen rapid progress (Khashabi et al., 2020). Overall, the near\nhuman-level performance on these benchmarks suggests that they are not capturing important facets\nof language understanding.\nTransformer models have driven this recent progress by pretraining on massive text corpora, including\nall of Wikipedia, thousands of books, and numerous websites. These models consequently see\nextensive information about specialized topics, most of which is not assessed by existing NLP\nbenchmarks. It consequently remains an open question just how capable current language models are\nat learning and applying knowledge from many domains.\nTo bridge the gap between the wide-ranging knowledge that models see during pretraining and the\nexisting measures of success, we introduce a new benchmark for assessing models across a diverse\nset of subjects that humans learn. We design the benchmark to measure knowledge acquired during\npretraining by evaluating models exclusively in zero-shot and few-shot settings. This makes the\nbenchmark more challenging and more similar to how we evaluate humans. The benchmark covers\n57subjects across STEM, the humanities, the social sciences, and more. It ranges in difﬁculty from\nan elementary level to an advanced professional level, and it tests both world knowledge and problem\nsolving ability. Subjects range from traditional areas, such as mathematics and history, to more\n1arXiv:2009.03300v3  [cs.CY]  12 Jan 2021'
 'lems of reporting bias (Gordon and Van Durme,\n2013). Alternatively, future models might answer\ncorrectly only by picking up on spurious patterns,\nin which case a new development of the bench-\nmark – using these models as adversaries – would\nplace us in the same position as we are right now.\nPut another way, for humans to answer Hella-\nSwag questions requires abstracting away from\nlanguage and modeling world states instead. We\npostulate that this is what separates solving the\ntask of commonsense NLI, as opposed to a par-\nticular dataset. Indeed, we ﬁnd that existing deep\nmethods often get fooled by lexical false friends.\nFor example, in the WikiHow example from Fig-\nure 10, BERT chooses an ending that matches\nthetechnology words in the context, rather than\nmatching the deeper topic: using technology as an\nexcuse for not doing homework.\n6.3 Towards a future of evolving benchmarks\nWhat happens when HellaSwag gets solved? We\nbelieve the answer is simple: crowdsource another\ndataset, with the same exact format, and see where\nmodels fail. Indeed, in our work we found this to\nbe straightforward from an algorithmic perspec-\ntive: by throwing in the best known generator\n(GPT) and the best known discriminator (BERT-\nLarge), we made a dataset that is adversarial - not\njust to BERT, but to all models we have access to.\nWhile this was easy algorithmically, care must\nbe taken from a data curation standpoint. Indeed,\nwe ﬁnd success exists within a Goldilocks zone:\nthe data source must be complex enough that state-\nof-the-art generators often make mistakes, while\nsimple enough such that discriminators often fail\nto catch them. This ties the future of SWAG-\nstyle benchmarks to progress on language gener-\nation: until generation is solved, commonsense\nNLI will remain unsolved. Even recent promis-\ning results on scaling up language models (Rad-\nford et al., 2019) ﬁnd problems in terms of consis-\ntency, with the best curated examples requiring 25\nrandom seeds.\n7 Conclusion\nIn this paper, we presented HellaSwag , a new\ndataset for physically situated commonsense rea-\nsoning. By constructing the dataset through ad-\nversarial ﬁltering, combined with state-of-the-art\nmodels for language generation and discrimina-\ntion, we produced a dataset that is adversarial tothe most robust models available – even when\nmodels are evaluated on items from the train-\ning distribution. In turn, we provided insight\ninto the inner workings of pretrained models, and\nsuggest a path for NLP progress going forward:\ntowards benchmarks that adversarially co-evolve\nwith evolving state-of-the-art models.\nAcknowledgments\nWe thank the reviewers, as well as Jesse Thoma-\nson, for their helpful feedback. We thank the\nMechanical Turk workers for their great work\nduring dataset collection. Thanks also to Zak\nStone and the Google Cloud TPU team for help\nwith the computing infrastructure. This work\nwas supported by the National Science Foundation\nthrough a Graduate Research Fellowship (DGE-\n1256082) and NSF grants (IIS-1524371, 1637479,\n165205, 1703166), the DARPA CwC program\nthrough ARO (W911NF-15-1-0543), the IARPA\nDIV A program through D17PC00343, the Sloan\nResearch Foundation through a Sloan Fellowship,\nthe Allen Institute for Artiﬁcial Intelligence, the\nNVIDIA Artiﬁcial Intelligence Lab, and gifts by\nGoogle and Facebook. The views and conclu-\nsions contained herein are those of the authors and\nshould not be interpreted as representing endorse-\nments of IARPA, DOI /IBC, or the U.S. Govern-\nment.\nReferences\nYonatan Belinkov and Yonatan Bisk. 2018. Synthetic\nand natural noise both break neural machine transla-\ntion. In ICLR . ICLR.\nQian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui\nJiang, and Diana Inkpen. 2017. Enhanced lstm for\nnatural language inference. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers) , vol-\nume 1, pages 1657–1668.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and'
 'Large Language Models Struggle to Learn Long-Tail Knowledge\nKassner, N., Krojer, B., and Sch ¨utze, H. Are pretrained\nlanguage models symbolic reasoners over knowledge? In\nCoNLL , 2020.\nKwiatkowski, T., Palomaki, J., Rhinehart, O., Collins, M.,\nParikh, A., Alberti, C., Epstein, D., Polosukhin, I., Kelcey,\nM., Devlin, J., et al. Natural Questions: A benchmark for\nquestion answering research. In TACL , 2019.\nLauren c ¸on, H., Saulnier, L., Wang, T., Akiki, C., del Moral,\nA. V ., Scao, T. L., Werra, L. V ., Mou, C., Ponferrada,\nE. G., Nguyen, H., Frohberg, J., ˇSaˇsko, M., Lhoest, Q.,\nMcMillan-Major, A., et al. The BigScience ROOTS\ncorpus: A 1.6TB composite multilingual dataset. In\nNeurIPS , 2022.\nLee, K., Chang, M.-W., and Toutanova, K. Latent retrieval\nfor weakly supervised open domain question answering.\nInACL, 2019.\nLee, K., Ippolito, D., Nystrom, A., Zhang, C., Eck, D.,\nCallison-Burch, C., and Carlini, N. Deduplicating train-\ning data makes language models better. In ACL, 2021.\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V .,\nGoyal, N., K ¨uttler, H., Lewis, M., Yih, W.-t., Rockt ¨aschel,\nT., et al. Retrieval-augmented generation for knowledge-\nintensive NLP tasks. In NeurIPS , 2020.\nLiu, L., Lewis, P., Riedel, S., and Stenetorp, P. Challenges\nin generalization in open domain question answering. In\nFindings of NAACL , 2022.\nMallen, A., Asai, A., Zhong, V ., Das, R., Hajishirzi, H., and\nKhashabi, D. When not to trust language models: Investi-\ngating effectiveness and limitations of parametric and non-\nparametric memories. arXiv preprint arXiv:2212.10511 ,\n2022.\nMendes, P. N., Jakob, M., Garc ´ıa-Silva, A., and Bizer, C.\nDBpedia Spotlight: Shedding light on the web of docu-\nments. In International Conference on Semantic Systems ,\n2011.\nPetroni, F., Rockt ¨aschel, T., Lewis, P., Bakhtin, A., Wu,\nY ., Miller, A. H., and Riedel, S. Language models as\nknowledge bases? In EMNLP , 2019.\nPetroni, F., Lewis, P. S. H., Piktus, A., Rockt ¨aschel, T., Wu,\nY ., Miller, A. H., and Riedel, S. How context affects\nlanguage models’ factual predictions. In AKBC , 2020.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W., Liu, P. J., et al. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer. In JMLR , 2020.Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. SQuAD:\n100,000+ questions for machine comprehension of text.\nInEMNLP , 2016.\nRao, R. M., Liu, J., Verkuil, R., Meier, J., Canny, J., Abbeel,\nP., Sercu, T., and Rives, A. Msa transformer. In ICML ,\n2021.\nRazeghi, Y ., Logan IV , R. L., Gardner, M., and Singh, S.\nImpact of pretraining term frequencies on few-shot rea-\nsoning. In Findings of the Association for Computational\nLinguistics: EMNLP 2022 , 2022.\nRoberts, A., Raffel, C., and Shazeer, N. How much knowl-\nedge can you pack into the parameters of a language\nmodel? In EMNLP , 2020.\nRobertson, S. and Zaragoza, H. The probabilistic relevance\nframework: BM25 and beyond. Foundations and Trends\nin IR , 2009.\nScao, T. L., Fan, A., Akiki, C., Pavlick, E.-J., Ili’c, S.,\nHesslow, D., Castagn’e, R., Luccioni, A. S., Yvon, F.,\nGall´e, M., Tow, J., Rush, A. M., et al. BLOOM: A\n176b-parameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100 , 2022.\nShin, S., Lee, S.-W., Ahn, H., Kim, S., Kim, H., Kim, B.,\nCho, K., Lee, G., Park, W., Ha, J.-W., et al. On the\neffect of pretraining corpora on in-context learning by a\nlarge-scale language model. In NAACL , 2022.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. In NeurIPS , 2017.\nWang, B. and Komatsuzaki, A. GPT-J-6B: A 6 Billion\nParameter Autoregressive Language Model. https://\ngithub.com/kingoflolz/mesh-transformer-jax , 2021.\nWang, T., Roberts, A., Hesslow, D., Scao, T. L., Chung,\nH. W., Beltagy, I., Launay, J., and Raffel, C. What lan-\nguage model architecture and pretraining objective work']","Future benchmarks should integrate a broader spectrum of subjects and cognitive skills, emphasizing the inclusion of tasks that test models' ethical reasoning, understanding of human values, and ability to perform complex problem-solving, beyond the mere scale of data and parameters.",0.999999999975,1.0,0.9377411643221389,1.0,0.6921112708820799,0.8934450835283195
How does DetectGPT's approach to machine-generated text detection differ from previous zero-shot methods?,"DetectGPT's approach to machine-generated text detection is based on the hypothesis that samples from a source model typically lie in areas of negative curvature of the log probability function, while human text does not. This method utilizes random perturbations to evaluate the log probability of slightly modified versions of the text, which allows for a distinction between machine-generated and human-written content. In contrast, previous zero-shot methods may not leverage this curvature-based discrepancy and might rely on more general techniques without the specific focus on perturbations.","['have evaluated the detection performance of supervised\nmethods (typically fine-tuned transformers) for detecting\nmachine-generated text. In this section, we explore several\ndomains to better understand the relative strengths of super-\nvised and zero-shot detectors. The results are presented in\nFigure 4, using 200 samples from each dataset for evalua-\ntion. We find that supervised detectors can provide similar\ndetection performance to DetectGPT on in-distribution data\nlike English news, but perform significantly worse than zero-\nshot methods in the case of English scientific writing and\nfail altogether for German writing. This finding echoes past\nwork showing that language models trained for machine-\ngenerated text detection overfit to their training data (source\nmodel, decoding strategy, topic, language, etc.; Uchendu\net al. (2020); Ippolito et al. (2020); Jawahar et al. (2020)).\nIn contrast, zero-shot methods generalize relatively easily\nto new languages and domains; DetectGPT’s performance\nin particular is mostly unaffected by the change in language\nfrom English to German.\nWhile our experiments have shown that DetectGPT is ef-\nfective on a variety of domains and models, it is natural to\nwonder if it is effective for the largest publicly-available\nLMs. Therefore, we also evaluate multiple zero-shot and su-\npervised methods on two 175B parameter models, OpenAI’s\nGPT-3 and AI21 Labs’ Jurassic-2 Jumbo. Because neither\nAPI provides access to the complete conditional distribution\n4The overall ease of detecting machine-generated fake writing\ncorroborates anecdotal reporting that machine-generated creative\nwriting tends to be noticeably generic, and therefore relatively easy\nto detect (Roose & Newton, 2022).\n6'
 'Zero-Shot Machine-Generated Text Detection using Probability Curvature\ngenerated text detection. Other work explores watermarks\nfor generated text (Kirchenbauer et al., 2023), which modify\na model’s generations to make them easier to detect. Our\nwork does not assume text is generated with the goal of easy\ndetection; DetectGPT detects text generated from publicly\navailable LLMs using standard LLM sampling strategies.\nThe widespread use of LLMs has led to much other con-\ntemporaneous work on detecting LLM output. Sadasivan\net al. (2023) show that the detection AUROC of the an de-\ntector is upper bounded by a function of the TV distance\nbetween the model and human text. However, we find that\nAUROC of DetectGPT is high even for the largest publicly-\navailable models (Table 2), suggesting that TV distance may\nnot correlate strongly with model scale and capability. This\ndisconnect may be exacerbated by new training objectives\nother than maximum likelihood, e.g., reinforcement learn-\ning with human feedback (Christiano et al., 2017; Ziegler\net al., 2020). Both Sadasivan et al. (2023) and Krishna et al.\n(2023) show the effectiveness of paraphrasing as a tool for\nevading detection, suggesting an important area of study\nfor future work. Liang et al. (2023) show that multi-lingual\ndetection is difficult, with non-DetectGPT detectors show-\ning bias against non-native speakers; this result highlights\nthe advantage of zero-shot detectors like DetectGPT, which\ngeneralize well to any data generated by the original gener-\nating model. Mireshghallah et al. (2023) study which proxy\nscoring models produce the most useful log probabilities\nfor detection when the generating model is not known (a\nlarge-scale version of our Figure 6). Surprisingly (but con-\nsistent with our findings), they find that smaller models are\nin fact better proxy models for performing detection with\nperturbation-based methods like DetectGPT.\nThe problem of machine-generated text detection echoes ear-\nlier work on detecting deepfakes, artificial images or videos\ngenerated by deep nets, which has spawned substantial ef-\nforts in detection of fake visual content (Dolhansky et al.,\n2020; Zi et al., 2020). While early works in deepfake de-\ntection used relatively general-purpose model architectures\n(G¨uera & Delp, 2018), many deepfake detection methods\nrely on the continuous nature of image data to achieve state-\nof-the-art performance (Zhao et al., 2021; Guarnera et al.,\n2020), making direct application to text difficult.\n3. The Zero-Shot Machine-Generated Text\nDetection Problem\nWe study zero-shot machine-generated text detection, the\nproblem of detecting whether a piece of text, or candidate\npassage x, is a sample from a source model pθ. The problem\nis zero-shot in the sense that we do not assume access to\nhuman-written or generated samples to perform detection.\nAs in prior work, we study a ‘white box’ setting (Gehrmann\net al., 2019) in which the detector may evaluate the log prob-Algorithm 1 DetectGPT model-generated text detection\n1:Input: passage x, source model pθ, perturbation function q,\nnumber of perturbations k, decision threshold ϵ\n2:˜xi∼q(· |x), i∈[1..k]// mask spans, sample replacements\n3:˜µ←1\nkP\nilogpθ(˜xi)// approximate expectation in Eq. 1\n4:ˆdx←logpθ(x)−˜µ // estimate d(x, pθ, q)\n5:˜σ2\nx←1\nk−1P\ni(logpθ(˜xi)−˜µ)2// variance for normalization\n6:ifˆdx√˜σx> ϵthen\n7: return true // probably model sample\n8:else\n9: return false // probably not model sample\nability of a sample logpθ(x). The white box setting does\nnotassume access to the model architecture or parameters.\nMost public APIs for LLMs (such as GPT-3) enable scoring\ntext, though some exceptions exist, notably ChatGPT. While\nmost of our experiments consider the white box setting, see\nSection 5.2 for experiments in which we score text using\nmodels other than the source model. See Mireshghallah\net al. (2023) for a comprehensive evaluation in this setting.\nThe detection criterion we propose, DetectGPT, also makes'
 'Zero-Shot Machine-Generated Text Detection using Probability Curvature\n0.00 0.08 0.16 0.04 0.12 0.24 0.20\nFraction of GPT-J-generated news article re-written0.60.70.80.91.0Detection AUROC\nRank\nDetectGPTLogRank\nLikelihoodEntropy\nFigure 5. We simulate human edits to machine-generated text by\nreplacing varying fractions of model samples with T5-3B gener-\nated text (masking out random five word spans until r% of text is\nmasked to simulate human edits to machine-generated text). The\nfour top-performing methods all generally degrade in performance\nwith heavier revision, but DetectGPT is consistently most accurate.\nExperiment is conducted on the XSum dataset.\nfor each token, we cannot compare to the rank, log rank, and\nentropy-based prior methods. We sample 150 examples5\nfrom the PubMedQA, XSum, and WritingPrompts datasets\nand compare the two pre-trained RoBERTa-based detector\nmodels with DetectGPT and the probability thresholding\nbaseline. We show in Table 2 that DetectGPT can provide\ndetection competitive with or better than the stronger of the\ntwo supervised models, and it again greatly outperforms\nprobability thresholding on average.\n5.2. Variants of Machine-Generated Text Detection\nDetecting paraphrased machine-generated text. In prac-\ntice, humans may manually edit or refine machine-generated\ntext rather than blindly use a model’s generations for their\ntask of interest. We therefore conduct an experiment to\nsimulate the detection problem for model samples that have\nbeen increasingly heavily revised. We simulate human re-\nvision by replacing 5 word spans of the text with samples\nfrom T5-3B until r% of the text has been replaced, and\nreport performance as rvaries. Figure 5 shows that De-\ntectGPT maintains detection AUROC above 0.8 even when\nnearly a quarter of the text in model samples has been re-\nplaced. Unsurprisingly, almost all methods show a gradual\ndegradation in performance as the sample is more heavily\nrevised. The entropy baseline shows surprisingly robust\nperformance in this setting (althought it is least accurate\non average), even slightly improving detection performance\nup to 24% replacement. DetectGPT shows the strongest\ndetection performance for all revision levels.\nImpact of alternative decoding strategies on detection.\nWhile Table 1 suggests that DetectGPT is effective for\n5We reduce the number of evaluation samples from 500 in our\nmain experiments to reduce the API costs of these experiments.XSum SQuAD WritingPrompts\nMethod top- ptop-ktop-ptop-ktop-p top-k\nlogp(x) 0.92 0.87 0.89 0.85 0.98 0.96\nRank 0.76 0.76 0.81 0.80 0.84 0.83\nLogRank 0.93* 0.90* 0.92* 0.90* 0.98 0.97\nEntropy 0.53 0.55 0.54 0.56 0.32 0.35\nDetectGPT 0.98 0.98 0.94 0.93 0.98 0.97\nTable 3. AUROC for zero-shot methods averaged across the five\nmodels in Table 1 for both top- kand top- psampling, with k=\n40andp= 0.96. Both settings enable slightly more accurate\ndetection, and DetectGPT consistently provides the best detection\nperformance. See Appendix Tables 4 and 5 for complete results.\ndetecting machine-generated text, prior work notes that\nthe decoding strategy (i.e., temperature sampling, top- k,\nnucleus/top- p) can impact the difficulty of detection. We re-\npeat the analysis from Section 5.1 using top- ksampling and\nnucleus sampling. Top- ksampling truncates the sampling\ndistribution to only the khighest-probability next tokens;\nnucleus sampling samples from only the smallest set of to-\nkens whose combined probability exceeds p. The results\nare summarized in Table 3; Appendix Tables 4 and 5 show\ncomplete results. We use k= 40 , andp= 0.96, in line with\nprior work (Ippolito et al., 2020). We find that both top- k\nand nucleus sampling make detection easier, on average.\nAveraging across domains, DetectGPT provides the clearest\nsignal for zero-shot detection.\nDetection when the source model is unknown. While\nour experiments have focused on the white-box setting\nfor machine-generated text detection, in this section, we'
 'models other than the source model. See Mireshghallah\net al. (2023) for a comprehensive evaluation in this setting.\nThe detection criterion we propose, DetectGPT, also makes\nuse of generic pre-trained mask-filling models in order to\ngenerate passages that are ‘nearby’ the candidate passage.\nHowever, these mask-filling models are used off-the-shelf,\nwithout any fine-tuning or adaptation to the target domain.\n4. DetectGPT: Zero-shot Machine-Generated\nText Detection with Random Perturbations\nDetectGPT is based on the hypothesis that samples from a\nsource model pθtypically lie in areas of negative curvature\nof the log probability function of pθ, unlike human text. In\nother words, if we apply small perturbations to a passage\nx∼pθ, producing ˜x, the quantity logpθ(x)−logpθ(˜x)\nshould be relatively large on average for machine-generated\nsamples compared to human-written text. To leverage this\nhypothesis, first consider a perturbation function q(· |x)\nthat gives a distribution over ˜x, slightly modified versions of\nxwith similar meaning (we will generally consider roughly\nparagraph-length texts x). As an example, q(· |x)might be\nthe result of simply asking a human to rewrite one of the\nsentences of x, while preserving the meaning of x. Using\nthe notion of a perturbation function, we can define the\nperturbation discrepancy d(x, pθ, q):\nd(x, pθ, q)≜logpθ(x)−E˜x∼q(·|x)logpθ(˜x)(1)\nWe state our hypothesis more formally as the Local Pertur-\nbation Discrepancy Gap Hypothesis, which describes a gap\nin the perturbation discrepancy for model-generated text\nand human-generated text.\nPerturbation Discrepancy Gap Hypothesis. Ifqproduces\nsamples on the data manifold, d(x, pθ, q)is positive and\nlarge with high probability for samples x∼pθ. For human-\nwritten text, d(x, pθ, q)tends toward zero for all x.\n3']","Previous zero-shot methods mainly relied on evaluating the average per-token log probability of generated text, whereas DetectGPT assesses the local structure around a passage through log probability curvature.",0.999999999975,0.6666666666666666,0.9385040265855039,1.0,0.4595963962218542,0.9153086618104939
"What percentage of BERT's language understanding capabilities does DistilBERT retain, and what is the size reduction achieved?",DistilBERT retains 97% of BERT's language understanding capabilities and achieves a size reduction of 40%.,"['6 Conclusion and future work\nWe introduced DistilBERT, a general-purpose pre-trained version of BERT, 40% smaller, 60% faster,\nthat retains 97% of the language understanding capabilities. We showed that a general-purpose\nlanguage model can be successfully trained with distillation and analyzed the various components\nwith an ablation study. We further demonstrated that DistilBERT is a compelling option for edge\napplications.\nReferences\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. In NAACL-HLT , 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\nunsupervised multitask learners. 2019.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar S. Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke S. Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv ,\nabs/1907.11692, 2019.\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green ai. ArXiv , abs/1907.10597, 2019.\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in\nnlp. In ACL, 2019.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In NIPS , 2017.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. Transformers: State-of-the-art natural language\nprocessing, 2019.\nCristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In KDD , 2006.\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. ArXiv ,\nabs/1503.02531, 2015.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading\nbooks. 2015 IEEE International Conference on Computer Vision (ICCV) , pages 19–27, 2015.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A\nmulti-task benchmark and analysis platform for natural language understanding. In ICLR , 2018.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. Deep contextualized word representations. In NAACL , 2018.\nAlex Wang, Ian F. Tenney, Yada Pruksachatkun, Katherin Yu, Jan Hula, Patrick Xia, Raghu Pappagari, Shuning\nJin, R. Thomas McCoy, Roma Patel, Yinghui Huang, Jason Phang, Edouard Grave, Najoung Kim, Phu Mon\nHtut, Thibault F’evry, Berlin Chen, Nikita Nangia, Haokun Liu, Anhad Mohananey, Shikha Bordia, Nicolas\nPatry, Ellie Pavlick, and Samuel R. Bowman. jiant 1.1: A software toolkit for research on general-purpose\ntext understanding models. http://jiant.info/ , 2019.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y . Ng, and Christopher Potts. Learning\nword vectors for sentiment analysis. In ACL, 2011.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for machine\ncomprehension of text. In EMNLP , 2016.\nRaphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. Distilling task-speciﬁc\nknowledge from bert into simple neural networks. ArXiv , abs/1903.12136, 2019.\nDebajyoti Chatterjee. Making neural machine reading comprehension faster. ArXiv , abs/1904.00796, 2019.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better: The impact\nof student initialization on knowledge distillation. ArXiv , abs/1908.08962, 2019.\nZe Yang, Linjun Shou, Ming Gong, Wutao Lin, and Daxin Jiang. Model compression with multi-task knowledge\ndistillation for web-scale question answering system. ArXiv , abs/1904.09636, 2019.'
 'Table 1: DistilBERT retains 97% of BERT performance. Comparison on the dev sets of the\nGLUE benchmark. ELMo results as reported by the authors. BERT and DistilBERT results are the\nmedians of 5 runs with different seeds.\nModel Score CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B WNLI\nELMo 68.7 44.1 68.6 76.6 71.1 86.2 53.4 91.5 70.4 56.3\nBERT-base 79.5 56.3 86.7 88.6 91.8 89.6 69.3 92.7 89.0 53.5\nDistilBERT 77.0 51.3 82.2 87.5 89.2 88.5 59.9 91.3 86.9 56.3\nTable 2: DistilBERT yields to comparable\nperformance on downstream tasks. Com-\nparison on downstream tasks: IMDb (test ac-\ncuracy) and SQuAD 1.1 (EM/F1 on dev set).\nD: with a second step of distillation during\nﬁne-tuning.\nModel IMDb SQuAD\n(acc.) (EM/F1)\nBERT-base 93.46 81.2/88.5\nDistilBERT 92.82 77.7/85.8\nDistilBERT (D) - 79.1/86.9Table 3: DistilBERT is signiﬁcantly smaller\nwhile being constantly faster. Inference\ntime of a full pass of GLUE task STS-B (sen-\ntiment analysis) on CPU with a batch size of\n1.\nModel # param. Inf. time\n(Millions) (seconds)\nELMo 180 895\nBERT-base 110 668\nDistilBERT 66 410\nDistillation We applied best practices for training BERT model recently proposed in Liu et al. [2019].\nAs such, DistilBERT is distilled on very large batches leveraging gradient accumulation (up to 4K\nexamples per batch) using dynamic masking and without the next sentence prediction objective.\nData and compute power We train DistilBERT on the same corpus as the original BERT model:\na concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015]. DistilBERT\nwas trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the\nRoBERTa model [Liu et al., 2019] required 1 day of training on 1024 32GB V100.\n4 Experiments\nGeneral Language Understanding We assess the language understanding and generalization ca-\npabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark\n[Wang et al., 2018], a collection of 9 datasets for evaluating natural language understanding systems.\nWe report scores on the development sets for each task by ﬁne-tuning DistilBERT without the use\nof ensembling or multi-tasking scheme for ﬁne-tuning (which are mostly orthogonal to the present\nwork). We compare the results to the baseline provided by the authors of GLUE: an ELMo (Peters\net al. [2018]) encoder followed by two BiLSTMs.4\nThe results on each of the 9 tasks are showed on Table 1 along with the macro-score (average of\nindividual scores). Among the 9 tasks, DistilBERT is always on par or improving over the ELMo\nbaseline (up to 19 points of accuracy on STS-B). DistilBERT also compares surprisingly well to\nBERT, retaining 97% of the performance with 40% fewer parameters.\n4.1 Downstream task benchmark\nDownstream tasks We further study the performances of DistilBERT on several downstream tasks\nunder efﬁcient inference constraints: a classiﬁcation task (IMDb sentiment classiﬁcation - Maas et al.\n[2011]) and a question answering task (SQuAD v1.1 - Rajpurkar et al. [2016]).\nAs shown in Table 2, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb\nbenchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.9 points of the full BERT.\nWe also studied whether we could add another step of distillation during the adaptation phase by\nﬁne-tuning DistilBERT on SQuAD using a BERT model previously ﬁne-tuned on SQuAD as a\n4We use jiant [Wang et al., 2019] to compute the baseline.\n3'
 'DistilBERT, a distilled version of BERT: smaller,\nfaster, cheaper and lighter\nVictor SANH, Lysandre DEBUT, Julien CHAUMOND, Thomas WOLF\nHugging Face\n{victor,lysandre,julien,thomas}@huggingface.co\nAbstract\nAs Transfer Learning from large-scale pre-trained models becomes more prevalent\nin Natural Language Processing (NLP), operating these large models in on-the-\nedge and/or under constrained computational training or inference budgets remains\nchallenging. In this work, we propose a method to pre-train a smaller general-\npurpose language representation model, called DistilBERT, which can then be ﬁne-\ntuned with good performances on a wide range of tasks like its larger counterparts.\nWhile most prior work investigated the use of distillation for building task-speciﬁc\nmodels, we leverage knowledge distillation during the pre-training phase and show\nthat it is possible to reduce the size of a BERT model by 40%, while retaining 97%\nof its language understanding capabilities and being 60% faster. To leverage the\ninductive biases learned by larger models during pre-training, we introduce a triple\nloss combining language modeling, distillation and cosine-distance losses. Our\nsmaller, faster and lighter model is cheaper to pre-train and we demonstrate its\ncapabilities for on-device computations in a proof-of-concept experiment and a\ncomparative on-device study.\n1 Introduction\nFigure 1: Parameter counts of several recently released\npretrained language models.The last two years have seen the rise\nof Transfer Learning approaches in\nNatural Language Processing (NLP)\nwith large-scale pre-trained language\nmodels becoming a basic tool in\nmany NLP tasks [Devlin et al., 2018,\nRadford et al., 2019, Liu et al., 2019].\nWhile these models lead to signiﬁ-\ncant improvement, they often have\nseveral hundred million parameters\nand current research1on pre-trained\nmodels indicates that training even\nlarger models still leads to better per-\nformances on downstream tasks.\nThe trend toward bigger models\nraises several concerns. First is the\nenvironmental cost of exponentially scaling these models’ computational requirements as mentioned\nin Schwartz et al. [2019], Strubell et al. [2019]. Second, while operating these models on-device\nin real-time has the potential to enable novel and interesting language processing applications, the\ngrowing computational and memory requirements of these models may hamper wide adoption.\n1See for instance the recently released MegatronLM ( https://nv-adlr.github.io/MegatronLM )\nEMC^2: 5th Edition Co-located with NeurIPS’19arXiv:1910.01108v4  [cs.CL]  1 Mar 2020'
 '(Zellers et al., 2018), this new task of common-\nsense natural language inference seemed trivial\nfor humans (88%) and yet challenging for then-\nstate-of-the-art models ( ă60%), including ELMo\n(Peters et al., 2018). However, BERT (Devlin\net al., 2018) soon reached over 86%, almost\nhuman-level performance. One news article on\nthis development was headlined “ ﬁnally, a ma-\nchine that can ﬁnish your sentence. ”1\nIn this paper, we investigate the following ques-\ntion: How well do deep pretrained models, like\n1A New York Times article at https: //nyti.ms /2DycutY.\n1arXiv:1905.07830v1  [cs.CL]  19 May 2019']",DistilBERT retains 97% of BERT's language understanding capabilities with a 40% reduction in size.,0.9999999999666667,1.0,0.9676369557771279,1.0,0.9982673102003822,0.9930475396196595
"What datasets and computational resources were used to train DistilBERT, and how do they compare to the original BERT training setup?","DistilBERT was trained on the same corpus as the original BERT model, which includes a concatenation of English Wikipedia and the Toronto Book Corpus. It utilized 8 16GB V100 GPUs for approximately 90 hours, whereas the BERT model required 1 day of training on 1024 32GB V100 GPUs. This indicates that DistilBERT is trained with fewer computational resources and less time compared to the original BERT setup.","['DistilBERT, a distilled version of BERT: smaller,\nfaster, cheaper and lighter\nVictor SANH, Lysandre DEBUT, Julien CHAUMOND, Thomas WOLF\nHugging Face\n{victor,lysandre,julien,thomas}@huggingface.co\nAbstract\nAs Transfer Learning from large-scale pre-trained models becomes more prevalent\nin Natural Language Processing (NLP), operating these large models in on-the-\nedge and/or under constrained computational training or inference budgets remains\nchallenging. In this work, we propose a method to pre-train a smaller general-\npurpose language representation model, called DistilBERT, which can then be ﬁne-\ntuned with good performances on a wide range of tasks like its larger counterparts.\nWhile most prior work investigated the use of distillation for building task-speciﬁc\nmodels, we leverage knowledge distillation during the pre-training phase and show\nthat it is possible to reduce the size of a BERT model by 40%, while retaining 97%\nof its language understanding capabilities and being 60% faster. To leverage the\ninductive biases learned by larger models during pre-training, we introduce a triple\nloss combining language modeling, distillation and cosine-distance losses. Our\nsmaller, faster and lighter model is cheaper to pre-train and we demonstrate its\ncapabilities for on-device computations in a proof-of-concept experiment and a\ncomparative on-device study.\n1 Introduction\nFigure 1: Parameter counts of several recently released\npretrained language models.The last two years have seen the rise\nof Transfer Learning approaches in\nNatural Language Processing (NLP)\nwith large-scale pre-trained language\nmodels becoming a basic tool in\nmany NLP tasks [Devlin et al., 2018,\nRadford et al., 2019, Liu et al., 2019].\nWhile these models lead to signiﬁ-\ncant improvement, they often have\nseveral hundred million parameters\nand current research1on pre-trained\nmodels indicates that training even\nlarger models still leads to better per-\nformances on downstream tasks.\nThe trend toward bigger models\nraises several concerns. First is the\nenvironmental cost of exponentially scaling these models’ computational requirements as mentioned\nin Schwartz et al. [2019], Strubell et al. [2019]. Second, while operating these models on-device\nin real-time has the potential to enable novel and interesting language processing applications, the\ngrowing computational and memory requirements of these models may hamper wide adoption.\n1See for instance the recently released MegatronLM ( https://nv-adlr.github.io/MegatronLM )\nEMC^2: 5th Edition Co-located with NeurIPS’19arXiv:1910.01108v4  [cs.CL]  1 Mar 2020'
 'Table 1: DistilBERT retains 97% of BERT performance. Comparison on the dev sets of the\nGLUE benchmark. ELMo results as reported by the authors. BERT and DistilBERT results are the\nmedians of 5 runs with different seeds.\nModel Score CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B WNLI\nELMo 68.7 44.1 68.6 76.6 71.1 86.2 53.4 91.5 70.4 56.3\nBERT-base 79.5 56.3 86.7 88.6 91.8 89.6 69.3 92.7 89.0 53.5\nDistilBERT 77.0 51.3 82.2 87.5 89.2 88.5 59.9 91.3 86.9 56.3\nTable 2: DistilBERT yields to comparable\nperformance on downstream tasks. Com-\nparison on downstream tasks: IMDb (test ac-\ncuracy) and SQuAD 1.1 (EM/F1 on dev set).\nD: with a second step of distillation during\nﬁne-tuning.\nModel IMDb SQuAD\n(acc.) (EM/F1)\nBERT-base 93.46 81.2/88.5\nDistilBERT 92.82 77.7/85.8\nDistilBERT (D) - 79.1/86.9Table 3: DistilBERT is signiﬁcantly smaller\nwhile being constantly faster. Inference\ntime of a full pass of GLUE task STS-B (sen-\ntiment analysis) on CPU with a batch size of\n1.\nModel # param. Inf. time\n(Millions) (seconds)\nELMo 180 895\nBERT-base 110 668\nDistilBERT 66 410\nDistillation We applied best practices for training BERT model recently proposed in Liu et al. [2019].\nAs such, DistilBERT is distilled on very large batches leveraging gradient accumulation (up to 4K\nexamples per batch) using dynamic masking and without the next sentence prediction objective.\nData and compute power We train DistilBERT on the same corpus as the original BERT model:\na concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015]. DistilBERT\nwas trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the\nRoBERTa model [Liu et al., 2019] required 1 day of training on 1024 32GB V100.\n4 Experiments\nGeneral Language Understanding We assess the language understanding and generalization ca-\npabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark\n[Wang et al., 2018], a collection of 9 datasets for evaluating natural language understanding systems.\nWe report scores on the development sets for each task by ﬁne-tuning DistilBERT without the use\nof ensembling or multi-tasking scheme for ﬁne-tuning (which are mostly orthogonal to the present\nwork). We compare the results to the baseline provided by the authors of GLUE: an ELMo (Peters\net al. [2018]) encoder followed by two BiLSTMs.4\nThe results on each of the 9 tasks are showed on Table 1 along with the macro-score (average of\nindividual scores). Among the 9 tasks, DistilBERT is always on par or improving over the ELMo\nbaseline (up to 19 points of accuracy on STS-B). DistilBERT also compares surprisingly well to\nBERT, retaining 97% of the performance with 40% fewer parameters.\n4.1 Downstream task benchmark\nDownstream tasks We further study the performances of DistilBERT on several downstream tasks\nunder efﬁcient inference constraints: a classiﬁcation task (IMDb sentiment classiﬁcation - Maas et al.\n[2011]) and a question answering task (SQuAD v1.1 - Rajpurkar et al. [2016]).\nAs shown in Table 2, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb\nbenchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.9 points of the full BERT.\nWe also studied whether we could add another step of distillation during the adaptation phase by\nﬁne-tuning DistilBERT on SQuAD using a BERT model previously ﬁne-tuned on SQuAD as a\n4We use jiant [Wang et al., 2019] to compute the baseline.\n3'
 '6 Conclusion and future work\nWe introduced DistilBERT, a general-purpose pre-trained version of BERT, 40% smaller, 60% faster,\nthat retains 97% of the language understanding capabilities. We showed that a general-purpose\nlanguage model can be successfully trained with distillation and analyzed the various components\nwith an ablation study. We further demonstrated that DistilBERT is a compelling option for edge\napplications.\nReferences\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. In NAACL-HLT , 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\nunsupervised multitask learners. 2019.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar S. Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke S. Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv ,\nabs/1907.11692, 2019.\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green ai. ArXiv , abs/1907.10597, 2019.\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in\nnlp. In ACL, 2019.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In NIPS , 2017.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. Transformers: State-of-the-art natural language\nprocessing, 2019.\nCristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In KDD , 2006.\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. ArXiv ,\nabs/1503.02531, 2015.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading\nbooks. 2015 IEEE International Conference on Computer Vision (ICCV) , pages 19–27, 2015.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A\nmulti-task benchmark and analysis platform for natural language understanding. In ICLR , 2018.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. Deep contextualized word representations. In NAACL , 2018.\nAlex Wang, Ian F. Tenney, Yada Pruksachatkun, Katherin Yu, Jan Hula, Patrick Xia, Raghu Pappagari, Shuning\nJin, R. Thomas McCoy, Roma Patel, Yinghui Huang, Jason Phang, Edouard Grave, Najoung Kim, Phu Mon\nHtut, Thibault F’evry, Berlin Chen, Nikita Nangia, Haokun Liu, Anhad Mohananey, Shikha Bordia, Nicolas\nPatry, Ellie Pavlick, and Samuel R. Bowman. jiant 1.1: A software toolkit for research on general-purpose\ntext understanding models. http://jiant.info/ , 2019.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y . Ng, and Christopher Potts. Learning\nword vectors for sentiment analysis. In ACL, 2011.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for machine\ncomprehension of text. In EMNLP , 2016.\nRaphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. Distilling task-speciﬁc\nknowledge from bert into simple neural networks. ArXiv , abs/1903.12136, 2019.\nDebajyoti Chatterjee. Making neural machine reading comprehension faster. ArXiv , abs/1904.00796, 2019.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better: The impact\nof student initialization on knowledge distillation. ArXiv , abs/1908.08962, 2019.\nZe Yang, Linjun Shou, Ming Gong, Wutao Lin, and Daxin Jiang. Model compression with multi-task knowledge\ndistillation for web-scale question answering system. ArXiv , abs/1904.09636, 2019.'
 'alternatives that lead to better downstream task\nperformance; (2) We use a novel dataset, CC-\nNEWS, and conﬁrm that using more data for pre-\ntraining further improves performance on down-\nstream tasks; (3) Our training improvements show\nthat masked language model pretraining, under\nthe right design choices, is competitive with all\nother recently published methods. We release our\nmodel, pretraining and ﬁne-tuning code imple-\nmented in PyTorch ( Paszke et al. ,2017 ).\n2 Background\nIn this section, we give a brief overview of the\nBERT ( Devlin et al. ,2019 ) pretraining approach\nand some of the training choices that we will ex-\namine experimentally in the following section.\n2.1 Setup\nBERT takes as input a concatenation of two\nsegments (sequences of tokens), x1,...,x N\nandy1,...,yM. Segments usually consist of\nmore than one natural sentence. The two seg-\nments are presented as a single input sequence\nto BERT with special tokens delimiting them:\n[CLS],x1,...,x N,[SEP],y1,...,yM,[EOS].\nMandNare constrained such that M+N < T ,\nwhereTis a parameter that controls the maximum\nsequence length during training.\nThe model is ﬁrst pretrained on a large unla-\nbeled text corpus and subsequently ﬁnetuned us-\ning end-task labeled data.\n2.2 Architecture\nBERT uses the now ubiquitous transformer archi-\ntecture ( Vaswani et al. ,2017 ), which we will not\nreview in detail. We use a transformer architecture\nwithLlayers. Each block uses Aself-attention\nheads and hidden dimension H.\n2.3 Training Objectives\nDuring pretraining, BERT uses two objectives:\nmasked language modeling and next sentence pre-\ndiction.\nMasked Language Model (MLM) A random\nsample of the tokens in the input sequence is\nselected and replaced with the special token\n[MASK]. The MLM objective is a cross-entropy\nloss on predicting the masked tokens. BERT uni-\nformly selects 15% of the input tokens for possi-\nble replacement. Of the selected tokens, 80% are\nreplaced with [MASK], 10% are left unchanged,and 10% are replaced by a randomly selected vo-\ncabulary token.\nIn the original implementation, random mask-\ning and replacement is performed once in the be-\nginning and saved for the duration of training, al-\nthough in practice, data is duplicated so the mask\nis not always the same for every training sentence\n(see Section 4.1).\nNext Sentence Prediction (NSP) NSP is a bi-\nnary classiﬁcation loss for predicting whether two\nsegments follow each other in the original text.\nPositive examples are created by taking consecu-\ntive sentences from the text corpus. Negative ex-\namples are created by pairing segments from dif-\nferent documents. Positive and negative examples\nare sampled with equal probability.\nThe NSP objective was designed to improve\nperformance on downstream tasks, such as Natural\nLanguage Inference ( Bowman et al. ,2015 ), which\nrequire reasoning about the relationships between\npairs of sentences.\n2.4 Optimization\nBERT is optimized with Adam ( Kingma and Ba ,\n2015 ) using the following parameters: β1= 0.9,\nβ2= 0.999,ǫ=1e-6 and L2weight de-\ncay of0.01. The learning rate is warmed up\nover the ﬁrst 10,000 steps to a peak value of\n1e-4, and then linearly decayed. BERT trains\nwith a dropout of 0.1 on all layers and at-\ntention weights, and a GELU activation func-\ntion ( Hendrycks and Gimpel ,2016 ). Models are\npretrained for S=1,000,000 updates, with mini-\nbatches containing B=256 sequences of maxi-\nmum length T=512 tokens.\n2.5 Data\nBERT is trained on a combination of B OOK COR-\nPUS (Zhu et al. ,2015 ) plus English W IKIPEDIA ,\nwhich totals 16GB of uncompressed text.3\n3 Experimental Setup\nIn this section, we describe the experimental setup\nfor our replication study of BERT.\n3.1 Implementation\nWe reimplement BERT in FAIRSEQ (Ott et al. ,\n2019 ). We primarily follow the original BERT\n3Yang et al. (2019 ) use the same dataset but report having\nonly 13GB of text after data cleaning. This is most likely due\nto subtle differences in cleaning of the Wikipedia data.']","DistilBERT was trained on the same English Wikipedia and Toronto Book Corpus as BERT, using 8 NVIDIA V100 GPUs for approximately 90 hours, showcasing efficient resource use compared to larger models.",0.9999999999666667,0.5,0.9380290579777446,,0.9227258212591134,0.9636305577637265
"What findings were revealed about model performance on HellaSwag when evaluated in zero-shot scenarios, and what implications does this have for future model development?","In zero-shot scenarios, models significantly underperformed, achieving accuracy below 50%, indicating a gap of over 50% compared to human performance which exceeds 95%. This highlights the challenge of commonsense reasoning for current models, suggesting that future development may require not only advancements in model architecture but also improved pretraining and data curation strategies to address these limitations. The results imply that simply scaling models may not suffice; instead, a focus on adversarial training and more complex datasets will be crucial for progress in natural language inference tasks.","['Overall In-Domain Zero-Shot ActivityNet WikiHow\nModel Val Test Val Test Val Test Val Test Val Test\nSplit Size Ñ 10K 10K 5K 5K 5K 5K 3.2K 3.5K 6.8K 6.5K\nChance 25.0\nfastText 30.9 31.6 33.8 32.9 28.0 30.2 27.7 28.4 32.4 33.3\nLSTM +GloVe 31.9 31.7 34.3 32.9 29.5 30.4 34.3 33.8 30.7 30.5\nLSTM +ELMo 31.7 31.4 33.2 32.8 30.4 30.0 33.8 33.3 30.8 30.4\nLSTM +BERT-Base 35.9 36.2 38.7 38.2 33.2 34.1 40.5 40.5 33.7 33.8\nESIM +ELMo 33.6 33.3 35.7 34.2 31.5 32.3 37.7 36.6 31.6 31.5\nOpenAI GPT 41.9 41.7 45.3 44.0 38.6 39.3 46.4 43.8 39.8 40.5\nBERT-Base 39.5 40.5 42.9 42.8 36.1 38.3 48.9 45.7 34.9 37.7\nBERT-Large 46.7 47.3 50.2 49.7 43.3 45.0 54.7 51.7 42.9 45.0\nHuman 95.7 95.6 95.6 95.6 95.8 95.7 94.0 94.0 96.5 96.5\nTable 1: Performance of models, evaluated with accuracy (%).We report results on the full validation and test sets\n(Overall), as well as results on informative subsets of the data: evaluated on in-domain, versus zero-shot situations,\nalong with performance on the underlying data sources (ActivityNet versus WikiHow). All models substantially\nunderperform humans: the gap is over 45% on in-domain categories, and 50% on zero-shot categories.\nFigure 8: Examples on the in-domain validation set of\nHellaSwag , grouped by category label. Our evaluation\nsetup equally weights performance on categories seen\nduring training as well as out-of-domain.\n5 Results\nWe evaluate the di ﬃculty of HellaSwag using a va-\nriety of strong baselines, with and without mas-\nsive pretraining. The models share the same for-\nmat: given a context and an ending, return a logit\nfor that ending. Accordingly, we train our models\nusing a four-way cross-entropy loss, where the ob-\njective is to predict the correct ending. In addition\nto BERT-Large, our comparisons include:\na.OpenAI GPT (Radford et al., 2018): A ﬁne-\ntuned 12-layer transformer that was pre-trained on\nthe BookCorpus (Zhu et al., 2015).\nb.Bert-Base : A smaller version of the BERT\nmodel whose architecture size matches GPT.\nc.ESIM +ELMo (Chen et al., 2017; Peters et al.,\n2018): This is the best-performing ELMo model\nfor NLI, modiﬁed slightly so the ﬁnal output layeris now a four-way softmax over endings.\nd.LSTM sentence encoder : This is a randomly\ninitialized two-layer bi-LSTM; the second layer’s\nhidden states are max-pooled and fed into an MLP\nto predict the logit. We consider three varia-\ntions: GloVe embeddings, ELMo embeddings, or\n(frozen) BERT-Base embeddings.9\ne.FastText : (Joulin et al., 2017) An o ﬀ-the-shelf\nlibrary for bag-of-words text classiﬁcation.10\nWe compare all models to human performance\nby asking ﬁve independent crowd workers to solve\nthe same four-way multiple choice problems; their\npredictions are combined via majority vote.\nOur results, shown in Table 1, hint at the di ﬃ-\nculty of the dataset: human performance is over\n95%, while overall model performance is below\n50% for every model. Surprisingly, despite BERT-\nLarge having been used as the adversarial ﬁlter,\nit still performs the strongest at 47.3% overall.\nBy making the dataset adversarial for BERT, it\nseems to also have become adversarial for every\nother model. For instance, while ESIM +ELMo\nobtained 59% accuracy on SWAG, it obtains only\n33.3% accuracy on HellaSwag .\nIn addition to pretraining being critical, so too is\nend-to-end ﬁnetuning. Freezing BERT-Base and\nadding an LSTM on top lowers its overall perfor-\nmance 4.3%. This may help explain why mod-\nels such as ESIM +ELMo struggled on SWAG, as\nELMo isn’t updated during ﬁnetuning.\nWhile BERT is the best model, it still struggles\nonHellaSwag , and especially so on zero-shot cat-\n9For ELMo and BERT-Base, the model learns scalar\nweights to combine each internal layer of the encoder.\n10This model is trained with binary cross entropy loss.\n6'
 'lems of reporting bias (Gordon and Van Durme,\n2013). Alternatively, future models might answer\ncorrectly only by picking up on spurious patterns,\nin which case a new development of the bench-\nmark – using these models as adversaries – would\nplace us in the same position as we are right now.\nPut another way, for humans to answer Hella-\nSwag questions requires abstracting away from\nlanguage and modeling world states instead. We\npostulate that this is what separates solving the\ntask of commonsense NLI, as opposed to a par-\nticular dataset. Indeed, we ﬁnd that existing deep\nmethods often get fooled by lexical false friends.\nFor example, in the WikiHow example from Fig-\nure 10, BERT chooses an ending that matches\nthetechnology words in the context, rather than\nmatching the deeper topic: using technology as an\nexcuse for not doing homework.\n6.3 Towards a future of evolving benchmarks\nWhat happens when HellaSwag gets solved? We\nbelieve the answer is simple: crowdsource another\ndataset, with the same exact format, and see where\nmodels fail. Indeed, in our work we found this to\nbe straightforward from an algorithmic perspec-\ntive: by throwing in the best known generator\n(GPT) and the best known discriminator (BERT-\nLarge), we made a dataset that is adversarial - not\njust to BERT, but to all models we have access to.\nWhile this was easy algorithmically, care must\nbe taken from a data curation standpoint. Indeed,\nwe ﬁnd success exists within a Goldilocks zone:\nthe data source must be complex enough that state-\nof-the-art generators often make mistakes, while\nsimple enough such that discriminators often fail\nto catch them. This ties the future of SWAG-\nstyle benchmarks to progress on language gener-\nation: until generation is solved, commonsense\nNLI will remain unsolved. Even recent promis-\ning results on scaling up language models (Rad-\nford et al., 2019) ﬁnd problems in terms of consis-\ntency, with the best curated examples requiring 25\nrandom seeds.\n7 Conclusion\nIn this paper, we presented HellaSwag , a new\ndataset for physically situated commonsense rea-\nsoning. By constructing the dataset through ad-\nversarial ﬁltering, combined with state-of-the-art\nmodels for language generation and discrimina-\ntion, we produced a dataset that is adversarial tothe most robust models available – even when\nmodels are evaluated on items from the train-\ning distribution. In turn, we provided insight\ninto the inner workings of pretrained models, and\nsuggest a path for NLP progress going forward:\ntowards benchmarks that adversarially co-evolve\nwith evolving state-of-the-art models.\nAcknowledgments\nWe thank the reviewers, as well as Jesse Thoma-\nson, for their helpful feedback. We thank the\nMechanical Turk workers for their great work\nduring dataset collection. Thanks also to Zak\nStone and the Google Cloud TPU team for help\nwith the computing infrastructure. This work\nwas supported by the National Science Foundation\nthrough a Graduate Research Fellowship (DGE-\n1256082) and NSF grants (IIS-1524371, 1637479,\n165205, 1703166), the DARPA CwC program\nthrough ARO (W911NF-15-1-0543), the IARPA\nDIV A program through D17PC00343, the Sloan\nResearch Foundation through a Sloan Fellowship,\nthe Allen Institute for Artiﬁcial Intelligence, the\nNVIDIA Artiﬁcial Intelligence Lab, and gifts by\nGoogle and Facebook. The views and conclu-\nsions contained herein are those of the authors and\nshould not be interpreted as representing endorse-\nments of IARPA, DOI /IBC, or the U.S. Govern-\nment.\nReferences\nYonatan Belinkov and Yonatan Bisk. 2018. Synthetic\nand natural noise both break neural machine transla-\ntion. In ICLR . ICLR.\nQian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui\nJiang, and Diana Inkpen. 2017. Enhanced lstm for\nnatural language inference. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers) , vol-\nume 1, pages 1657–1668.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and'
 'Stylistic\nEnsembleELMo+\nLSTMGPT BERTBase BERTLarge30405060708090100 Accuracy (%)\n48.2%53.7%64.8%71.4%83.0%\n28.0% 28.2% 28.4%32.0%41.1%78.5%77.4%\n71.3%\n63.0%\n41.1%Accuracy of the filtering model before AF\nAccuracy of the filtering model after AF\nBERT-Large accuracy after AFFigure 11: Performance on the WikiHow subset of al-\nternative variations of HellaSwag , where di ﬀerent Ad-\nversarial Filters are used (but without human valida-\ntion). We consider the shallow stylistic adversaries\nused by Zellers et al. (2018) (Stylistic Ensemble),\nas well as an LSTM with ELMo embeddings, GPT,\nBERT-Base, and BERT-Large. For each adversarial ﬁl-\ntering model, we record the accuracy of that model be-\nfore and after AF is used. We also evaluate each al-\nternative dataset using BERT-Large. The results sug-\ngest that using a a stronger model at test time (over the\nmodel used for AF) improves performance, but is not\nenough to solve the task.\n6 Discussion\nOur results suggest that HellaSwag is a challenging\ntestbed for state-of-the-art NLI models, even those\nbuilt on extensive pretraining. The question still\nremains, though, of where will the ﬁeld go next?\n6.1 How easy might HellaSwag be for future\ndiscriminators?\nIn this paper, we showed the existence of a\nGoldilocks zone of text complexity – in which\ngenerations are nonsensical, but existing state-\nof-the-art NLP models cannot tell the di ﬀerence.\nHow hard will the dataset be for future, even more\npowerful, models?\nAnswering this question is challenging because\nthese models don’t exist (or are unavailable) at\nthe time of writing . However, one remedy is to\nperform an ablation study on the Adversarial Fil-\ntering model used, comparing weaker ﬁlters with\nstronger discriminators. We present our results\nin Figure 11, and ﬁnd that while weak discrim-\ninators (like the stylistic ensemble used to make\nSWAG) only marginally reduce the accuracy of\nBERT-Large, increasing the gap between the ﬁlter\nand the ﬁnal discriminator is not enough to solve\nthe task. For instance, using a discriminator with\n3x the parameters as the adversarial ﬁlter (BERT-\nLarge vs. BERT-Base) results in 63% machine ac-\ncuracy.\n30 40 50 60 70 80 90 100\nOverall Accuracy on HellaSwag1021041061081010Pretraining Hours (Estimate)\nHuman performance?\nELMoGPTBERT-BaseBERT-LargeFigure 12: Estimated pretraining hours required to\nreach a desired accuracy on HellaSwag . We estimate\nperfomance with respect to a RTX 2080 Ti - a modern,\nfast GPU, and ﬁt a log-linear regression line. An ex-\ntrapolation suggests that to reach human-level perfor-\nmance on HellaSwag , without algorithmic or computa-\ntional improvements, would require 109GPU-hours of\npretraining (over 100k GPU years).\n6.2 How well does pretraining scale?\nOverall, the current paradigm of pretraining large\nmodels on lots of data has made immense progress\non NLP benchmarks. Though we expect this\ntrend to continue, it also behooves us to con-\nsider its limits. If more compute is indeed the\nanswer for human-level commonsense inference,\nwhat would the compute requirements of this hy-\npothetical massive model look like?\nWe investigate this in Figure 12 by compar-\ning the accuracies of known models on Hella-\nSwag with their computational needs. This estima-\ntion is a rough estimate: we convert reported TPU\nruntimes to our benchmark RTX 2080 Ti GPU us-\ning the Rooﬂine model (Williams et al., 2009),\nwhich focuses primarily on the bottleneck of load-\ning tensors into GPU memory. Extrapolating from\nan exponential ﬁt suggests that reaching human-\nlevel performance on our dataset would require\n109GPU hours, or 100k years – unless algorith-\nmic improvements are made.\nWhat might these algorithmic improvements\nlook like? These could include architectural ad-\nvances, better pretraining objectives, and beyond.\nHowever, these improvements share the bottle-\nneck of the data source. To answer some Hella-\nSwag questions correctly without reasoning deeply\n– like knowing that it is a bad idea to stop at a'
 ""HellaSwag : Can a Machine Really Finish Your Sentence?\nRowan Zellers♠Ari Holtzman♠Yonatan Bisk♠Ali Farhadi♠♥Yejin Choi♠♥\n♠Paul G. Allen School of Computer Science & Engineering, University of Washington\n♥Allen Institute for Artiﬁcial Intelligence\nhttps://rowanzellers.com/hellaswag\nAbstract\nRecent work by Zellers et al. (2018) intro-\nduced a new task of commonsense natural lan-\nguage inference : given an event description\nsuch as “A woman sits at a piano,” a machine\nmust select the most likely followup: “She\nsets her ﬁngers on the keys.” With the intro-\nduction of BERT (Devlin et al., 2018), near\nhuman-level performance was reached. Does\nthis mean that machines can perform human\nlevel commonsense inference?\nIn this paper, we show that commonsense in-\nference still proves di ﬃcult for even state-\nof-the-art models, by presenting HellaSwag ,\na new challenge dataset. Though its ques-\ntions are trivial for humans ( ą95% accuracy),\nstate-of-the-art models struggle ( ă48%). We\nachieve this via Adversarial Filtering (AF), a\ndata collection paradigm wherein a series of\ndiscriminators iteratively select an adversarial\nset of machine-generated wrong answers. AF\nproves to be surprisingly robust. The key in-\nsight is to scale up the length and complex-\nity of the dataset examples towards a critical\n‘Goldilocks’ zone wherein generated text is\nridiculous to humans, yet often misclassiﬁed\nby state-of-the-art models.\nOur construction of HellaSwag , and its result-\ning di ﬃculty, sheds light on the inner work-\nings of deep pretrained models. More broadly,\nit suggests a new path forward for NLP re-\nsearch, in which benchmarks co-evolve with\nthe evolving state-of-the-art in an adversarial\nway, so as to present ever-harder challenges.\n1 Introduction\nImagine a woman chasing a dog around outside,\ntrying to give it a bath. What might happen next?\nHumans can read a narrative like this, shown in\nFigure 1, and connect it to a rich model of the\nworld: the dog is currently dry and not soapy, and\nit actively doesn’t want to be bathed. Thus, one\nA woman is outside with a bucket and a dog. The dog is running around trying to avoid a bath. She…A. rinses the bucket off with soap and blow dry the dog’s head.B. uses a hose to keep it from getting soapy.C. gets the dog wet, then it runs away again.D. gets into a bath tub with the dog.Come to a complete halt at a stop sign or red light. At a stop sign, come to a complete halt for about 2 seconds or until vehicles that arrived before you clear the intersection. If you're stopped at a red light, proceed when the light has turned green. …A. Stop for no more than two seconds, or until the light turns yellow. A red light in front of you indicates that you should stop.B. After you come to a complete stop, turn off your turn signal. Allow vehicles to move in different directions before moving onto the sidewalk.C. Stay out of the oncoming traﬃc. People coming in from behind may elect to stay left or right.D. If the intersection has a white stripe in your lane, stop before this line. Wait until all traﬃc has cleared before crossing the intersection.\nOpenAIGPT\nHow to determine who has right of way. \neasy!???+\nAdversarial Filtering+\nAdversarial FilteringFigure 1: Models like BERT struggle to ﬁnish the sen-\ntences in HellaSwag , even when they come from the\nsame distribution as the training set. While the wrong\nendings are on-topic, with words that relate to the con-\ntext, humans consistently judge their meanings to be\neither incorrect or implausible. For example, option A\nof the WikiHow passage suggests that a driver should\nstop at a red light for no more than two seconds .\nplausible next event is option C—that she’ll get\nthe dog wet and it will run away again.\nWhen the SWAG dataset was ﬁrst announced\n(Zellers et al., 2018), this new task of common-\nsense natural language inference seemed trivial\nfor humans (88%) and yet challenging for then-\nstate-of-the-art models ( ă60%), including ELMo""]","Models showed a decline in performance in zero-shot scenarios, highlighting the need for future models to improve generalization capabilities and adapt to unseen contexts or domains.",0.999999999975,1.0,0.9260376162769449,1.0,0.33351941081300407,0.9055062146805877
Describe RoBERTa's approach to training with large mini-batches and its effect on model optimization and performance.,"RoBERTa's approach to training involves using larger mini-batches, specifically eight times larger than BERT, which allows for more efficient data utilization during training. This strategy, combined with pretraining for longer periods and over more data, leads to significant improvements in model performance across various tasks. The use of larger mini-batches helps stabilize the training process and enhances optimization, ultimately enabling RoBERTa to achieve state-of-the-art results on benchmarks like GLUE and SQuAD.","['and (2) the number of training passes through the\ndata. For example, the recently proposed XLNet\narchitecture ( Yang et al. ,2019 ) is pretrained us-\ning nearly 10 times more data than the original\nBERT ( Devlin et al. ,2019 ). It is also trained with\na batch size eight times larger for half as many op-\ntimization steps, thus seeing four times as many\nsequences in pretraining compared to BERT.\nTo help disentangle the importance of these fac-\ntors from other modeling choices (e.g., the pre-\ntraining objective), we begin by training RoBERTa\nfollowing the BERT LARGE architecture ( L= 24 ,\nH= 1024 ,A= 16 , 355M parameters). We\npretrain for 100K steps over a comparable B OOK -\nCORPUS plus W IKIPEDIA dataset as was used in'
 'arXiv:1907.11692v1  [cs.CL]  26 Jul 2019RoBERTa: A Robustly Optimized BERT Pretraining Approach\nYinhan Liu∗§Myle Ott∗§Naman Goyal∗§Jingfei Du∗§Mandar Joshi†\nDanqi Chen§Omer Levy§Mike Lewis§Luke Zettlemoyer†§Veselin Stoyanov§\n†Paul G. Allen School of Computer Science & Engineering,\nUniversity of Washington, Seattle, WA\n{mandar90,lsz }@cs.washington.edu\n§Facebook AI\n{yinhanliu,myleott,naman,jingfeidu,\ndanqi,omerlevy,mikelewis,lsz,ves }@fb.com\nAbstract\nLanguage model pretraining has led to sig-\nniﬁcant performance gains but careful com-\nparison between different approaches is chal-\nlenging. Training is computationally expen-\nsive, often done on private datasets of different\nsizes, and, as we will show, hyperparameter\nchoices have signiﬁcant impact on the ﬁnal re-\nsults. We present a replication study of BERT\npretraining ( Devlin et al. ,2019 ) that carefully\nmeasures the impact of many key hyperparam-\neters and training data size. We ﬁnd that BERT\nwas signiﬁcantly undertrained, and can match\nor exceed the performance of every model\npublished after it. Our best model achieves\nstate-of-the-art results on GLUE, RACE and\nSQuAD. These results highlight the impor-\ntance of previously overlooked design choices,\nand raise questions about the source of re-\ncently reported improvements. We release our\nmodels and code.1\n1 Introduction\nSelf-training methods such as ELMo ( Peters et al. ,\n2018 ), GPT ( Radford et al. ,2018 ), BERT\n(Devlin et al. ,2019 ), XLM ( Lample and Conneau ,\n2019 ), and XLNet ( Yang et al. ,2019 ) have\nbrought signiﬁcant performance gains, but it can\nbe challenging to determine which aspects of\nthe methods contribute the most. Training is\ncomputationally expensive, limiting the amount\nof tuning that can be done, and is often done with\nprivate training data of varying sizes, limiting\nour ability to measure the effects of the modeling\nadvances.\n∗Equal contribution.\n1Our models and code are available at:\nhttps://github.com/pytorch/fairseqWe present a replication study of BERT pre-\ntraining ( Devlin et al. ,2019 ), which includes a\ncareful evaluation of the effects of hyperparmeter\ntuning and training set size. We ﬁnd that BERT\nwas signiﬁcantly undertrained and propose an im-\nproved recipe for training BERT models, which\nwe call RoBERTa, that can match or exceed the\nperformance of all of the post-BERT methods.\nOur modiﬁcations are simple, they include: (1)\ntraining the model longer, with bigger batches,\nover more data; (2) removing the next sentence\nprediction objective; (3) training on longer se-\nquences; and (4) dynamically changing the mask-\ning pattern applied to the training data. We also\ncollect a large new dataset (CC-N EWS) of compa-\nrable size to other privately used datasets, to better\ncontrol for training set size effects.\nWhen controlling for training data, our im-\nproved training procedure improves upon the pub-\nlished BERT results on both GLUE and SQuAD.\nWhen trained for longer over additional data, our\nmodel achieves a score of 88.5 on the public\nGLUE leaderboard, matching the 88.4 reported\nbyYang et al. (2019 ). Our model establishes a\nnew state-of-the-art on 4/9 of the GLUE tasks:\nMNLI, QNLI, RTE and STS-B. We also match\nstate-of-the-art results on SQuAD and RACE.\nOverall, we re-establish that BERT’s masked lan-\nguage model training objective is competitive\nwith other recently proposed training objectives\nsuch as perturbed autoregressive language model-\ning (Yang et al. ,2019 ).2\nIn summary, the contributions of this paper\nare: (1) We present a set of important BERT de-\nsign choices and training strategies and introduce\n2It is possible that these other methods could also improve\nwith more tuning. We leave this exploration to future work.'
 'Model data bsz stepsSQuADMNLI-m SST-2(v1.1/2.0)\nRoBERTa\nwith B OOKS + W IKI 16GB 8K 100K 93.6/87.3 89.0 95.3\n+ additional data ( §3.2) 160GB 8K 100K 94.0/87.7 89.3 95.6\n+ pretrain longer 160GB 8K 300K 94.4/88.7 90.0 96.1\n+ pretrain even longer 160GB 8K 500K 94.6/89.4 90.2 96.4\nBERT LARGE\nwith B OOKS + W IKI 13GB 256 1M 90.9/81.8 86.6 93.7\nXLNet LARGE\nwith B OOKS + W IKI 13GB 256 1M 94.0/87.8 88.4 94.4\n+ additional data 126GB 2K 500K 94.5/88.8 89.8 95.6\nTable 4: Development set results for RoBERTa as we pretrain o ver more data (16GB →160GB of text) and pretrain\nfor longer (100K →300K→500K steps). Each row accumulates improvements from the row s above. RoBERTa\nmatches the architecture and training objective of BERT LARGE . Results for BERT LARGE and XLNet LARGE are from\nDevlin et al. (2019 ) and Yang et al. (2019 ), respectively. Complete results on all GLUE tasks can be fo und in the\nAppendix.\nDevlin et al. (2019 ). We pretrain our model using\n1024 V100 GPUs for approximately one day.\nResults We present our results in Table 4. When\ncontrolling for training data, we observe that\nRoBERTa provides a large improvement over the\noriginally reported BERT LARGE results, reafﬁrming\nthe importance of the design choices we explored\nin Section 4.\nNext, we combine this data with the three ad-\nditional datasets described in Section 3.2. We\ntrain RoBERTa over the combined data with the\nsame number of training steps as before (100K).\nIn total, we pretrain over 160GB of text. We ob-\nserve further improvements in performance across\nall downstream tasks, validating the importance of\ndata size and diversity in pretraining.9\nFinally, we pretrain RoBERTa for signiﬁcantly\nlonger, increasing the number of pretraining steps\nfrom 100K to 300K, and then further to 500K. We\nagain observe signiﬁcant gains in downstream task\nperformance, and the 300K and 500K step mod-\nels outperform XLNet LARGE across most tasks. We\nnote that even our longest-trained model does not\nappear to overﬁt our data and would likely beneﬁt\nfrom additional training.\nIn the rest of the paper, we evaluate our best\nRoBERTa model on the three different bench-\nmarks: GLUE, SQuaD and RACE. Speciﬁcally\n9Our experiments conﬂate increases in data size and di-\nversity. We leave a more careful analysis of these two dimen-\nsions to future work.we consider RoBERTa trained for 500K steps over\nall ﬁve of the datasets introduced in Section 3.2.\n5.1 GLUE Results\nFor GLUE we consider two ﬁnetuning settings.\nIn the ﬁrst setting ( single-task, dev ) we ﬁnetune\nRoBERTa separately for each of the GLUE tasks,\nusing only the training data for the correspond-\ning task. We consider a limited hyperparameter\nsweep for each task, with batch sizes ∈ {16,32}\nand learning rates ∈ {1e−5,2e−5,3e−5}, with a\nlinear warmup for the ﬁrst 6% of steps followed by\na linear decay to 0. We ﬁnetune for 10 epochs and\nperform early stopping based on each task’s eval-\nuation metric on the dev set. The rest of the hyper-\nparameters remain the same as during pretraining.\nIn this setting, we report the median development\nset results for each task over ﬁve random initial-\nizations, without model ensembling.\nIn the second setting ( ensembles, test ), we com-\npare RoBERTa to other approaches on the test set\nvia the GLUE leaderboard. While many submis-\nsions to the GLUE leaderboard depend on multi-\ntask ﬁnetuning, our submission depends only on\nsingle-task ﬁnetuning . For RTE, STS and MRPC\nwe found it helpful to ﬁnetune starting from the\nMNLI single-task model, rather than the baseline\npretrained RoBERTa. We explore a slightly wider\nhyperparameter space, described in the Appendix,\nand ensemble between 5 and 7 models per task.'
 'is (L=64, H=512, A=2) with 235M parameters\n(Al-Rfou et al., 2018). By contrast, BERT BASE\ncontains 110M parameters and BERT LARGE con-\ntains 340M parameters.\nIt has long been known that increasing the\nmodel size will lead to continual improvements\non large-scale tasks such as machine translation\nand language modeling, which is demonstrated\nby the LM perplexity of held-out training data\nshown in Table 6. However, we believe that\nthis is the ﬁrst work to demonstrate convinc-\ningly that scaling to extreme model sizes also\nleads to large improvements on very small scale\ntasks, provided that the model has been sufﬁ-\nciently pre-trained. Peters et al. (2018b) presented']","RoBERTa's training with large mini-batches, significantly larger than those used for BERT, improves both optimization speed and end-task performance. This approach, facilitated by appropriate scaling of the learning rate, enables more efficient training and better generalization across tasks.",0.9999999999666667,0.5555555555555556,0.9045854543111673,0.5,0.9042355056162364,0.9502753557982792
What findings does RoBERTa reveal about the efficacy of masked language model (MLM) pretraining under its optimized design choices?,"RoBERTa demonstrates that the masked language model (MLM) pretraining is competitive with other recently proposed training objectives when optimized with specific design choices. The findings suggest that training longer with larger batches, removing the next sentence prediction objective, training on longer sequences, and dynamically changing the masking pattern significantly enhance performance on downstream tasks. Overall, these improvements enable RoBERTa to achieve state-of-the-art results on benchmarks like GLUE and SQuAD.","['arXiv:1907.11692v1  [cs.CL]  26 Jul 2019RoBERTa: A Robustly Optimized BERT Pretraining Approach\nYinhan Liu∗§Myle Ott∗§Naman Goyal∗§Jingfei Du∗§Mandar Joshi†\nDanqi Chen§Omer Levy§Mike Lewis§Luke Zettlemoyer†§Veselin Stoyanov§\n†Paul G. Allen School of Computer Science & Engineering,\nUniversity of Washington, Seattle, WA\n{mandar90,lsz }@cs.washington.edu\n§Facebook AI\n{yinhanliu,myleott,naman,jingfeidu,\ndanqi,omerlevy,mikelewis,lsz,ves }@fb.com\nAbstract\nLanguage model pretraining has led to sig-\nniﬁcant performance gains but careful com-\nparison between different approaches is chal-\nlenging. Training is computationally expen-\nsive, often done on private datasets of different\nsizes, and, as we will show, hyperparameter\nchoices have signiﬁcant impact on the ﬁnal re-\nsults. We present a replication study of BERT\npretraining ( Devlin et al. ,2019 ) that carefully\nmeasures the impact of many key hyperparam-\neters and training data size. We ﬁnd that BERT\nwas signiﬁcantly undertrained, and can match\nor exceed the performance of every model\npublished after it. Our best model achieves\nstate-of-the-art results on GLUE, RACE and\nSQuAD. These results highlight the impor-\ntance of previously overlooked design choices,\nand raise questions about the source of re-\ncently reported improvements. We release our\nmodels and code.1\n1 Introduction\nSelf-training methods such as ELMo ( Peters et al. ,\n2018 ), GPT ( Radford et al. ,2018 ), BERT\n(Devlin et al. ,2019 ), XLM ( Lample and Conneau ,\n2019 ), and XLNet ( Yang et al. ,2019 ) have\nbrought signiﬁcant performance gains, but it can\nbe challenging to determine which aspects of\nthe methods contribute the most. Training is\ncomputationally expensive, limiting the amount\nof tuning that can be done, and is often done with\nprivate training data of varying sizes, limiting\nour ability to measure the effects of the modeling\nadvances.\n∗Equal contribution.\n1Our models and code are available at:\nhttps://github.com/pytorch/fairseqWe present a replication study of BERT pre-\ntraining ( Devlin et al. ,2019 ), which includes a\ncareful evaluation of the effects of hyperparmeter\ntuning and training set size. We ﬁnd that BERT\nwas signiﬁcantly undertrained and propose an im-\nproved recipe for training BERT models, which\nwe call RoBERTa, that can match or exceed the\nperformance of all of the post-BERT methods.\nOur modiﬁcations are simple, they include: (1)\ntraining the model longer, with bigger batches,\nover more data; (2) removing the next sentence\nprediction objective; (3) training on longer se-\nquences; and (4) dynamically changing the mask-\ning pattern applied to the training data. We also\ncollect a large new dataset (CC-N EWS) of compa-\nrable size to other privately used datasets, to better\ncontrol for training set size effects.\nWhen controlling for training data, our im-\nproved training procedure improves upon the pub-\nlished BERT results on both GLUE and SQuAD.\nWhen trained for longer over additional data, our\nmodel achieves a score of 88.5 on the public\nGLUE leaderboard, matching the 88.4 reported\nbyYang et al. (2019 ). Our model establishes a\nnew state-of-the-art on 4/9 of the GLUE tasks:\nMNLI, QNLI, RTE and STS-B. We also match\nstate-of-the-art results on SQuAD and RACE.\nOverall, we re-establish that BERT’s masked lan-\nguage model training objective is competitive\nwith other recently proposed training objectives\nsuch as perturbed autoregressive language model-\ning (Yang et al. ,2019 ).2\nIn summary, the contributions of this paper\nare: (1) We present a set of important BERT de-\nsign choices and training strategies and introduce\n2It is possible that these other methods could also improve\nwith more tuning. We leave this exploration to future work.'
 'alternatives that lead to better downstream task\nperformance; (2) We use a novel dataset, CC-\nNEWS, and conﬁrm that using more data for pre-\ntraining further improves performance on down-\nstream tasks; (3) Our training improvements show\nthat masked language model pretraining, under\nthe right design choices, is competitive with all\nother recently published methods. We release our\nmodel, pretraining and ﬁne-tuning code imple-\nmented in PyTorch ( Paszke et al. ,2017 ).\n2 Background\nIn this section, we give a brief overview of the\nBERT ( Devlin et al. ,2019 ) pretraining approach\nand some of the training choices that we will ex-\namine experimentally in the following section.\n2.1 Setup\nBERT takes as input a concatenation of two\nsegments (sequences of tokens), x1,...,x N\nandy1,...,yM. Segments usually consist of\nmore than one natural sentence. The two seg-\nments are presented as a single input sequence\nto BERT with special tokens delimiting them:\n[CLS],x1,...,x N,[SEP],y1,...,yM,[EOS].\nMandNare constrained such that M+N < T ,\nwhereTis a parameter that controls the maximum\nsequence length during training.\nThe model is ﬁrst pretrained on a large unla-\nbeled text corpus and subsequently ﬁnetuned us-\ning end-task labeled data.\n2.2 Architecture\nBERT uses the now ubiquitous transformer archi-\ntecture ( Vaswani et al. ,2017 ), which we will not\nreview in detail. We use a transformer architecture\nwithLlayers. Each block uses Aself-attention\nheads and hidden dimension H.\n2.3 Training Objectives\nDuring pretraining, BERT uses two objectives:\nmasked language modeling and next sentence pre-\ndiction.\nMasked Language Model (MLM) A random\nsample of the tokens in the input sequence is\nselected and replaced with the special token\n[MASK]. The MLM objective is a cross-entropy\nloss on predicting the masked tokens. BERT uni-\nformly selects 15% of the input tokens for possi-\nble replacement. Of the selected tokens, 80% are\nreplaced with [MASK], 10% are left unchanged,and 10% are replaced by a randomly selected vo-\ncabulary token.\nIn the original implementation, random mask-\ning and replacement is performed once in the be-\nginning and saved for the duration of training, al-\nthough in practice, data is duplicated so the mask\nis not always the same for every training sentence\n(see Section 4.1).\nNext Sentence Prediction (NSP) NSP is a bi-\nnary classiﬁcation loss for predicting whether two\nsegments follow each other in the original text.\nPositive examples are created by taking consecu-\ntive sentences from the text corpus. Negative ex-\namples are created by pairing segments from dif-\nferent documents. Positive and negative examples\nare sampled with equal probability.\nThe NSP objective was designed to improve\nperformance on downstream tasks, such as Natural\nLanguage Inference ( Bowman et al. ,2015 ), which\nrequire reasoning about the relationships between\npairs of sentences.\n2.4 Optimization\nBERT is optimized with Adam ( Kingma and Ba ,\n2015 ) using the following parameters: β1= 0.9,\nβ2= 0.999,ǫ=1e-6 and L2weight de-\ncay of0.01. The learning rate is warmed up\nover the ﬁrst 10,000 steps to a peak value of\n1e-4, and then linearly decayed. BERT trains\nwith a dropout of 0.1 on all layers and at-\ntention weights, and a GELU activation func-\ntion ( Hendrycks and Gimpel ,2016 ). Models are\npretrained for S=1,000,000 updates, with mini-\nbatches containing B=256 sequences of maxi-\nmum length T=512 tokens.\n2.5 Data\nBERT is trained on a combination of B OOK COR-\nPUS (Zhu et al. ,2015 ) plus English W IKIPEDIA ,\nwhich totals 16GB of uncompressed text.3\n3 Experimental Setup\nIn this section, we describe the experimental setup\nfor our replication study of BERT.\n3.1 Implementation\nWe reimplement BERT in FAIRSEQ (Ott et al. ,\n2019 ). We primarily follow the original BERT\n3Yang et al. (2019 ) use the same dataset but report having\nonly 13GB of text after data cleaning. This is most likely due\nto subtle differences in cleaning of the Wikipedia data.'
 '7 Conclusion\nWe carefully evaluate a number of design de-\ncisions when pretraining BERT models. We\nﬁnd that performance can be substantially im-\nproved by training the model longer, with bigger\nbatches over more data; removing the next sen-\ntence prediction objective; training on longer se-\nquences; and dynamically changing the masking\npattern applied to the training data. Our improved\npretraining procedure, which we call RoBERTa,\nachieves state-of-the-art results on GLUE, RACE\nand SQuAD, without multi-task ﬁnetuning for\nGLUE or additional data for SQuAD. These re-\nsults illustrate the importance of these previ-\nously overlooked design decisions and suggest\nthat BERT’s pretraining objective remains com-\npetitive with recently proposed alternatives.\nWe additionally use a novel dataset,\nCC-N EWS, and release our models and\ncode for pretraining and ﬁnetuning at:\nhttps://github.com/pytorch/fairseq .\nReferences\nEneko Agirre, Llu’is M‘arquez, and Richard Wicen-\ntowski, editors. 2007. Proceedings of the Fourth\nInternational Workshop on Semantic Evaluations\n(SemEval-2007) .\nAlexei Baevski, Sergey Edunov, Yinhan Liu, Luke\nZettlemoyer, and Michael Auli. 2019. Cloze-\ndriven pretraining of self-attention networks. arXiv\npreprint arXiv:1903.07785 .\nRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,\nDanilo Giampiccolo, Bernardo Magnini, and Idan\nSzpektor. 2006. The second PASCAL recognising\ntextual entailment challenge. In Proceedings of the\nsecond PASCAL challenges workshop on recognis-\ning textual entailment .\nLuisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo\nGiampiccolo, and Bernardo Magnini. 2009. The\nﬁfth PASCAL recognizing textual entailment chal-\nlenge.\nSamuel R Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nInEmpirical Methods in Natural Language Process-\ning (EMNLP) .\nWilliam Chan, Nikita Kitaev, Kelvin Guu, Mitchell\nStern, and Jakob Uszkoreit. 2019. KERMIT: Gener-\native insertion-based modeling for sequences. arXiv\npreprint arXiv:1906.01604 .Ido Dagan, Oren Glickman, and Bernardo Magnini.\n2006. The PASCAL recognising textual entailment\nchallenge. In Machine learning challenges. evalu-\nating predictive uncertainty, visual object classiﬁca-\ntion, and recognising tectual entailment .\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in Neural Informa-\ntion Processing Systems (NIPS) .\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In North American Association for Com-\nputational Linguistics (NAACL) .\nWilliam B Dolan and Chris Brockett. 2005. Auto-\nmatically constructing a corpus of sentential para-\nphrases. In Proceedings of the International Work-\nshop on Paraphrasing .\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei,\nXiaodong Liu, Yu Wang, Jianfeng Gao, Ming\nZhou, and Hsiao-Wuen Hon. 2019. Uniﬁed\nlanguage model pre-training for natural language\nunderstanding and generation. arXiv preprint\narXiv:1905.03197 .\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand Bill Dolan. 2007. The third PASCAL recog-\nnizing textual entailment challenge. In Proceedings\nof the ACL-PASCAL workshop on textual entailment\nand paraphrasing .\nAaron Gokaslan and Vanya Cohen. 2019. Openweb-\ntext corpus. http://web.archive.org/\nsave/http://Skylion007.github.io/\nOpenWebTextCorpus .\nFelix Hamborg, Norman Meuschke, Corinna Bre-\nitinger, and Bela Gipp. 2017. news-please: A\ngeneric news crawler and extractor. In Proceedings\nof the 15th International Symposium of Information\nScience .\nDan Hendrycks and Kevin Gimpel. 2016. Gaus-\nsian error linear units (gelus). arXiv preprint\narXiv:1606.08415 .\nMatthew Honnibal and Ines Montani. 2017. spaCy 2:\nNatural language understanding with Bloom embed-\ndings, convolutional neural networks and incremen-\ntal parsing. To appear.\nJeremy Howard and Sebastian Ruder. 2018. Universal'
 'jority class.\nC Additional Ablation Studies\nC.1 Effect of Number of Training Steps\nFigure 5 presents MNLI Dev accuracy after ﬁne-\ntuning from a checkpoint that has been pre-trained\nforksteps. This allows us to answer the following\nquestions:\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh ﬁne-tuning accuracy?\nAnswer: Yes, BERT BASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\nC.2 Ablation for Different Masking\nProcedures\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n200 400 600 800 1,0007678808284\nPre-training Steps (Thousands)MNLI Dev Accuracy\nBERT BASE (Masked LM)\nBERT BASE (Left-to-Right)\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after ﬁne-tuning, starting\nfrom model parameters that have been pre-trained for\nksteps. The x-axis is the value of k.Note that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand ﬁne-tuning, as the [MASK] symbol never ap-\npears during the ﬁne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both ﬁne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npliﬁed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\nMasking Rates Dev Set Results\nMASK SAME RND MNLI NER\nFine-tune Fine-tune Feature-based\n80% 10% 10% 84.2 95.4 94.9\n100% 0% 0% 84.3 94.9 94.0\n80% 0% 20% 84.1 95.2 94.6\n80% 20% 0% 84.4 95.2 94.7\n0% 20% 80% 83.7 94.8 94.6\n0% 0% 100% 83.6 94.9 94.6\nTable 8: Ablation over different masking strategies.\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe[MASK] symbol for MLM; S AME means that\nwe keep the target token as is; R NDmeans that\nwe replace the target token with another random\ntoken.\nThe numbers in the left part of the table repre-\nsent the probabilities of the speciﬁc strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\nFrom the table it can be seen that ﬁne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the M ASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe R NDstrategy performs much worse than our\nstrategy as well.']","RoBERTa's findings reaffirm the efficacy of MLM pretraining, suggesting that with optimized design choices such as dynamic masking and training on extended sequences, MLM pretraining remains highly competitive with, or superior to, other recently proposed pretraining objectives.",0.999999999975,1.0,0.9407158718423126,1.0,0.8901274099011681,0.9355096396046725
Describe the triple loss used in DistilBERT's training and its components.,"The triple loss used in DistilBERT's training combines three components: the masked language modeling loss (Lmlm), the distillation loss (Lce), and a cosine embedding loss (Lcos). The distillation loss helps the smaller model (student) mimic the output distribution of the larger model (teacher), while the masked language modeling loss ensures effective language understanding. The cosine embedding loss aligns the hidden state vectors of the student and teacher, enhancing the overall performance of the distilled model.","['In this paper, we show that it is possible to reach similar performances on many downstream-tasks\nusing much smaller language models pre-trained with knowledge distillation, resulting in models\nthat are lighter and faster at inference time, while also requiring a smaller computational training\nbudget. Our general-purpose pre-trained models can be ﬁne-tuned with good performances on several\ndownstream tasks, keeping the ﬂexibility of larger models. We also show that our compressed models\nare small enough to run on the edge, e.g. on mobile devices.\nUsing a triple loss, we show that a 40% smaller Transformer (Vaswani et al. [2017]) pre-trained\nthrough distillation via the supervision of a bigger Transformer language model can achieve similar\nperformance on a variety of downstream tasks, while being 60% faster at inference time. Further\nablation studies indicate that all the components of the triple loss are important for best performances.\nWe have made the trained weights available along with the training code in the Transformers2\nlibrary from HuggingFace [Wolf et al., 2019].\n2 Knowledge distillation\nKnowledge distillation [Bucila et al., 2006, Hinton et al., 2015] is a compression technique in which\na compact model - the student - is trained to reproduce the behaviour of a larger model - the teacher -\nor an ensemble of models.\nIn supervised learning, a classiﬁcation model is generally trained to predict an instance class by\nmaximizing the estimated probability of gold labels. A standard training objective thus involves\nminimizing the cross-entropy between the model’s predicted distribution and the one-hot empirical\ndistribution of training labels. A model performing well on the training set will predict an output\ndistribution with high probability on the correct class and with near-zero probabilities on other\nclasses. But some of these ""near-zero"" probabilities are larger than others and reﬂect, in part, the\ngeneralization capabilities of the model and how well it will perform on the test set3.\nTraining loss The student is trained with a distillation loss over the soft target probabilities of\nthe teacher: Lce=∑\niti∗log(si)where ti(resp. si) is a probability estimated by the teacher\n(resp. the student). This objective results in a rich training signal by leveraging the full teacher\ndistribution. Following Hinton et al. [2015] we used a softmax-temperature :pi=exp(zi/T)∑\njexp(zj/T)\nwhere Tcontrols the smoothness of the output distribution and ziis the model score for the class i.\nThe same temperature Tis applied to the student and the teacher at training time, while at inference,\nTis set to 1 to recover a standard softmax .\nThe ﬁnal training objective is a linear combination of the distillation loss Lcewith the supervised\ntraining loss, in our case the masked language modeling lossLmlm [Devlin et al., 2018]. We found it\nbeneﬁcial to add a cosine embedding loss (Lcos) which will tend to align the directions of the student\nand teacher hidden states vectors.\n3 DistilBERT: a distilled version of BERT\nStudent architecture In the present work, the student - DistilBERT - has the same general architec-\nture as BERT. The token-type embeddings and the pooler are removed while the number of layers\nis reduced by a factor of 2. Most of the operations used in the Transformer architecture ( linear\nlayer andlayer normalisation ) are highly optimized in modern linear algebra frameworks and our\ninvestigations showed that variations on the last dimension of the tensor (hidden size dimension) have\na smaller impact on computation efﬁciency (for a ﬁxed parameters budget) than variations on other\nfactors like the number of layers. Thus we focus on reducing the number of layers.\nStudent initialization In addition to the previously described optimization and architectural choices,\nan important element in our training procedure is to ﬁnd the right initialization for the sub-network to'
 'Table 4: Ablation study. Variations are relative to the model trained with triple loss and teacher\nweights initialization.\nAblation Variation on GLUE macro-score\n∅-Lcos-Lmlm -2.96\nLce-∅-Lmlm -1.46\nLce-Lcos-∅ -0.31\nTriple loss + random weights initialization -3.69\nteacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two\nsuccessive steps of distillation, one during the pre-training phase and one during the adaptation phase.\nIn this case, we were able to reach interesting performances given the size of the model: 79.8 F1 and\n70.4 EM, i.e. within 3 points of the full model.\nSize and inference speed\nTo further investigate the speed-up/size trade-off of DistilBERT, we compare (in Table 3) the number\nof parameters of each model along with the inference time needed to do a full pass on the STS-\nB development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) using a batch size of 1.\nDistilBERT has 40% fewer parameters than BERT and is 60% faster than BERT.\nOn device computation We studied whether DistilBERT could be used for on-the-edge applications\nby building a mobile application for question answering. We compare the average inference time on\na recent smartphone (iPhone 7 Plus) against our previously trained question answering model based\non BERT-base. Excluding the tokenization step, DistilBERT is 71% faster than BERT, and the whole\nmodel weighs 207 MB (which could be further reduced with quantization). Our code is available5.\n4.2 Ablation study\nIn this section, we investigate the inﬂuence of various components of the triple loss and the student\ninitialization on the performances of the distilled model. We report the macro-score on GLUE. Table 4\npresents the deltas with the full triple loss: removing the Masked Language Modeling loss has little\nimpact while the two distillation losses account for a large portion of the performance.\n5 Related work\nTask-speciﬁc distillation Most of the prior works focus on building task-speciﬁc distillation se-\ntups. Tang et al. [2019] transfer ﬁne-tune classiﬁcation model BERT to an LSTM-based classiﬁer.\nChatterjee [2019] distill BERT model ﬁne-tuned on SQuAD in a smaller Transformer model previ-\nously initialized from BERT. In the present work, we found it beneﬁcial to use a general-purpose\npre-training distillation rather than a task-speciﬁc distillation. Turc et al. [2019] use the original\npretraining objective to train smaller student, then ﬁne-tuned via distillation. As shown in the abla-\ntion study, we found it beneﬁcial to leverage the teacher’s knowledge to pre-train with additional\ndistillation signal.\nMulti-distillation Yang et al. [2019] combine the knowledge of an ensemble of teachers using\nmulti-task learning to regularize the distillation. The authors apply Multi-Task Knowledge Distillation\nto learn a compact question answering model from a set of large question answering models. An\napplication of multi-distillation is multi-linguality: Tsai et al. [2019] adopts a similar approach to us\nby pre-training a multilingual model from scratch solely through distillation. However, as shown in\nthe ablation study, leveraging the teacher’s knowledge with initialization and additional losses leads\nto substantial gains.\nOther compression techniques have been studied to compress large models. Recent developments\nin weights pruning reveal that it is possible to remove some heads in the self-attention at test time\nwithout signiﬁcantly degrading the performance Michel et al. [2019]. Some layers can be reduced\nto one head. A separate line of study leverages quantization to derive smaller models (Gupta et al.\n[2015]). Pruning and quantization are orthogonal to the present work.\n5https://github.com/huggingface/swift-coreml-transformers\n4'
 '6 Conclusion and future work\nWe introduced DistilBERT, a general-purpose pre-trained version of BERT, 40% smaller, 60% faster,\nthat retains 97% of the language understanding capabilities. We showed that a general-purpose\nlanguage model can be successfully trained with distillation and analyzed the various components\nwith an ablation study. We further demonstrated that DistilBERT is a compelling option for edge\napplications.\nReferences\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. In NAACL-HLT , 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\nunsupervised multitask learners. 2019.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar S. Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke S. Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv ,\nabs/1907.11692, 2019.\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green ai. ArXiv , abs/1907.10597, 2019.\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in\nnlp. In ACL, 2019.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In NIPS , 2017.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. Transformers: State-of-the-art natural language\nprocessing, 2019.\nCristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In KDD , 2006.\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. ArXiv ,\nabs/1503.02531, 2015.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading\nbooks. 2015 IEEE International Conference on Computer Vision (ICCV) , pages 19–27, 2015.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A\nmulti-task benchmark and analysis platform for natural language understanding. In ICLR , 2018.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. Deep contextualized word representations. In NAACL , 2018.\nAlex Wang, Ian F. Tenney, Yada Pruksachatkun, Katherin Yu, Jan Hula, Patrick Xia, Raghu Pappagari, Shuning\nJin, R. Thomas McCoy, Roma Patel, Yinghui Huang, Jason Phang, Edouard Grave, Najoung Kim, Phu Mon\nHtut, Thibault F’evry, Berlin Chen, Nikita Nangia, Haokun Liu, Anhad Mohananey, Shikha Bordia, Nicolas\nPatry, Ellie Pavlick, and Samuel R. Bowman. jiant 1.1: A software toolkit for research on general-purpose\ntext understanding models. http://jiant.info/ , 2019.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y . Ng, and Christopher Potts. Learning\nword vectors for sentiment analysis. In ACL, 2011.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for machine\ncomprehension of text. In EMNLP , 2016.\nRaphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. Distilling task-speciﬁc\nknowledge from bert into simple neural networks. ArXiv , abs/1903.12136, 2019.\nDebajyoti Chatterjee. Making neural machine reading comprehension faster. ArXiv , abs/1904.00796, 2019.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better: The impact\nof student initialization on knowledge distillation. ArXiv , abs/1908.08962, 2019.\nZe Yang, Linjun Shou, Ming Gong, Wutao Lin, and Daxin Jiang. Model compression with multi-task knowledge\ndistillation for web-scale question answering system. ArXiv , abs/1904.09636, 2019.'
 'DistilBERT, a distilled version of BERT: smaller,\nfaster, cheaper and lighter\nVictor SANH, Lysandre DEBUT, Julien CHAUMOND, Thomas WOLF\nHugging Face\n{victor,lysandre,julien,thomas}@huggingface.co\nAbstract\nAs Transfer Learning from large-scale pre-trained models becomes more prevalent\nin Natural Language Processing (NLP), operating these large models in on-the-\nedge and/or under constrained computational training or inference budgets remains\nchallenging. In this work, we propose a method to pre-train a smaller general-\npurpose language representation model, called DistilBERT, which can then be ﬁne-\ntuned with good performances on a wide range of tasks like its larger counterparts.\nWhile most prior work investigated the use of distillation for building task-speciﬁc\nmodels, we leverage knowledge distillation during the pre-training phase and show\nthat it is possible to reduce the size of a BERT model by 40%, while retaining 97%\nof its language understanding capabilities and being 60% faster. To leverage the\ninductive biases learned by larger models during pre-training, we introduce a triple\nloss combining language modeling, distillation and cosine-distance losses. Our\nsmaller, faster and lighter model is cheaper to pre-train and we demonstrate its\ncapabilities for on-device computations in a proof-of-concept experiment and a\ncomparative on-device study.\n1 Introduction\nFigure 1: Parameter counts of several recently released\npretrained language models.The last two years have seen the rise\nof Transfer Learning approaches in\nNatural Language Processing (NLP)\nwith large-scale pre-trained language\nmodels becoming a basic tool in\nmany NLP tasks [Devlin et al., 2018,\nRadford et al., 2019, Liu et al., 2019].\nWhile these models lead to signiﬁ-\ncant improvement, they often have\nseveral hundred million parameters\nand current research1on pre-trained\nmodels indicates that training even\nlarger models still leads to better per-\nformances on downstream tasks.\nThe trend toward bigger models\nraises several concerns. First is the\nenvironmental cost of exponentially scaling these models’ computational requirements as mentioned\nin Schwartz et al. [2019], Strubell et al. [2019]. Second, while operating these models on-device\nin real-time has the potential to enable novel and interesting language processing applications, the\ngrowing computational and memory requirements of these models may hamper wide adoption.\n1See for instance the recently released MegatronLM ( https://nv-adlr.github.io/MegatronLM )\nEMC^2: 5th Edition Co-located with NeurIPS’19arXiv:1910.01108v4  [cs.CL]  1 Mar 2020']","The triple loss in DistilBERT's training combines distillation loss (Lce), masked language modeling loss (Lmlm), and cosine embedding loss (Lcos), aiming to retain performance while reducing model size.",0.9166666666361111,0.6666666666666666,0.9595348164167867,1.0,0.9360279881309697,0.9748811832931095
What advantages does DistilBERT present for on-device computations and mobile applications?,"DistilBERT is 40% smaller and 60% faster than BERT, making it more suitable for on-device computations and mobile applications. Its reduced size allows it to run more efficiently under constrained computational resources, facilitating real-time language processing. Additionally, it retains 97% of BERT's language understanding capabilities, ensuring effective performance in various NLP tasks.","['DistilBERT, a distilled version of BERT: smaller,\nfaster, cheaper and lighter\nVictor SANH, Lysandre DEBUT, Julien CHAUMOND, Thomas WOLF\nHugging Face\n{victor,lysandre,julien,thomas}@huggingface.co\nAbstract\nAs Transfer Learning from large-scale pre-trained models becomes more prevalent\nin Natural Language Processing (NLP), operating these large models in on-the-\nedge and/or under constrained computational training or inference budgets remains\nchallenging. In this work, we propose a method to pre-train a smaller general-\npurpose language representation model, called DistilBERT, which can then be ﬁne-\ntuned with good performances on a wide range of tasks like its larger counterparts.\nWhile most prior work investigated the use of distillation for building task-speciﬁc\nmodels, we leverage knowledge distillation during the pre-training phase and show\nthat it is possible to reduce the size of a BERT model by 40%, while retaining 97%\nof its language understanding capabilities and being 60% faster. To leverage the\ninductive biases learned by larger models during pre-training, we introduce a triple\nloss combining language modeling, distillation and cosine-distance losses. Our\nsmaller, faster and lighter model is cheaper to pre-train and we demonstrate its\ncapabilities for on-device computations in a proof-of-concept experiment and a\ncomparative on-device study.\n1 Introduction\nFigure 1: Parameter counts of several recently released\npretrained language models.The last two years have seen the rise\nof Transfer Learning approaches in\nNatural Language Processing (NLP)\nwith large-scale pre-trained language\nmodels becoming a basic tool in\nmany NLP tasks [Devlin et al., 2018,\nRadford et al., 2019, Liu et al., 2019].\nWhile these models lead to signiﬁ-\ncant improvement, they often have\nseveral hundred million parameters\nand current research1on pre-trained\nmodels indicates that training even\nlarger models still leads to better per-\nformances on downstream tasks.\nThe trend toward bigger models\nraises several concerns. First is the\nenvironmental cost of exponentially scaling these models’ computational requirements as mentioned\nin Schwartz et al. [2019], Strubell et al. [2019]. Second, while operating these models on-device\nin real-time has the potential to enable novel and interesting language processing applications, the\ngrowing computational and memory requirements of these models may hamper wide adoption.\n1See for instance the recently released MegatronLM ( https://nv-adlr.github.io/MegatronLM )\nEMC^2: 5th Edition Co-located with NeurIPS’19arXiv:1910.01108v4  [cs.CL]  1 Mar 2020'
 '6 Conclusion and future work\nWe introduced DistilBERT, a general-purpose pre-trained version of BERT, 40% smaller, 60% faster,\nthat retains 97% of the language understanding capabilities. We showed that a general-purpose\nlanguage model can be successfully trained with distillation and analyzed the various components\nwith an ablation study. We further demonstrated that DistilBERT is a compelling option for edge\napplications.\nReferences\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. In NAACL-HLT , 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\nunsupervised multitask learners. 2019.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar S. Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke S. Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv ,\nabs/1907.11692, 2019.\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green ai. ArXiv , abs/1907.10597, 2019.\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in\nnlp. In ACL, 2019.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In NIPS , 2017.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. Transformers: State-of-the-art natural language\nprocessing, 2019.\nCristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In KDD , 2006.\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. ArXiv ,\nabs/1503.02531, 2015.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading\nbooks. 2015 IEEE International Conference on Computer Vision (ICCV) , pages 19–27, 2015.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A\nmulti-task benchmark and analysis platform for natural language understanding. In ICLR , 2018.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. Deep contextualized word representations. In NAACL , 2018.\nAlex Wang, Ian F. Tenney, Yada Pruksachatkun, Katherin Yu, Jan Hula, Patrick Xia, Raghu Pappagari, Shuning\nJin, R. Thomas McCoy, Roma Patel, Yinghui Huang, Jason Phang, Edouard Grave, Najoung Kim, Phu Mon\nHtut, Thibault F’evry, Berlin Chen, Nikita Nangia, Haokun Liu, Anhad Mohananey, Shikha Bordia, Nicolas\nPatry, Ellie Pavlick, and Samuel R. Bowman. jiant 1.1: A software toolkit for research on general-purpose\ntext understanding models. http://jiant.info/ , 2019.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y . Ng, and Christopher Potts. Learning\nword vectors for sentiment analysis. In ACL, 2011.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for machine\ncomprehension of text. In EMNLP , 2016.\nRaphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. Distilling task-speciﬁc\nknowledge from bert into simple neural networks. ArXiv , abs/1903.12136, 2019.\nDebajyoti Chatterjee. Making neural machine reading comprehension faster. ArXiv , abs/1904.00796, 2019.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better: The impact\nof student initialization on knowledge distillation. ArXiv , abs/1908.08962, 2019.\nZe Yang, Linjun Shou, Ming Gong, Wutao Lin, and Daxin Jiang. Model compression with multi-task knowledge\ndistillation for web-scale question answering system. ArXiv , abs/1904.09636, 2019.'
 'Table 1: DistilBERT retains 97% of BERT performance. Comparison on the dev sets of the\nGLUE benchmark. ELMo results as reported by the authors. BERT and DistilBERT results are the\nmedians of 5 runs with different seeds.\nModel Score CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B WNLI\nELMo 68.7 44.1 68.6 76.6 71.1 86.2 53.4 91.5 70.4 56.3\nBERT-base 79.5 56.3 86.7 88.6 91.8 89.6 69.3 92.7 89.0 53.5\nDistilBERT 77.0 51.3 82.2 87.5 89.2 88.5 59.9 91.3 86.9 56.3\nTable 2: DistilBERT yields to comparable\nperformance on downstream tasks. Com-\nparison on downstream tasks: IMDb (test ac-\ncuracy) and SQuAD 1.1 (EM/F1 on dev set).\nD: with a second step of distillation during\nﬁne-tuning.\nModel IMDb SQuAD\n(acc.) (EM/F1)\nBERT-base 93.46 81.2/88.5\nDistilBERT 92.82 77.7/85.8\nDistilBERT (D) - 79.1/86.9Table 3: DistilBERT is signiﬁcantly smaller\nwhile being constantly faster. Inference\ntime of a full pass of GLUE task STS-B (sen-\ntiment analysis) on CPU with a batch size of\n1.\nModel # param. Inf. time\n(Millions) (seconds)\nELMo 180 895\nBERT-base 110 668\nDistilBERT 66 410\nDistillation We applied best practices for training BERT model recently proposed in Liu et al. [2019].\nAs such, DistilBERT is distilled on very large batches leveraging gradient accumulation (up to 4K\nexamples per batch) using dynamic masking and without the next sentence prediction objective.\nData and compute power We train DistilBERT on the same corpus as the original BERT model:\na concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015]. DistilBERT\nwas trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the\nRoBERTa model [Liu et al., 2019] required 1 day of training on 1024 32GB V100.\n4 Experiments\nGeneral Language Understanding We assess the language understanding and generalization ca-\npabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark\n[Wang et al., 2018], a collection of 9 datasets for evaluating natural language understanding systems.\nWe report scores on the development sets for each task by ﬁne-tuning DistilBERT without the use\nof ensembling or multi-tasking scheme for ﬁne-tuning (which are mostly orthogonal to the present\nwork). We compare the results to the baseline provided by the authors of GLUE: an ELMo (Peters\net al. [2018]) encoder followed by two BiLSTMs.4\nThe results on each of the 9 tasks are showed on Table 1 along with the macro-score (average of\nindividual scores). Among the 9 tasks, DistilBERT is always on par or improving over the ELMo\nbaseline (up to 19 points of accuracy on STS-B). DistilBERT also compares surprisingly well to\nBERT, retaining 97% of the performance with 40% fewer parameters.\n4.1 Downstream task benchmark\nDownstream tasks We further study the performances of DistilBERT on several downstream tasks\nunder efﬁcient inference constraints: a classiﬁcation task (IMDb sentiment classiﬁcation - Maas et al.\n[2011]) and a question answering task (SQuAD v1.1 - Rajpurkar et al. [2016]).\nAs shown in Table 2, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb\nbenchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.9 points of the full BERT.\nWe also studied whether we could add another step of distillation during the adaptation phase by\nﬁne-tuning DistilBERT on SQuAD using a BERT model previously ﬁne-tuned on SQuAD as a\n4We use jiant [Wang et al., 2019] to compute the baseline.\n3'
 'In this paper, we show that it is possible to reach similar performances on many downstream-tasks\nusing much smaller language models pre-trained with knowledge distillation, resulting in models\nthat are lighter and faster at inference time, while also requiring a smaller computational training\nbudget. Our general-purpose pre-trained models can be ﬁne-tuned with good performances on several\ndownstream tasks, keeping the ﬂexibility of larger models. We also show that our compressed models\nare small enough to run on the edge, e.g. on mobile devices.\nUsing a triple loss, we show that a 40% smaller Transformer (Vaswani et al. [2017]) pre-trained\nthrough distillation via the supervision of a bigger Transformer language model can achieve similar\nperformance on a variety of downstream tasks, while being 60% faster at inference time. Further\nablation studies indicate that all the components of the triple loss are important for best performances.\nWe have made the trained weights available along with the training code in the Transformers2\nlibrary from HuggingFace [Wolf et al., 2019].\n2 Knowledge distillation\nKnowledge distillation [Bucila et al., 2006, Hinton et al., 2015] is a compression technique in which\na compact model - the student - is trained to reproduce the behaviour of a larger model - the teacher -\nor an ensemble of models.\nIn supervised learning, a classiﬁcation model is generally trained to predict an instance class by\nmaximizing the estimated probability of gold labels. A standard training objective thus involves\nminimizing the cross-entropy between the model’s predicted distribution and the one-hot empirical\ndistribution of training labels. A model performing well on the training set will predict an output\ndistribution with high probability on the correct class and with near-zero probabilities on other\nclasses. But some of these ""near-zero"" probabilities are larger than others and reﬂect, in part, the\ngeneralization capabilities of the model and how well it will perform on the test set3.\nTraining loss The student is trained with a distillation loss over the soft target probabilities of\nthe teacher: Lce=∑\niti∗log(si)where ti(resp. si) is a probability estimated by the teacher\n(resp. the student). This objective results in a rich training signal by leveraging the full teacher\ndistribution. Following Hinton et al. [2015] we used a softmax-temperature :pi=exp(zi/T)∑\njexp(zj/T)\nwhere Tcontrols the smoothness of the output distribution and ziis the model score for the class i.\nThe same temperature Tis applied to the student and the teacher at training time, while at inference,\nTis set to 1 to recover a standard softmax .\nThe ﬁnal training objective is a linear combination of the distillation loss Lcewith the supervised\ntraining loss, in our case the masked language modeling lossLmlm [Devlin et al., 2018]. We found it\nbeneﬁcial to add a cosine embedding loss (Lcos) which will tend to align the directions of the student\nand teacher hidden states vectors.\n3 DistilBERT: a distilled version of BERT\nStudent architecture In the present work, the student - DistilBERT - has the same general architec-\nture as BERT. The token-type embeddings and the pooler are removed while the number of layers\nis reduced by a factor of 2. Most of the operations used in the Transformer architecture ( linear\nlayer andlayer normalisation ) are highly optimized in modern linear algebra frameworks and our\ninvestigations showed that variations on the last dimension of the tensor (hidden size dimension) have\na smaller impact on computation efﬁciency (for a ﬁxed parameters budget) than variations on other\nfactors like the number of layers. Thus we focus on reducing the number of layers.\nStudent initialization In addition to the previously described optimization and architectural choices,\nan important element in our training procedure is to ﬁnd the right initialization for the sub-network to']","DistilBERT's reduced size and faster inference capabilities make it highly suitable for on-device computations, demonstrated through a mobile application for question answering.",0.999999999975,1.0,0.9147512911799011,0.5,0.9854730983136456,0.9418527488331213
"In what ways does HellaSwag expand upon its predecessor, SWAG, to offer a more rigorous test of AI commonsense reasoning?","HellaSwag expands upon SWAG by utilizing Adversarial Filtering to create a more challenging dataset, where the generated wrong answers are specifically designed to be nonsensical yet difficult for models to classify correctly. It increases the complexity and length of the context provided, aiming for a 'Goldilocks zone' that is easy for humans to understand but perplexing for AI models. Additionally, it incorporates diverse sources like WikiHow articles, enhancing context diversity compared to the original SWAG dataset.","['lems of reporting bias (Gordon and Van Durme,\n2013). Alternatively, future models might answer\ncorrectly only by picking up on spurious patterns,\nin which case a new development of the bench-\nmark – using these models as adversaries – would\nplace us in the same position as we are right now.\nPut another way, for humans to answer Hella-\nSwag questions requires abstracting away from\nlanguage and modeling world states instead. We\npostulate that this is what separates solving the\ntask of commonsense NLI, as opposed to a par-\nticular dataset. Indeed, we ﬁnd that existing deep\nmethods often get fooled by lexical false friends.\nFor example, in the WikiHow example from Fig-\nure 10, BERT chooses an ending that matches\nthetechnology words in the context, rather than\nmatching the deeper topic: using technology as an\nexcuse for not doing homework.\n6.3 Towards a future of evolving benchmarks\nWhat happens when HellaSwag gets solved? We\nbelieve the answer is simple: crowdsource another\ndataset, with the same exact format, and see where\nmodels fail. Indeed, in our work we found this to\nbe straightforward from an algorithmic perspec-\ntive: by throwing in the best known generator\n(GPT) and the best known discriminator (BERT-\nLarge), we made a dataset that is adversarial - not\njust to BERT, but to all models we have access to.\nWhile this was easy algorithmically, care must\nbe taken from a data curation standpoint. Indeed,\nwe ﬁnd success exists within a Goldilocks zone:\nthe data source must be complex enough that state-\nof-the-art generators often make mistakes, while\nsimple enough such that discriminators often fail\nto catch them. This ties the future of SWAG-\nstyle benchmarks to progress on language gener-\nation: until generation is solved, commonsense\nNLI will remain unsolved. Even recent promis-\ning results on scaling up language models (Rad-\nford et al., 2019) ﬁnd problems in terms of consis-\ntency, with the best curated examples requiring 25\nrandom seeds.\n7 Conclusion\nIn this paper, we presented HellaSwag , a new\ndataset for physically situated commonsense rea-\nsoning. By constructing the dataset through ad-\nversarial ﬁltering, combined with state-of-the-art\nmodels for language generation and discrimina-\ntion, we produced a dataset that is adversarial tothe most robust models available – even when\nmodels are evaluated on items from the train-\ning distribution. In turn, we provided insight\ninto the inner workings of pretrained models, and\nsuggest a path for NLP progress going forward:\ntowards benchmarks that adversarially co-evolve\nwith evolving state-of-the-art models.\nAcknowledgments\nWe thank the reviewers, as well as Jesse Thoma-\nson, for their helpful feedback. We thank the\nMechanical Turk workers for their great work\nduring dataset collection. Thanks also to Zak\nStone and the Google Cloud TPU team for help\nwith the computing infrastructure. This work\nwas supported by the National Science Foundation\nthrough a Graduate Research Fellowship (DGE-\n1256082) and NSF grants (IIS-1524371, 1637479,\n165205, 1703166), the DARPA CwC program\nthrough ARO (W911NF-15-1-0543), the IARPA\nDIV A program through D17PC00343, the Sloan\nResearch Foundation through a Sloan Fellowship,\nthe Allen Institute for Artiﬁcial Intelligence, the\nNVIDIA Artiﬁcial Intelligence Lab, and gifts by\nGoogle and Facebook. The views and conclu-\nsions contained herein are those of the authors and\nshould not be interpreted as representing endorse-\nments of IARPA, DOI /IBC, or the U.S. Govern-\nment.\nReferences\nYonatan Belinkov and Yonatan Bisk. 2018. Synthetic\nand natural noise both break neural machine transla-\ntion. In ICLR . ICLR.\nQian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui\nJiang, and Diana Inkpen. 2017. Enhanced lstm for\nnatural language inference. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers) , vol-\nume 1, pages 1657–1668.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and'
 ""HellaSwag : Can a Machine Really Finish Your Sentence?\nRowan Zellers♠Ari Holtzman♠Yonatan Bisk♠Ali Farhadi♠♥Yejin Choi♠♥\n♠Paul G. Allen School of Computer Science & Engineering, University of Washington\n♥Allen Institute for Artiﬁcial Intelligence\nhttps://rowanzellers.com/hellaswag\nAbstract\nRecent work by Zellers et al. (2018) intro-\nduced a new task of commonsense natural lan-\nguage inference : given an event description\nsuch as “A woman sits at a piano,” a machine\nmust select the most likely followup: “She\nsets her ﬁngers on the keys.” With the intro-\nduction of BERT (Devlin et al., 2018), near\nhuman-level performance was reached. Does\nthis mean that machines can perform human\nlevel commonsense inference?\nIn this paper, we show that commonsense in-\nference still proves di ﬃcult for even state-\nof-the-art models, by presenting HellaSwag ,\na new challenge dataset. Though its ques-\ntions are trivial for humans ( ą95% accuracy),\nstate-of-the-art models struggle ( ă48%). We\nachieve this via Adversarial Filtering (AF), a\ndata collection paradigm wherein a series of\ndiscriminators iteratively select an adversarial\nset of machine-generated wrong answers. AF\nproves to be surprisingly robust. The key in-\nsight is to scale up the length and complex-\nity of the dataset examples towards a critical\n‘Goldilocks’ zone wherein generated text is\nridiculous to humans, yet often misclassiﬁed\nby state-of-the-art models.\nOur construction of HellaSwag , and its result-\ning di ﬃculty, sheds light on the inner work-\nings of deep pretrained models. More broadly,\nit suggests a new path forward for NLP re-\nsearch, in which benchmarks co-evolve with\nthe evolving state-of-the-art in an adversarial\nway, so as to present ever-harder challenges.\n1 Introduction\nImagine a woman chasing a dog around outside,\ntrying to give it a bath. What might happen next?\nHumans can read a narrative like this, shown in\nFigure 1, and connect it to a rich model of the\nworld: the dog is currently dry and not soapy, and\nit actively doesn’t want to be bathed. Thus, one\nA woman is outside with a bucket and a dog. The dog is running around trying to avoid a bath. She…A. rinses the bucket off with soap and blow dry the dog’s head.B. uses a hose to keep it from getting soapy.C. gets the dog wet, then it runs away again.D. gets into a bath tub with the dog.Come to a complete halt at a stop sign or red light. At a stop sign, come to a complete halt for about 2 seconds or until vehicles that arrived before you clear the intersection. If you're stopped at a red light, proceed when the light has turned green. …A. Stop for no more than two seconds, or until the light turns yellow. A red light in front of you indicates that you should stop.B. After you come to a complete stop, turn off your turn signal. Allow vehicles to move in different directions before moving onto the sidewalk.C. Stay out of the oncoming traﬃc. People coming in from behind may elect to stay left or right.D. If the intersection has a white stripe in your lane, stop before this line. Wait until all traﬃc has cleared before crossing the intersection.\nOpenAIGPT\nHow to determine who has right of way. \neasy!???+\nAdversarial Filtering+\nAdversarial FilteringFigure 1: Models like BERT struggle to ﬁnish the sen-\ntences in HellaSwag , even when they come from the\nsame distribution as the training set. While the wrong\nendings are on-topic, with words that relate to the con-\ntext, humans consistently judge their meanings to be\neither incorrect or implausible. For example, option A\nof the WikiHow passage suggests that a driver should\nstop at a red light for no more than two seconds .\nplausible next event is option C—that she’ll get\nthe dog wet and it will run away again.\nWhen the SWAG dataset was ﬁrst announced\n(Zellers et al., 2018), this new task of common-\nsense natural language inference seemed trivial\nfor humans (88%) and yet challenging for then-\nstate-of-the-art models ( ă60%), including ELMo""
 'BERT, perform at commonsense natural language\ninference (NLI)? Our surprising conclusion is\nthat the underlying task remains unsolved. In-\ndeed, we ﬁnd that deep models such as BERT do\nnot demonstrate robust commonsense reasonining\nability by themselves. Instead, they operate more\nlikerapid surface learners for a particular dataset.\nTheir strong performance on SWAG is dependent\non the ﬁnetuning process, wherein they largely\nlearn to pick up on dataset-speciﬁc distributional\nbiases. When the distribution of language shifts\nslightly, performance drops drastically – even if\nthe domain remains identical.\nWe study this question by introducing Hella-\nSwag ,2a new benchmark for commonsense\nNLI. We use Adversarial Filtering (AF), a data-\ncollection paradigm in which a series of discrim-\ninators is used to select a challenging set of gen-\nerated wrong answers. AF is surprisingly e ﬀec-\ntive towards this goal: the resulting dataset of 70k\nproblems is easy for humans (95.6% accuracy),\nyet challenging for machines ( ă50%q. This result\nholds even when models are given a signiﬁcant\nnumber of training examples, and even when the\ntest data comes from the exact same distribution\nas the training data. Machine performance slips\nan additional 5% when evaluated on examples that\ncover novel concepts from the same domain.\nTo make this dataset robust to deep pre-\ntrained models, we use a trifecta of state-of-the-\nart generators (Radford et al., 2018), state-of-\nthe-art discriminators (BERT), and high quality\nsource text. We expand on the SWAG’s origi-\nnal video-captioning domain by using WikiHow\narticles, greatly increasing the context diversity\nand generation length. Our investigation reveals\na Goldilocks zone – roughly three sentences of\ncontext, and two generated sentences – wherein\ngenerations are largely nonsensical, even though\nstate-of-the-art discriminators cannot reliably tell\nthe di ﬀerence between these generations and the\nground truth.\nMore broadly, our paper presents a case-study\ntowards a future of veriﬁed progress in NLP, via it-\nerative rounds of building and breaking datasets. If\nour ultimate goal is to provide reliable benchmarks\nfor challenging tasks, such as commonsense NLI,\nthese benchmarks cannot be static. Instead, they\nmust evolve together with the evolving state-of-\n2Short for Harder Endings, Longer contexts, and Low-\nshot Activities for Situations WithAdversarial Generations.\nDataset and code at https: //rowanzellers.com /hellaswag.\nContext 2Context 1\nContext N…\nContext 1Context M…Real ending…Real ending(N instances)(M instances)DtrainReal ending…Real endingReal ending\nGen’d ending K Gen’d ending K Gen’d ending K ……………Gen’d ending2 …Gen’d ending2 Gen’d ending2 Gen’d ending 1 …Gen’d ending 1 Gen’d ending 1 DtestGen’d ending2 …Gen’d ending 1…………Gen’d ending 2Gen’d ending 1 Gen’d ending K Gen’d ending K …\nf\nTrain f to discriminate real vs. generated\nReplace easily-classiﬁed generations with adversarial ones that currently aren’t includedGenerated Ending (context M)Generated Ending (context 2)New!New!Figure 2: An overview of Adversarial Filtering. On\neach iteration, a new classiﬁer is trained on a dummy\ntraining set Dtrainto replace easily-classiﬁed negative\nendings on the dummy test set Dtestwith adversarial\nendings. This process is repeated iteratively, to obtain\na challenging dataset regardless of the ﬁnal split.\nthe-art. Continued evolution in turn requires prin-\ncipled dataset creation algorithms. Whenever a\nnew iteration of a dataset is created, these algo-\nrithms must leverage existing modeling advance-\nments to ﬁlter out spurious biases. Only once this\ncycle becomes impossible can we say that the un-\nderlying task – as opposed an individual dataset –\nis solved.\n2 Background\nSWAG is a dataset for commonsense NLI. For\neach question, a model is given a context from a\nvideo caption and four ending choices for what\nmight happen next. Only one choice is right – the\nactual next caption of the video.'
 'Stylistic\nEnsembleELMo+\nLSTMGPT BERTBase BERTLarge30405060708090100 Accuracy (%)\n48.2%53.7%64.8%71.4%83.0%\n28.0% 28.2% 28.4%32.0%41.1%78.5%77.4%\n71.3%\n63.0%\n41.1%Accuracy of the filtering model before AF\nAccuracy of the filtering model after AF\nBERT-Large accuracy after AFFigure 11: Performance on the WikiHow subset of al-\nternative variations of HellaSwag , where di ﬀerent Ad-\nversarial Filters are used (but without human valida-\ntion). We consider the shallow stylistic adversaries\nused by Zellers et al. (2018) (Stylistic Ensemble),\nas well as an LSTM with ELMo embeddings, GPT,\nBERT-Base, and BERT-Large. For each adversarial ﬁl-\ntering model, we record the accuracy of that model be-\nfore and after AF is used. We also evaluate each al-\nternative dataset using BERT-Large. The results sug-\ngest that using a a stronger model at test time (over the\nmodel used for AF) improves performance, but is not\nenough to solve the task.\n6 Discussion\nOur results suggest that HellaSwag is a challenging\ntestbed for state-of-the-art NLI models, even those\nbuilt on extensive pretraining. The question still\nremains, though, of where will the ﬁeld go next?\n6.1 How easy might HellaSwag be for future\ndiscriminators?\nIn this paper, we showed the existence of a\nGoldilocks zone of text complexity – in which\ngenerations are nonsensical, but existing state-\nof-the-art NLP models cannot tell the di ﬀerence.\nHow hard will the dataset be for future, even more\npowerful, models?\nAnswering this question is challenging because\nthese models don’t exist (or are unavailable) at\nthe time of writing . However, one remedy is to\nperform an ablation study on the Adversarial Fil-\ntering model used, comparing weaker ﬁlters with\nstronger discriminators. We present our results\nin Figure 11, and ﬁnd that while weak discrim-\ninators (like the stylistic ensemble used to make\nSWAG) only marginally reduce the accuracy of\nBERT-Large, increasing the gap between the ﬁlter\nand the ﬁnal discriminator is not enough to solve\nthe task. For instance, using a discriminator with\n3x the parameters as the adversarial ﬁlter (BERT-\nLarge vs. BERT-Base) results in 63% machine ac-\ncuracy.\n30 40 50 60 70 80 90 100\nOverall Accuracy on HellaSwag1021041061081010Pretraining Hours (Estimate)\nHuman performance?\nELMoGPTBERT-BaseBERT-LargeFigure 12: Estimated pretraining hours required to\nreach a desired accuracy on HellaSwag . We estimate\nperfomance with respect to a RTX 2080 Ti - a modern,\nfast GPU, and ﬁt a log-linear regression line. An ex-\ntrapolation suggests that to reach human-level perfor-\nmance on HellaSwag , without algorithmic or computa-\ntional improvements, would require 109GPU-hours of\npretraining (over 100k GPU years).\n6.2 How well does pretraining scale?\nOverall, the current paradigm of pretraining large\nmodels on lots of data has made immense progress\non NLP benchmarks. Though we expect this\ntrend to continue, it also behooves us to con-\nsider its limits. If more compute is indeed the\nanswer for human-level commonsense inference,\nwhat would the compute requirements of this hy-\npothetical massive model look like?\nWe investigate this in Figure 12 by compar-\ning the accuracies of known models on Hella-\nSwag with their computational needs. This estima-\ntion is a rough estimate: we convert reported TPU\nruntimes to our benchmark RTX 2080 Ti GPU us-\ning the Rooﬂine model (Williams et al., 2009),\nwhich focuses primarily on the bottleneck of load-\ning tensors into GPU memory. Extrapolating from\nan exponential ﬁt suggests that reaching human-\nlevel performance on our dataset would require\n109GPU hours, or 100k years – unless algorith-\nmic improvements are made.\nWhat might these algorithmic improvements\nlook like? These could include architectural ad-\nvances, better pretraining objectives, and beyond.\nHowever, these improvements share the bottle-\nneck of the data source. To answer some Hella-\nSwag questions correctly without reasoning deeply\n– like knowing that it is a bad idea to stop at a']","HellaSwag expands on SWAG by incorporating longer and more diverse contexts from WikiHow articles, introducing more complex and varied scenarios for testing commonsense reasoning.",0.999999999975,0.8888888888888888,0.8937634021811521,1.0,0.45626344428930277,0.901976854080288
How does RoBERTa's use of a byte-level BPE (Byte Pair Encoding) vocabulary contribute to its model architecture and performance?,"RoBERTa's use of a byte-level BPE vocabulary allows it to handle large and diverse corpora more effectively by encoding any input text without introducing ""unknown"" tokens. This approach results in a modest vocabulary size of 50K subword units, which balances performance and flexibility. Although early experiments showed slight differences in end-task performance compared to character-level BPE, the universal encoding scheme is considered advantageous for its robustness.","['bsz steps lr ppl MNLI-m SST-2\n256 1M 1e-4 3.99 84.7 92.7\n2K 125K 7e-4 3.68 85.2 92.9\n8K 31K 1e-3 3.77 84.6 92.8\nTable 3: Perplexity on held-out training data ( ppl) and\ndevelopment set accuracy for base models trained over\nBOOK CORPUS and W IKIPEDIA with varying batch\nsizes ( bsz). We tune the learning rate ( lr) for each set-\nting. Models make the same number of passes over the\ndata (epochs) and have the same computational cost.\ntask performance of BERT BASE as we increase the\nbatch size, controlling for the number of passes\nthrough the training data. We observe that train-\ning with large batches improves perplexity for the\nmasked language modeling objective, as well as\nend-task accuracy. Large batches are also easier to\nparallelize via distributed data parallel training,8\nand in later experiments we train with batches of\n8K sequences.\nNotably You et al. (2019 ) train BERT with even\nlarger batche sizes, up to 32K sequences. We leave\nfurther exploration of the limits of large batch\ntraining to future work.\n4.4 Text Encoding\nByte-Pair Encoding (BPE) ( Sennrich et al. ,2016 )\nis a hybrid between character- and word-level rep-\nresentations that allows handling the large vocab-\nularies common in natural language corpora. In-\nstead of full words, BPE relies on subwords units,\nwhich are extracted by performing statistical anal-\nysis of the training corpus.\nBPE vocabulary sizes typically range from\n10K-100K subword units. However, unicode char-\nacters can account for a sizeable portion of this\nvocabulary when modeling large and diverse cor-\npora, such as the ones considered in this work.\nRadford et al. (2019 ) introduce a clever imple-\nmentation of BPE that uses bytes instead of uni-\ncode characters as the base subword units. Using\nbytes makes it possible to learn a subword vocab-\nulary of a modest size (50K units) that can still en-\ncode any input text without introducing any “un-\nknown” tokens.\n8Large batch training can improve training efﬁciency even\nwithout large scale parallel hardware through gradient ac-\ncumulation , whereby gradients from multiple mini-batches\nare accumulated locally before each optimization step. Thi s\nfunctionality is supported natively in FAIRSEQ (Ott et al. ,\n2019 ).The original BERT implementa-\ntion ( Devlin et al. ,2019 ) uses a character-level\nBPE vocabulary of size 30K, which is learned\nafter preprocessing the input with heuristic tok-\nenization rules. Following Radford et al. (2019 ),\nwe instead consider training BERT with a larger\nbyte-level BPE vocabulary containing 50K sub-\nword units, without any additional preprocessing\nor tokenization of the input. This adds approxi-\nmately 15M and 20M additional parameters for\nBERT BASEand BERT LARGE , respectively.\nEarly experiments revealed only slight dif-\nferences between these encodings, with the\nRadford et al. (2019 ) BPE achieving slightly\nworse end-task performance on some tasks. Nev-\nertheless, we believe the advantages of a univer-\nsal encoding scheme outweighs the minor degre-\ndation in performance and use this encoding in\nthe remainder of our experiments. A more de-\ntailed comparison of these encodings is left to fu-\nture work.\n5 RoBERTa\nIn the previous section we propose modiﬁcations\nto the BERT pretraining procedure that improve\nend-task performance. We now aggregate these\nimprovements and evaluate their combined im-\npact. We call this conﬁguration RoBERTa for\nRobustly optimized BERT approach. Speciﬁ-\ncally, RoBERTa is trained with dynamic mask-\ning (Section 4.1),FULL -SENTENCES without NSP\nloss (Section 4.2), large mini-batches (Section 4.3)\nand a larger byte-level BPE (Section 4.4).\nAdditionally, we investigate two other impor-\ntant factors that have been under-emphasized in\nprevious work: (1) the data used for pretraining,\nand (2) the number of training passes through the\ndata. For example, the recently proposed XLNet\narchitecture ( Yang et al. ,2019 ) is pretrained us-\ning nearly 10 times more data than the original'
 'and (2) the number of training passes through the\ndata. For example, the recently proposed XLNet\narchitecture ( Yang et al. ,2019 ) is pretrained us-\ning nearly 10 times more data than the original\nBERT ( Devlin et al. ,2019 ). It is also trained with\na batch size eight times larger for half as many op-\ntimization steps, thus seeing four times as many\nsequences in pretraining compared to BERT.\nTo help disentangle the importance of these fac-\ntors from other modeling choices (e.g., the pre-\ntraining objective), we begin by training RoBERTa\nfollowing the BERT LARGE architecture ( L= 24 ,\nH= 1024 ,A= 16 , 355M parameters). We\npretrain for 100K steps over a comparable B OOK -\nCORPUS plus W IKIPEDIA dataset as was used in'
 'alternatives that lead to better downstream task\nperformance; (2) We use a novel dataset, CC-\nNEWS, and conﬁrm that using more data for pre-\ntraining further improves performance on down-\nstream tasks; (3) Our training improvements show\nthat masked language model pretraining, under\nthe right design choices, is competitive with all\nother recently published methods. We release our\nmodel, pretraining and ﬁne-tuning code imple-\nmented in PyTorch ( Paszke et al. ,2017 ).\n2 Background\nIn this section, we give a brief overview of the\nBERT ( Devlin et al. ,2019 ) pretraining approach\nand some of the training choices that we will ex-\namine experimentally in the following section.\n2.1 Setup\nBERT takes as input a concatenation of two\nsegments (sequences of tokens), x1,...,x N\nandy1,...,yM. Segments usually consist of\nmore than one natural sentence. The two seg-\nments are presented as a single input sequence\nto BERT with special tokens delimiting them:\n[CLS],x1,...,x N,[SEP],y1,...,yM,[EOS].\nMandNare constrained such that M+N < T ,\nwhereTis a parameter that controls the maximum\nsequence length during training.\nThe model is ﬁrst pretrained on a large unla-\nbeled text corpus and subsequently ﬁnetuned us-\ning end-task labeled data.\n2.2 Architecture\nBERT uses the now ubiquitous transformer archi-\ntecture ( Vaswani et al. ,2017 ), which we will not\nreview in detail. We use a transformer architecture\nwithLlayers. Each block uses Aself-attention\nheads and hidden dimension H.\n2.3 Training Objectives\nDuring pretraining, BERT uses two objectives:\nmasked language modeling and next sentence pre-\ndiction.\nMasked Language Model (MLM) A random\nsample of the tokens in the input sequence is\nselected and replaced with the special token\n[MASK]. The MLM objective is a cross-entropy\nloss on predicting the masked tokens. BERT uni-\nformly selects 15% of the input tokens for possi-\nble replacement. Of the selected tokens, 80% are\nreplaced with [MASK], 10% are left unchanged,and 10% are replaced by a randomly selected vo-\ncabulary token.\nIn the original implementation, random mask-\ning and replacement is performed once in the be-\nginning and saved for the duration of training, al-\nthough in practice, data is duplicated so the mask\nis not always the same for every training sentence\n(see Section 4.1).\nNext Sentence Prediction (NSP) NSP is a bi-\nnary classiﬁcation loss for predicting whether two\nsegments follow each other in the original text.\nPositive examples are created by taking consecu-\ntive sentences from the text corpus. Negative ex-\namples are created by pairing segments from dif-\nferent documents. Positive and negative examples\nare sampled with equal probability.\nThe NSP objective was designed to improve\nperformance on downstream tasks, such as Natural\nLanguage Inference ( Bowman et al. ,2015 ), which\nrequire reasoning about the relationships between\npairs of sentences.\n2.4 Optimization\nBERT is optimized with Adam ( Kingma and Ba ,\n2015 ) using the following parameters: β1= 0.9,\nβ2= 0.999,ǫ=1e-6 and L2weight de-\ncay of0.01. The learning rate is warmed up\nover the ﬁrst 10,000 steps to a peak value of\n1e-4, and then linearly decayed. BERT trains\nwith a dropout of 0.1 on all layers and at-\ntention weights, and a GELU activation func-\ntion ( Hendrycks and Gimpel ,2016 ). Models are\npretrained for S=1,000,000 updates, with mini-\nbatches containing B=256 sequences of maxi-\nmum length T=512 tokens.\n2.5 Data\nBERT is trained on a combination of B OOK COR-\nPUS (Zhu et al. ,2015 ) plus English W IKIPEDIA ,\nwhich totals 16GB of uncompressed text.3\n3 Experimental Setup\nIn this section, we describe the experimental setup\nfor our replication study of BERT.\n3.1 Implementation\nWe reimplement BERT in FAIRSEQ (Ott et al. ,\n2019 ). We primarily follow the original BERT\n3Yang et al. (2019 ) use the same dataset but report having\nonly 13GB of text after data cleaning. This is most likely due\nto subtle differences in cleaning of the Wikipedia data.'
 'is (L=64, H=512, A=2) with 235M parameters\n(Al-Rfou et al., 2018). By contrast, BERT BASE\ncontains 110M parameters and BERT LARGE con-\ntains 340M parameters.\nIt has long been known that increasing the\nmodel size will lead to continual improvements\non large-scale tasks such as machine translation\nand language modeling, which is demonstrated\nby the LM perplexity of held-out training data\nshown in Table 6. However, we believe that\nthis is the ﬁrst work to demonstrate convinc-\ningly that scaling to extreme model sizes also\nleads to large improvements on very small scale\ntasks, provided that the model has been sufﬁ-\nciently pre-trained. Peters et al. (2018b) presented']","RoBERTa adopts a byte-level BPE vocabulary, allowing for more efficient handling of large and diverse corpora without the need for additional preprocessing or tokenization. This encoding scheme supports a modest-sized vocabulary that can encode any input text, potentially enhancing the model's linguistic flexibility and performance on various NLP tasks.",0.9999999999,0.8571428571428571,0.951346522741643,1.0,0.8599989652934441,0.9694076258796587
Discuss RoBERTa's contributions to the understanding of effective pretraining strategies in NLP.,"RoBERTa contributes to the understanding of effective pretraining strategies in NLP by demonstrating that BERT was significantly undertrained and proposing modifications to its training process. Key changes include training the model longer with larger batches, removing the next sentence prediction objective, and utilizing longer sequences and dynamic masking patterns. These adjustments lead to RoBERTa achieving state-of-the-art results on various benchmarks, emphasizing the importance of hyperparameter tuning and the size of training data in model performance.","['arXiv:1907.11692v1  [cs.CL]  26 Jul 2019RoBERTa: A Robustly Optimized BERT Pretraining Approach\nYinhan Liu∗§Myle Ott∗§Naman Goyal∗§Jingfei Du∗§Mandar Joshi†\nDanqi Chen§Omer Levy§Mike Lewis§Luke Zettlemoyer†§Veselin Stoyanov§\n†Paul G. Allen School of Computer Science & Engineering,\nUniversity of Washington, Seattle, WA\n{mandar90,lsz }@cs.washington.edu\n§Facebook AI\n{yinhanliu,myleott,naman,jingfeidu,\ndanqi,omerlevy,mikelewis,lsz,ves }@fb.com\nAbstract\nLanguage model pretraining has led to sig-\nniﬁcant performance gains but careful com-\nparison between different approaches is chal-\nlenging. Training is computationally expen-\nsive, often done on private datasets of different\nsizes, and, as we will show, hyperparameter\nchoices have signiﬁcant impact on the ﬁnal re-\nsults. We present a replication study of BERT\npretraining ( Devlin et al. ,2019 ) that carefully\nmeasures the impact of many key hyperparam-\neters and training data size. We ﬁnd that BERT\nwas signiﬁcantly undertrained, and can match\nor exceed the performance of every model\npublished after it. Our best model achieves\nstate-of-the-art results on GLUE, RACE and\nSQuAD. These results highlight the impor-\ntance of previously overlooked design choices,\nand raise questions about the source of re-\ncently reported improvements. We release our\nmodels and code.1\n1 Introduction\nSelf-training methods such as ELMo ( Peters et al. ,\n2018 ), GPT ( Radford et al. ,2018 ), BERT\n(Devlin et al. ,2019 ), XLM ( Lample and Conneau ,\n2019 ), and XLNet ( Yang et al. ,2019 ) have\nbrought signiﬁcant performance gains, but it can\nbe challenging to determine which aspects of\nthe methods contribute the most. Training is\ncomputationally expensive, limiting the amount\nof tuning that can be done, and is often done with\nprivate training data of varying sizes, limiting\nour ability to measure the effects of the modeling\nadvances.\n∗Equal contribution.\n1Our models and code are available at:\nhttps://github.com/pytorch/fairseqWe present a replication study of BERT pre-\ntraining ( Devlin et al. ,2019 ), which includes a\ncareful evaluation of the effects of hyperparmeter\ntuning and training set size. We ﬁnd that BERT\nwas signiﬁcantly undertrained and propose an im-\nproved recipe for training BERT models, which\nwe call RoBERTa, that can match or exceed the\nperformance of all of the post-BERT methods.\nOur modiﬁcations are simple, they include: (1)\ntraining the model longer, with bigger batches,\nover more data; (2) removing the next sentence\nprediction objective; (3) training on longer se-\nquences; and (4) dynamically changing the mask-\ning pattern applied to the training data. We also\ncollect a large new dataset (CC-N EWS) of compa-\nrable size to other privately used datasets, to better\ncontrol for training set size effects.\nWhen controlling for training data, our im-\nproved training procedure improves upon the pub-\nlished BERT results on both GLUE and SQuAD.\nWhen trained for longer over additional data, our\nmodel achieves a score of 88.5 on the public\nGLUE leaderboard, matching the 88.4 reported\nbyYang et al. (2019 ). Our model establishes a\nnew state-of-the-art on 4/9 of the GLUE tasks:\nMNLI, QNLI, RTE and STS-B. We also match\nstate-of-the-art results on SQuAD and RACE.\nOverall, we re-establish that BERT’s masked lan-\nguage model training objective is competitive\nwith other recently proposed training objectives\nsuch as perturbed autoregressive language model-\ning (Yang et al. ,2019 ).2\nIn summary, the contributions of this paper\nare: (1) We present a set of important BERT de-\nsign choices and training strategies and introduce\n2It is possible that these other methods could also improve\nwith more tuning. We leave this exploration to future work.'
 '55th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers) , vol-\nume 1, pages 1657–1668.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805 .\nMax Glockner, Vered Shwartz, and Yoav Goldberg.\n2018. Breaking nli systems with sentences that re-\nquire simple lexical inferences. In Proceedings of\nthe 56th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 2: Short Papers) ,\npages 650–655.\nJonathan Gordon and Benjamin Van Durme. 2013. Re-\nporting bias and knowledge acquisition. In Proceed-\n9'
 'Published as a conference paper at ICLR 2021\nprocess is typically called pretraining, but it can be thought of as training in its own right, where the\ndownstream evaluation is demonstrating whatever knowledge we would expect a human to pick up\nfrom reading the same text.\nThis motivates us to propose a methodological change so that models are trained more like how\nhumans learn. While most previous machine learning benchmarks have models learn from a large\nquestion bank, humans primarily learn new subjects by reading books and listening to others talk\nabout the topic. For specialized subjects such as Professional Law, massive legal corpora are available,\nsuch as the 164-volume legal encyclopedia Corpus Juris Secundum , but there are fewer than 5,000\nmultistate bar exam questions available. Learning the entire law exclusively through a small number\nof practice tests is implausible, so future models must learn more during pretraining.\nFor this reason we assess pretrained models in a zero-shot, few-shot, or transfer setting and we provide\na dev, val, and test set for each task. The dev set is used for few-shot prompts, the val set could be\nused for hyperparameter tuning, and the test set is used to compute the ﬁnal accuracy. Importantly,\nthe format of our evaluation is not identical to the format in which information is acquired during\npretraining. This has the beneﬁt of obviating concerns about spurious training set annotation artifacts\n(Geirhos et al., 2020; Hendrycks et al., 2019b) and is in stark contrast to the previous paradigm\nof identically distributed training and test sets. This change also enables collecting a much more\nextensive and diverse set of tasks for evaluation. We anticipate our methodology becoming more\nwidespread as models improve at extracting information from diverse online sources.\nModel Limitations. We ﬁnd that current large-scale Transformers have wide room for improvement.\nThey are notably poor at modeling human (dis)approval, as evident by the low performance on the\nProfessional Law and Moral Scenarios tasks. For future systems to be aligned with human values, high\nperformance on these tasks is crucial (Hendrycks et al., 2020), so future research should especially\naim to increase accuracy on these tasks. Models also have difﬁculty performing calculations, so much\nso that they exhibit poor performance on Elementary Mathematics and many other STEM subjects\nwith “plug and chug” problems. Additionally, they do not match expert-level performance (90%) on\nany subject, so for all subjects it is subhuman. On average, models are only now starting to move\nbeyond random-chance accuracy levels.\nAddressing these shortcomings may be challenging. To illustrate this, we attempted to create a better\nProfessional Law model by pretraining on specialized data but achieved only limited success. We\ncollected approximately 2,000 additional Professional Law training examples. After ﬁne-tuning a\nRoBERTa-base model (Liu et al., 2019) using this custom training set, our model attained 32.8%test\naccuracy. To test the impact of additional specialized training data, we also had RoBERTa continue\npretraining on approximately 1.6 million legal case summaries using Harvard’s Law Library case law\ncorpus case.law , but after ﬁne-tuning it only attained 36.1%accuracy. This suggests that while\nadditional pretraining on relevant high quality text can help, it may not be enough to substantially\nincrease the performance of current models.\nIt is unclear whether simply scaling up existing language models will solve the test. Current\nunderstanding indicates that a 10×increase in model size must be accompanied by an approximate\n5×increase in data (Kaplan et al., 2020). Aside from the tremendous expense in creating multi-trillion\nparameter language models, data may also become a bottleneck, as there is far less written about\nesoteric branches of knowledge than about everyday situations.\n6 C ONCLUSION'
 'part of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to beneﬁt from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these ﬁndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.']","RoBERTa's comprehensive evaluation of pretraining strategies, including the effects of dynamic masking, data size, batch size, and the removal of the NSP objective, contributes valuable insights into the design and optimization of language models. Its success offers guidance for future model development, emphasizing the importance of these previously under-explored factors in achieving high performance in NLP tasks.",0.8055555555287036,1.0,0.9704178157557086,1.0,0.7077545137835661,0.9219271460433551
"How does Adversarial Filtering (AF) contribute to the creation of HellaSwag, and what unique characteristic does it bring to the dataset?","Adversarial Filtering (AF) contributes to the creation of HellaSwag by using a series of discriminators to select challenging wrong answers, resulting in a dataset that is easy for humans to solve but difficult for machines. This process ensures that the dataset remains adversarial to state-of-the-art models, as it continuously evolves by replacing easily-classified generations with more challenging adversarial ones. The unique characteristic of HellaSwag is its ability to consist of nonsensical generations that are hard for existing discriminators to differentiate from the correct answers, creating a true test for commonsense natural language inference models.","['BERT, perform at commonsense natural language\ninference (NLI)? Our surprising conclusion is\nthat the underlying task remains unsolved. In-\ndeed, we ﬁnd that deep models such as BERT do\nnot demonstrate robust commonsense reasonining\nability by themselves. Instead, they operate more\nlikerapid surface learners for a particular dataset.\nTheir strong performance on SWAG is dependent\non the ﬁnetuning process, wherein they largely\nlearn to pick up on dataset-speciﬁc distributional\nbiases. When the distribution of language shifts\nslightly, performance drops drastically – even if\nthe domain remains identical.\nWe study this question by introducing Hella-\nSwag ,2a new benchmark for commonsense\nNLI. We use Adversarial Filtering (AF), a data-\ncollection paradigm in which a series of discrim-\ninators is used to select a challenging set of gen-\nerated wrong answers. AF is surprisingly e ﬀec-\ntive towards this goal: the resulting dataset of 70k\nproblems is easy for humans (95.6% accuracy),\nyet challenging for machines ( ă50%q. This result\nholds even when models are given a signiﬁcant\nnumber of training examples, and even when the\ntest data comes from the exact same distribution\nas the training data. Machine performance slips\nan additional 5% when evaluated on examples that\ncover novel concepts from the same domain.\nTo make this dataset robust to deep pre-\ntrained models, we use a trifecta of state-of-the-\nart generators (Radford et al., 2018), state-of-\nthe-art discriminators (BERT), and high quality\nsource text. We expand on the SWAG’s origi-\nnal video-captioning domain by using WikiHow\narticles, greatly increasing the context diversity\nand generation length. Our investigation reveals\na Goldilocks zone – roughly three sentences of\ncontext, and two generated sentences – wherein\ngenerations are largely nonsensical, even though\nstate-of-the-art discriminators cannot reliably tell\nthe di ﬀerence between these generations and the\nground truth.\nMore broadly, our paper presents a case-study\ntowards a future of veriﬁed progress in NLP, via it-\nerative rounds of building and breaking datasets. If\nour ultimate goal is to provide reliable benchmarks\nfor challenging tasks, such as commonsense NLI,\nthese benchmarks cannot be static. Instead, they\nmust evolve together with the evolving state-of-\n2Short for Harder Endings, Longer contexts, and Low-\nshot Activities for Situations WithAdversarial Generations.\nDataset and code at https: //rowanzellers.com /hellaswag.\nContext 2Context 1\nContext N…\nContext 1Context M…Real ending…Real ending(N instances)(M instances)DtrainReal ending…Real endingReal ending\nGen’d ending K Gen’d ending K Gen’d ending K ……………Gen’d ending2 …Gen’d ending2 Gen’d ending2 Gen’d ending 1 …Gen’d ending 1 Gen’d ending 1 DtestGen’d ending2 …Gen’d ending 1…………Gen’d ending 2Gen’d ending 1 Gen’d ending K Gen’d ending K …\nf\nTrain f to discriminate real vs. generated\nReplace easily-classiﬁed generations with adversarial ones that currently aren’t includedGenerated Ending (context M)Generated Ending (context 2)New!New!Figure 2: An overview of Adversarial Filtering. On\neach iteration, a new classiﬁer is trained on a dummy\ntraining set Dtrainto replace easily-classiﬁed negative\nendings on the dummy test set Dtestwith adversarial\nendings. This process is repeated iteratively, to obtain\na challenging dataset regardless of the ﬁnal split.\nthe-art. Continued evolution in turn requires prin-\ncipled dataset creation algorithms. Whenever a\nnew iteration of a dataset is created, these algo-\nrithms must leverage existing modeling advance-\nments to ﬁlter out spurious biases. Only once this\ncycle becomes impossible can we say that the un-\nderlying task – as opposed an individual dataset –\nis solved.\n2 Background\nSWAG is a dataset for commonsense NLI. For\neach question, a model is given a context from a\nvideo caption and four ending choices for what\nmight happen next. Only one choice is right – the\nactual next caption of the video.'
 ""0 10 20 30 40 50\nActivitynet Adversarial Filtering iteration0255075100BERT accuracy (4-way)\n Zellers' LM GPT\n0 10 20 30 40\nWikihow Adversarial Filtering iteration0255075100BERT accuracy (4-way)\n1 sentence\n2 sentences\n3 sentencesFigure 5: Adversarial Filtering (AF) results with BERT-Large as the discriminator. Left: AF applied to ActivityNet\ngenerations produced by Zellers et al. (2018)’s language model versus OpenAI GPT. While GPT converges at\nrandom, the LM used for SWAG converges at 75%. Right : AF applied to WikiHow generations from GPT, while\nvarying the ending length from one to three sentences. They converge to random, „40%, and„50%, respectively.\nstructural patterns, we consider a new scenario,\nShuffled . Here the shared context is provided,\nbut the words in each ending choice are randomly\npermuted. Surprisingly, this reduces BERT perfor-\nmance by less than 10%. Even though BERT was\nnever exposed to randomly shu ﬄed text during\npretraining, it easily adapts to this setting, which\nsuggests that BERT is largely performing lexical\nreasoning over each (context, answer) pair.\nFinally, when the context is removed and the\nwords in each ending are shu ﬄed, performance\ndrops to 60.4%. While low, this is still higher\nthan ELMo’s performance ( ă60% from Zellers\net al., 2018). As neither context nor structure\nis needed to discriminate between human and\nmachine-written endings in a majority of cases, it\nis likely that systems primarily learn to detect dis-\ntributional stylistic patterns during ﬁnetuning.\n3.3 Where do the stylistic biases come from?\nSWAG was constructed via Adversarial Filter-\ning (AF). Endings were generated via a language\nmodel, and then selected to fool a discrimina-\ntor. To understand why it was solved requires\nunderstanding the interplay of AF with respect to\nSWAG’s generators and discriminators.\nZellers et al. (2018) used a two-layer LSTM for\ngeneration, with shallow stylistic adversarial ﬁl-\nters.6This setup was robust against ELMo mod-\nels, but has the shallow LM in particular produced\ndistributional artifacts that BERT picks up on?\n6The discriminator was an ensemble that featured a bag\nof words model, a shallow CNN, a multilayer perceptron op-\nerating on language model perplexities.To investigate this, we perform AF using BERT-\nLarge as the discriminator7in two settings, com-\nparing generations from Zellers et al. (2018) with\nthose from a ﬁnetuned GPT (Radford et al., 2018).\nStrikingly, the results, Figure 5 (left), show that\nthe generations used in SWAG are so di ﬀerent\nfrom the human-written endings that AF never\ndrops the accuracy to chance ; instead, it converges\nto roughly 75%. On the other hand, GPT’s gener-\nations are good enough that BERT accuracy drops\nbelow 30% over many random subsplits of the\ndata, revealing the importance of the generator.\n4HellaSwag\nThe success of BERT implies that high-quality\ngenerators and discriminators are crucial to AF’s\nsuccess. However, it does notimply that the un-\nderlying task of commonsense NLI – as opposed\nto a single dataset – is solved. To evaluate this\nclaim requires us to try making a new evolution\nof the SWAG dataset, one in which artifacts are\nremoved. In this section, we do just that by intro-\nducing HellaSwag .\n4.1 ActivityNet Captions\nWe start by including video captions from the\nActivityNet Captions dataset (Krishna et al.,\n2017). The original SWAG dataset contains these,\nalong with captions from LSMDC (Rohrbach\net al., 2017), but for HellaSwag we solely used\n7On each iteration, BERT-Large is re-initialized from its\npretrained checkpoint, ﬁnetuned, and then evaluated in a\nfour-way setting on the dummy test set of held-out data. See\nSupp A for a details of our BERT-Large AF setup.\n4""
 'Stylistic\nEnsembleELMo+\nLSTMGPT BERTBase BERTLarge30405060708090100 Accuracy (%)\n48.2%53.7%64.8%71.4%83.0%\n28.0% 28.2% 28.4%32.0%41.1%78.5%77.4%\n71.3%\n63.0%\n41.1%Accuracy of the filtering model before AF\nAccuracy of the filtering model after AF\nBERT-Large accuracy after AFFigure 11: Performance on the WikiHow subset of al-\nternative variations of HellaSwag , where di ﬀerent Ad-\nversarial Filters are used (but without human valida-\ntion). We consider the shallow stylistic adversaries\nused by Zellers et al. (2018) (Stylistic Ensemble),\nas well as an LSTM with ELMo embeddings, GPT,\nBERT-Base, and BERT-Large. For each adversarial ﬁl-\ntering model, we record the accuracy of that model be-\nfore and after AF is used. We also evaluate each al-\nternative dataset using BERT-Large. The results sug-\ngest that using a a stronger model at test time (over the\nmodel used for AF) improves performance, but is not\nenough to solve the task.\n6 Discussion\nOur results suggest that HellaSwag is a challenging\ntestbed for state-of-the-art NLI models, even those\nbuilt on extensive pretraining. The question still\nremains, though, of where will the ﬁeld go next?\n6.1 How easy might HellaSwag be for future\ndiscriminators?\nIn this paper, we showed the existence of a\nGoldilocks zone of text complexity – in which\ngenerations are nonsensical, but existing state-\nof-the-art NLP models cannot tell the di ﬀerence.\nHow hard will the dataset be for future, even more\npowerful, models?\nAnswering this question is challenging because\nthese models don’t exist (or are unavailable) at\nthe time of writing . However, one remedy is to\nperform an ablation study on the Adversarial Fil-\ntering model used, comparing weaker ﬁlters with\nstronger discriminators. We present our results\nin Figure 11, and ﬁnd that while weak discrim-\ninators (like the stylistic ensemble used to make\nSWAG) only marginally reduce the accuracy of\nBERT-Large, increasing the gap between the ﬁlter\nand the ﬁnal discriminator is not enough to solve\nthe task. For instance, using a discriminator with\n3x the parameters as the adversarial ﬁlter (BERT-\nLarge vs. BERT-Base) results in 63% machine ac-\ncuracy.\n30 40 50 60 70 80 90 100\nOverall Accuracy on HellaSwag1021041061081010Pretraining Hours (Estimate)\nHuman performance?\nELMoGPTBERT-BaseBERT-LargeFigure 12: Estimated pretraining hours required to\nreach a desired accuracy on HellaSwag . We estimate\nperfomance with respect to a RTX 2080 Ti - a modern,\nfast GPU, and ﬁt a log-linear regression line. An ex-\ntrapolation suggests that to reach human-level perfor-\nmance on HellaSwag , without algorithmic or computa-\ntional improvements, would require 109GPU-hours of\npretraining (over 100k GPU years).\n6.2 How well does pretraining scale?\nOverall, the current paradigm of pretraining large\nmodels on lots of data has made immense progress\non NLP benchmarks. Though we expect this\ntrend to continue, it also behooves us to con-\nsider its limits. If more compute is indeed the\nanswer for human-level commonsense inference,\nwhat would the compute requirements of this hy-\npothetical massive model look like?\nWe investigate this in Figure 12 by compar-\ning the accuracies of known models on Hella-\nSwag with their computational needs. This estima-\ntion is a rough estimate: we convert reported TPU\nruntimes to our benchmark RTX 2080 Ti GPU us-\ning the Rooﬂine model (Williams et al., 2009),\nwhich focuses primarily on the bottleneck of load-\ning tensors into GPU memory. Extrapolating from\nan exponential ﬁt suggests that reaching human-\nlevel performance on our dataset would require\n109GPU hours, or 100k years – unless algorith-\nmic improvements are made.\nWhat might these algorithmic improvements\nlook like? These could include architectural ad-\nvances, better pretraining objectives, and beyond.\nHowever, these improvements share the bottle-\nneck of the data source. To answer some Hella-\nSwag questions correctly without reasoning deeply\n– like knowing that it is a bad idea to stop at a'
 'lems of reporting bias (Gordon and Van Durme,\n2013). Alternatively, future models might answer\ncorrectly only by picking up on spurious patterns,\nin which case a new development of the bench-\nmark – using these models as adversaries – would\nplace us in the same position as we are right now.\nPut another way, for humans to answer Hella-\nSwag questions requires abstracting away from\nlanguage and modeling world states instead. We\npostulate that this is what separates solving the\ntask of commonsense NLI, as opposed to a par-\nticular dataset. Indeed, we ﬁnd that existing deep\nmethods often get fooled by lexical false friends.\nFor example, in the WikiHow example from Fig-\nure 10, BERT chooses an ending that matches\nthetechnology words in the context, rather than\nmatching the deeper topic: using technology as an\nexcuse for not doing homework.\n6.3 Towards a future of evolving benchmarks\nWhat happens when HellaSwag gets solved? We\nbelieve the answer is simple: crowdsource another\ndataset, with the same exact format, and see where\nmodels fail. Indeed, in our work we found this to\nbe straightforward from an algorithmic perspec-\ntive: by throwing in the best known generator\n(GPT) and the best known discriminator (BERT-\nLarge), we made a dataset that is adversarial - not\njust to BERT, but to all models we have access to.\nWhile this was easy algorithmically, care must\nbe taken from a data curation standpoint. Indeed,\nwe ﬁnd success exists within a Goldilocks zone:\nthe data source must be complex enough that state-\nof-the-art generators often make mistakes, while\nsimple enough such that discriminators often fail\nto catch them. This ties the future of SWAG-\nstyle benchmarks to progress on language gener-\nation: until generation is solved, commonsense\nNLI will remain unsolved. Even recent promis-\ning results on scaling up language models (Rad-\nford et al., 2019) ﬁnd problems in terms of consis-\ntency, with the best curated examples requiring 25\nrandom seeds.\n7 Conclusion\nIn this paper, we presented HellaSwag , a new\ndataset for physically situated commonsense rea-\nsoning. By constructing the dataset through ad-\nversarial ﬁltering, combined with state-of-the-art\nmodels for language generation and discrimina-\ntion, we produced a dataset that is adversarial tothe most robust models available – even when\nmodels are evaluated on items from the train-\ning distribution. In turn, we provided insight\ninto the inner workings of pretrained models, and\nsuggest a path for NLP progress going forward:\ntowards benchmarks that adversarially co-evolve\nwith evolving state-of-the-art models.\nAcknowledgments\nWe thank the reviewers, as well as Jesse Thoma-\nson, for their helpful feedback. We thank the\nMechanical Turk workers for their great work\nduring dataset collection. Thanks also to Zak\nStone and the Google Cloud TPU team for help\nwith the computing infrastructure. This work\nwas supported by the National Science Foundation\nthrough a Graduate Research Fellowship (DGE-\n1256082) and NSF grants (IIS-1524371, 1637479,\n165205, 1703166), the DARPA CwC program\nthrough ARO (W911NF-15-1-0543), the IARPA\nDIV A program through D17PC00343, the Sloan\nResearch Foundation through a Sloan Fellowship,\nthe Allen Institute for Artiﬁcial Intelligence, the\nNVIDIA Artiﬁcial Intelligence Lab, and gifts by\nGoogle and Facebook. The views and conclu-\nsions contained herein are those of the authors and\nshould not be interpreted as representing endorse-\nments of IARPA, DOI /IBC, or the U.S. Govern-\nment.\nReferences\nYonatan Belinkov and Yonatan Bisk. 2018. Synthetic\nand natural noise both break neural machine transla-\ntion. In ICLR . ICLR.\nQian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui\nJiang, and Diana Inkpen. 2017. Enhanced lstm for\nnatural language inference. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers) , vol-\nume 1, pages 1657–1668.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and']","AF iteratively selects challenging machine-generated wrong answers by using a series of discriminators, creating a dataset that is robust against model biases and exhibits a 'Goldilocks' zone of complexity.",0.999999999975,1.0,0.9396703233429817,1.0,0.41971044771466576,0.878841790858663
What is the impact of removing the NSP loss on RoBERTa's performance across various benchmarks compared to BERT?,"Removing the NSP loss in RoBERTa significantly improves performance on various benchmarks compared to BERT, where the NSP task is included. For instance, in evaluations like QNLI, MNLI, and SQuAD, models trained without NSP show better or comparable results to those that include it. This indicates that the NSP task may not be as crucial for achieving high performance in downstream tasks as originally thought in the BERT framework.","['necessity of the NSP loss ( Lample and Conneau ,\n2019 ;Yang et al. ,2019 ;Joshi et al. ,2019 ).\nTo better understand this discrepancy, we com-\npare several alternative training formats:\n•SEGMENT -PAIR +NSP: This follows the original\ninput format used in BERT ( Devlin et al. ,2019 ),\nwith the NSP loss. Each input has a pair of seg-\nments, which can each contain multiple natural\nsentences, but the total combined length must\nbe less than 512 tokens.'
 'Dev Set\nTasks MNLI-m QNLI MRPC SST-2 SQuAD\n(Acc) (Acc) (Acc) (Acc) (F1)\nBERT BASE 84.4 88.4 86.7 92.7 88.5\nNo NSP 83.9 84.9 86.5 92.6 87.9\nLTR & No NSP 82.1 84.3 77.5 92.1 77.8\n+ BiLSTM 82.1 84.1 75.7 91.6 84.9\nTable 5: Ablation over the pre-training tasks using the\nBERT BASE architecture. “No NSP” is trained without\nthe next sentence prediction task. “LTR & No NSP” is\ntrained as a left-to-right LM without the next sentence\nprediction, like OpenAI GPT. “+ BiLSTM” adds a ran-\ndomly initialized BiLSTM on top of the “LTR + No\nNSP” model during ﬁne-tuning.\nablation studies can be found in Appendix C.\n5.1 Effect of Pre-training Tasks\nWe demonstrate the importance of the deep bidi-\nrectionality of BERT by evaluating two pre-\ntraining objectives using exactly the same pre-\ntraining data, ﬁne-tuning scheme, and hyperpa-\nrameters as BERT BASE :\nNo NSP : A bidirectional model which is trained\nusing the “masked LM” (MLM) but without the\n“next sentence prediction” (NSP) task.\nLTR & No NSP : A left-context-only model which\nis trained using a standard Left-to-Right (LTR)\nLM, rather than an MLM. The left-only constraint\nwas also applied at ﬁne-tuning, because removing\nit introduced a pre-train/ﬁne-tune mismatch that\ndegraded downstream performance. Additionally,\nthis model was pre-trained without the NSP task.\nThis is directly comparable to OpenAI GPT, but\nusing our larger training dataset, our input repre-\nsentation, and our ﬁne-tuning scheme.\nWe ﬁrst examine the impact brought by the NSP\ntask. In Table 5, we show that removing NSP\nhurts performance signiﬁcantly on QNLI, MNLI,\nand SQuAD 1.1. Next, we evaluate the impact\nof training bidirectional representations by com-\nparing “No NSP” to “LTR & No NSP”. The LTR\nmodel performs worse than the MLM model on all\ntasks, with large drops on MRPC and SQuAD.\nFor SQuAD it is intuitively clear that a LTR\nmodel will perform poorly at token predictions,\nsince the token-level hidden states have no right-\nside context. In order to make a good faith at-\ntempt at strengthening the LTR system, we added\na randomly initialized BiLSTM on top. This does\nsigniﬁcantly improve results on SQuAD, but theresults are still far worse than those of the pre-\ntrained bidirectional models. The BiLSTM hurts\nperformance on the GLUE tasks.\nWe recognize that it would also be possible to\ntrain separate LTR and RTL models and represent\neach token as the concatenation of the two mod-\nels, as ELMo does. However: (a) this is twice as\nexpensive as a single bidirectional model; (b) this\nis non-intuitive for tasks like QA, since the RTL\nmodel would not be able to condition the answer\non the question; (c) this it is strictly less powerful\nthan a deep bidirectional model, since it can use\nboth left and right context at every layer.\n5.2 Effect of Model Size\nIn this section, we explore the effect of model size\non ﬁne-tuning task accuracy. We trained a number\nof BERT models with a differing number of layers,\nhidden units, and attention heads, while otherwise\nusing the same hyperparameters and training pro-\ncedure as described previously.\nResults on selected GLUE tasks are shown in\nTable 6. In this table, we report the average Dev\nSet accuracy from 5 random restarts of ﬁne-tuning.\nWe can see that larger models lead to a strict ac-\ncuracy improvement across all four datasets, even\nfor MRPC which only has 3,600 labeled train-\ning examples, and is substantially different from\nthe pre-training tasks. It is also perhaps surpris-\ning that we are able to achieve such signiﬁcant\nimprovements on top of models which are al-\nready quite large relative to the existing literature.\nFor example, the largest Transformer explored in\nVaswani et al. (2017) is (L=6, H=1024, A=16)\nwith 100M parameters for the encoder, and the\nlargest Transformer we have found in the literature\nis (L=64, H=512, A=2) with 235M parameters\n(Al-Rfou et al., 2018). By contrast, BERT BASE\ncontains 110M parameters and BERT LARGE con-\ntains 340M parameters.'
 'Model SQuAD 1.1/2.0 MNLI-m SST-2 RACE\nOur reimplementation (with NSP loss):\nSEGMENT -PAIR 90.4/78.7 84.0 92.9 64.2\nSENTENCE -PAIR 88.7/76.2 82.9 92.1 63.0\nOur reimplementation (without NSP loss):\nFULL -SENTENCES 90.4/79.1 84.7 92.5 64.8\nDOC-SENTENCES 90.6/79.7 84.7 92.7 65.6\nBERT BASE 88.5/76.3 84.3 92.8 64.3\nXLNet BASE (K = 7) –/81.3 85.8 92.7 66.1\nXLNet BASE (K = 6) –/81.0 85.6 93.4 66.7\nTable 2: Development set results for base models pretrained over B OOK CORPUS and W IKIPEDIA . All models are\ntrained for 1M steps with a batch size of 256 sequences. We rep ort F1 for SQuAD and accuracy for MNLI-m,\nSST-2 and RACE. Reported results are medians over ﬁve random initializations (seeds). Results for BERT BASEand\nXLNet BASEare from Yang et al. (2019 ).\n•SENTENCE -PAIR +NSP: Each input contains a\npair of natural sentences , either sampled from\na contiguous portion of one document or from\nseparate documents. Since these inputs are sig-\nniﬁcantly shorter than 512 tokens, we increase\nthe batch size so that the total number of tokens\nremains similar to SEGMENT -PAIR +NSP. We re-\ntain the NSP loss.\n•FULL -SENTENCES : Each input is packed with\nfull sentences sampled contiguously from one\nor more documents, such that the total length is\nat most 512 tokens. Inputs may cross document\nboundaries. When we reach the end of one doc-\nument, we begin sampling sentences from the\nnext document and add an extra separator token\nbetween documents. We remove the NSP loss.\n•DOC-SENTENCES : Inputs are constructed sim-\nilarly to FULL -SENTENCES , except that they\nmay not cross document boundaries. Inputs\nsampled near the end of a document may be\nshorter than 512 tokens, so we dynamically in-\ncrease the batch size in these cases to achieve\na similar number of total tokens as FULL -\nSENTENCES . We remove the NSP loss.\nResults Table 2shows results for the four dif-\nferent settings. We ﬁrst compare the original\nSEGMENT -PAIR input format from Devlin et al.\n(2019 ) to the SENTENCE -PAIR format; both for-\nmats retain the NSP loss, but the latter uses sin-\ngle sentences. We ﬁnd that using individual\nsentences hurts performance on downstream\ntasks , which we hypothesize is because the model\nis not able to learn long-range dependencies.We next compare training without the NSP\nloss and training with blocks of text from a sin-\ngle document ( DOC-SENTENCES ). We ﬁnd that\nthis setting outperforms the originally published\nBERT BASEresults and that removing the NSP loss\nmatches or slightly improves downstream task\nperformance , in contrast to Devlin et al. (2019 ).\nIt is possible that the original BERT implementa-\ntion may only have removed the loss term while\nstill retaining the SEGMENT -PAIR input format.\nFinally we ﬁnd that restricting sequences to\ncome from a single document ( DOC-SENTENCES )\nperforms slightly better than packing sequences\nfrom multiple documents ( FULL -SENTENCES ).\nHowever, because the DOC-SENTENCES format\nresults in variable batch sizes, we use FULL -\nSENTENCES in the remainder of our experiments\nfor easier comparison with related work.\n4.3 Training with large batches\nPast work in Neural Machine Translation has\nshown that training with very large mini-batches\ncan both improve optimization speed and end-task\nperformance when the learning rate is increased\nappropriately ( Ott et al. ,2018 ). Recent work has\nshown that BERT is also amenable to large batch\ntraining ( You et al. ,2019 ).\nDevlin et al. (2019 ) originally trained\nBERT BASE for 1M steps with a batch size of\n256 sequences. This is equivalent in computa-\ntional cost, via gradient accumulation, to training\nfor 125K steps with a batch size of 2K sequences,\nor for 31K steps with a batch size of 8K.\nIn Table 3we compare perplexity and end-'
 'across all augmented variants. In contrast, InstructGPT\nand GLM exhibit a statistically significant degradation\nin performance, averaging approximately 5% across the\nvarious perturbation methods. Notably, all three language\nmodels exhibit a heightened level of susceptibility to\nsemantics-altering perturbations in the IMDB sentiment\nanalysis scenario, as indicated by the EM ctstmetric.\nIn addition, the model performance is evaluated on\nthe BoolQA dataset using identical experimental settings.\nThe results of this evaluation are consistent with the\naforementioned findings, with ChatGPT outperforming\nthe other two baselines on both the original and per-\nturbed subsets. Furthermore, it is observed that the three\nlanguage models exhibit a degradation in performance\nwhen subjected to contrasting semantic perturbations,\nsuggesting that equivariance of robustness may pose a\nsignificant challenge for the current advanced language\nmodels and their applications.']","Removing the NSP loss from RoBERTa's pretraining objective leads to significant improvements in downstream task performance. This finding challenges the initial hypothesis that NSP is crucial for learning sentence relationships, suggesting that masked language modeling alone, under optimal conditions, is sufficient for achieving superior performance.",0.5833333333041666,0.3333333333333333,0.9773870647049185,0.6666666666666666,0.535269070320588,0.9410762812823517
In what ways does RoBERTa's training process leverage data size and training duration for improved model performance?,"RoBERTa's training process improves model performance by utilizing significantly larger data sizes and longer training durations compared to previous models like BERT. It is pretrained on up to 160GB of text and can be trained for as many as 500K steps, leading to substantial gains in performance across various downstream tasks. This approach emphasizes the importance of data size and diversity, as well as the duration of training in enhancing model capabilities.","['and (2) the number of training passes through the\ndata. For example, the recently proposed XLNet\narchitecture ( Yang et al. ,2019 ) is pretrained us-\ning nearly 10 times more data than the original\nBERT ( Devlin et al. ,2019 ). It is also trained with\na batch size eight times larger for half as many op-\ntimization steps, thus seeing four times as many\nsequences in pretraining compared to BERT.\nTo help disentangle the importance of these fac-\ntors from other modeling choices (e.g., the pre-\ntraining objective), we begin by training RoBERTa\nfollowing the BERT LARGE architecture ( L= 24 ,\nH= 1024 ,A= 16 , 355M parameters). We\npretrain for 100K steps over a comparable B OOK -\nCORPUS plus W IKIPEDIA dataset as was used in'
 'Model data bsz stepsSQuADMNLI-m SST-2(v1.1/2.0)\nRoBERTa\nwith B OOKS + W IKI 16GB 8K 100K 93.6/87.3 89.0 95.3\n+ additional data ( §3.2) 160GB 8K 100K 94.0/87.7 89.3 95.6\n+ pretrain longer 160GB 8K 300K 94.4/88.7 90.0 96.1\n+ pretrain even longer 160GB 8K 500K 94.6/89.4 90.2 96.4\nBERT LARGE\nwith B OOKS + W IKI 13GB 256 1M 90.9/81.8 86.6 93.7\nXLNet LARGE\nwith B OOKS + W IKI 13GB 256 1M 94.0/87.8 88.4 94.4\n+ additional data 126GB 2K 500K 94.5/88.8 89.8 95.6\nTable 4: Development set results for RoBERTa as we pretrain o ver more data (16GB →160GB of text) and pretrain\nfor longer (100K →300K→500K steps). Each row accumulates improvements from the row s above. RoBERTa\nmatches the architecture and training objective of BERT LARGE . Results for BERT LARGE and XLNet LARGE are from\nDevlin et al. (2019 ) and Yang et al. (2019 ), respectively. Complete results on all GLUE tasks can be fo und in the\nAppendix.\nDevlin et al. (2019 ). We pretrain our model using\n1024 V100 GPUs for approximately one day.\nResults We present our results in Table 4. When\ncontrolling for training data, we observe that\nRoBERTa provides a large improvement over the\noriginally reported BERT LARGE results, reafﬁrming\nthe importance of the design choices we explored\nin Section 4.\nNext, we combine this data with the three ad-\nditional datasets described in Section 3.2. We\ntrain RoBERTa over the combined data with the\nsame number of training steps as before (100K).\nIn total, we pretrain over 160GB of text. We ob-\nserve further improvements in performance across\nall downstream tasks, validating the importance of\ndata size and diversity in pretraining.9\nFinally, we pretrain RoBERTa for signiﬁcantly\nlonger, increasing the number of pretraining steps\nfrom 100K to 300K, and then further to 500K. We\nagain observe signiﬁcant gains in downstream task\nperformance, and the 300K and 500K step mod-\nels outperform XLNet LARGE across most tasks. We\nnote that even our longest-trained model does not\nappear to overﬁt our data and would likely beneﬁt\nfrom additional training.\nIn the rest of the paper, we evaluate our best\nRoBERTa model on the three different bench-\nmarks: GLUE, SQuaD and RACE. Speciﬁcally\n9Our experiments conﬂate increases in data size and di-\nversity. We leave a more careful analysis of these two dimen-\nsions to future work.we consider RoBERTa trained for 500K steps over\nall ﬁve of the datasets introduced in Section 3.2.\n5.1 GLUE Results\nFor GLUE we consider two ﬁnetuning settings.\nIn the ﬁrst setting ( single-task, dev ) we ﬁnetune\nRoBERTa separately for each of the GLUE tasks,\nusing only the training data for the correspond-\ning task. We consider a limited hyperparameter\nsweep for each task, with batch sizes ∈ {16,32}\nand learning rates ∈ {1e−5,2e−5,3e−5}, with a\nlinear warmup for the ﬁrst 6% of steps followed by\na linear decay to 0. We ﬁnetune for 10 epochs and\nperform early stopping based on each task’s eval-\nuation metric on the dev set. The rest of the hyper-\nparameters remain the same as during pretraining.\nIn this setting, we report the median development\nset results for each task over ﬁve random initial-\nizations, without model ensembling.\nIn the second setting ( ensembles, test ), we com-\npare RoBERTa to other approaches on the test set\nvia the GLUE leaderboard. While many submis-\nsions to the GLUE leaderboard depend on multi-\ntask ﬁnetuning, our submission depends only on\nsingle-task ﬁnetuning . For RTE, STS and MRPC\nwe found it helpful to ﬁnetune starting from the\nMNLI single-task model, rather than the baseline\npretrained RoBERTa. We explore a slightly wider\nhyperparameter space, described in the Appendix,\nand ensemble between 5 and 7 models per task.'
 'is (L=64, H=512, A=2) with 235M parameters\n(Al-Rfou et al., 2018). By contrast, BERT BASE\ncontains 110M parameters and BERT LARGE con-\ntains 340M parameters.\nIt has long been known that increasing the\nmodel size will lead to continual improvements\non large-scale tasks such as machine translation\nand language modeling, which is demonstrated\nby the LM perplexity of held-out training data\nshown in Table 6. However, we believe that\nthis is the ﬁrst work to demonstrate convinc-\ningly that scaling to extreme model sizes also\nleads to large improvements on very small scale\ntasks, provided that the model has been sufﬁ-\nciently pre-trained. Peters et al. (2018b) presented'
 'Large Language Models Struggle to Learn Long-Tail Knowledge\n101010121014101610181020\nNumber of Parameters0.00.10.20.30.40.50.6Accuracy\nHuman Accuracy w/ Context\nStrong Supervised Model\nLinear Fit (R2 = 0.98)\nFigure 6. Scaling trends for fact learning . We plot BLOOM ac-\ncuracy on rare instances from Natural Questions ( <100relevant\ndocs) as a function of the log of the model size. Extrapolating\nfrom the empirical line of best fit—which approximates the trend\nwell at R2= 0.98—implies that immensely large models would\nbe necessary to get high accuracy.\nure 5. For questions with few relevant documents in the\noriginal C4 dataset, performance is poor for both the base-\nline and the counterfactual LM, i.e., their performance dif-\nference is small. However, for questions with many rele-\nvant documents, performance is significantly worse for the\ncounterfactual LM. This suggests a causal link between the\nnumber of relevant documents and QA performance.\n4. Methods to Improve Rare Fact Learning\nThus far, we showed that LLMs have a strong dependence on\nrelevant document count. Here, we investigate methods to\nmitigate this dependence: increasing data scale, increasing\nmodel scale, and adding an auxiliary retrieval module.\n4.1. Can We Scale Up Datasets?\nToday’s largest LLMs are pre-trained on hundreds of bil-\nlions of tokens. One na ¨ıve approach for improving accuracy\non questions about less-prevalent knowledge is to collect\nlarger quantities of data. Our results suggest that this would\nnot significantly improve accuracy as scaling datasets by\nmoderate factors (e.g., 5x) usually results in small accu-\nracy gains. An alternative idea would be to increase the\ndiversity of the pre-training data. However, we also be-\nlieve this would provide minimal benefit because many data\nsources are surprisingly correlated . Although each of the\npre-training datasets considered were collected indepen-ROOTS Pile C4 OWT Wiki\nROOTS - 0.97 0.97 0.94 0.87\nPile - - 0.95 0.96 0.87\nC4 - - - 0.96 0.90\nOWT - - - - 0.91\nWiki - - - - -\nTable 1. Spearman rank correlations of the relevant document\ncounts for TriviaQA examples in The Pile, ROOTS, C4, OpenWeb-\nText, and Wikipedia. Despite having different collection method-\nologies, these pre-training datasets are highly correlated in terms of\nhow much information they contain related to different QA pairs.\ndently, the amount of supporting information they provide\nfor different TriviaQA examples is highly consistent as seen\nby the rank correlations between their relevant document\ncounts in Table 1.\n4.2. Can We Scale Up Models?\nUsing larger models consistently produces better QA perfor-\nmance. However, our results suggest that one would need\nimmensely large LMs to achieve high accuracy on long-tail\nquestions. In Figure 6 we plot a scaling trend line for rare\nfact learning, where we show BLOOM accuracy on rare\ninstances from Natural Questions ( <100relevant docs)\nas a function of the log of the model size. The empirical\nlog-linear trend—which approximates the scaling extremely\nwell ( R2= 0.98)—shows that in order to match a strong\nsupervised baseline (Izacard & Grave, 2021) or human per-\nformance, one would need a BLOOM model with over 1018\n(one quintillion) parameters.1We see similar trends for\nother models and datasets (see Figure 12 in the Appendix).\nModifying the Training Objective Another option sim-\nilar to scaling up models is to directly modify the training\nobjective to encourage memorization. One simple method\nto accomplish this is to increase the number of training\nepochs. All of the LMs that we study do limited epochs,\nas it is generally seen as preferable to use large enough\npre-training datasets so that the LM completes one epoch of\ntraining when the compute budget is exhausted (Raffel et al.,\n2020). However, in the context of QA, it may be preferable\nto increase epochs and reduce data size to ensure models\nmemorize as much as possible. Alternatively, one could']","RoBERTa extensively explores the impact of both increased data size and prolonged training duration, demonstrating that both factors significantly contribute to enhanced model performance. Training over larger datasets and for more steps than BERT allows RoBERTa to better generalize and excel on downstream tasks.",0.999999999975,0.9090909090909091,0.9236857472616894,1.0,0.7724367592537619,0.9468898941579047
What defines the Task2Vec embedding's ability to capture the complexity of a visual classification task and its semantic relationships?,"The Task2Vec embedding's ability to capture the complexity of a visual classification task is defined by the norm of the embedding, which correlates with the test error on the task. Additionally, the distance between embeddings reflects the semantic similarities between tasks, as it correlates positively with natural distances such as taxonomic relationships. This is achieved through the computation of the diagonal Fisher Information Matrix associated with a probe network, which provides a rich representation of the task.","['TASK2VEC: Task Embedding for Meta-Learning\nAlessandro Achille\nUCLA and AWS\nachille@cs.ucla.eduMichael Lam\nAWS\nmichlam@amazon.comRahul Tewari\nAWS\ntewarir@amazon.comAvinash Ravichandran\nAWS\nravinash@amazon.com\nSubhransu Maji\nUMass and AWS\nsmmaji@amazon.comCharless Fowlkes\nUCI and AWS\nfowlkec@amazon.comStefano Soatto\nUCLA and AWS\nsoattos@amazon.comPietro Perona\nCaltech and AWS\nperonapp@amazon.com\nAbstract\nWe introduce a method to provide vectorial represen-\ntations of visual classiﬁcation tasks which can be used\nto reason about the nature of those tasks and their re-\nlations. Given a dataset with ground-truth labels and a\nloss function deﬁned over those labels, we process images\nthrough a “probe network” and compute an embedding\nbased on estimates of the Fisher information matrix asso-\nciated with the probe network parameters. This provides a\nﬁxed-dimensional embedding of the task that is independent\nof details such as the number of classes and does not require\nany understanding of the class label semantics. We demon-\nstrate that this embedding is capable of predicting task sim-\nilarities that match our intuition about semantic and tax-\nonomic relations between different visual tasks ( e.g., tasks\nbased on classifying different types of plants are similar).\nWe also demonstrate the practical value of this framework\nfor the meta-task of selecting a pre-trained feature extractor\nfor a new task. We present a simple meta-learning frame-\nwork for learning a metric on embeddings that is capable of\npredicting which feature extractors will perform well. Se-\nlecting a feature extractor with task embedding obtains a\nperformance close to the best available feature extractor,\nwhile costing substantially less than exhaustively training\nand evaluating on all available feature extractors.\n1. Introduction\nThe success of Deep Learning hinges in part on the fact\nthat models learned for one task can be used on other related\ntasks. Yet, no general framework exists to describe and\nlearn relations between tasks. We introduce the TASK 2VEC\nembedding, a technique to represent tasks as elements of a\nvector space based on the Fisher Information Matrix. The\nnorm of the embedding correlates with the complexity of\nthe task, while the distance between embeddings capturessemantic similarities between tasks (Fig. 1). When other\nnatural distances are available, such as the taxonomical dis-\ntance in biological classiﬁcation, we ﬁnd that the embed-\nding distance correlates positively with it (Fig. 2). More-\nover, we introduce an asymmetric distance on tasks which\ncorrelates with the transferability between tasks.\nComputation of the embedding leverages a duality be-\ntween network parameters (weights) and outputs (activa-\ntions) in a deep neural network (DNN): Just as the activa-\ntions of a DNN trained on a complex visual recognition task\nare a rich representation of the input images, we show that\nthe gradients of the weights relative to a task-speciﬁc loss\nare a rich representation of the task itself. Speciﬁcally, given\na task deﬁned by a dataset D={(xi,yi)}N\ni=1of labeled\nsamples, we feed the data through a pre-trained reference\nconvolutional neural network which we call “ probe net-\nwork ”, and compute the diagonal Fisher Information Ma-\ntrix (FIM) of the network ﬁlter parameters to capture the\nstructure of the task (Sect. 2). Since the architecture and\nweights of the probe network are ﬁxed, the FIM provides a\nﬁxed-dimensional representation of the task. We show this\nembedding encodes the “difﬁculty” of the task, character-\nistics of the input domain, and which features of the probe\nnetwork are useful to solve it (Sect. 2.1).\nOur task embedding can be used to reason about the\nspace of tasks and solve meta-tasks. As a motivating exam-\nple, we study the problem of selecting the best pre-trained\nfeature extractor to solve a new task. This can be particu-\nlarly valuable when there is insufﬁcient data to train or ﬁne-'
 'Task embedding for iMaterialist In Fig. 1 we show a\nt-SNE visualization of the embedding for iMaterialist and\niNaturalist tasks. Task embedding yields interpretable re-\nsults: Tasks that are correlated in the dataset, such as binary\nclasses corresponding to the same categorical attribute, may\nend up far away from each other and close to other tasks that\nare semantically more similar ( e.g., the jeans category task\nis close to the ripped attribute and the denim material). This\nis reﬂected in the mixture of colors of semantically related\nnearby tasks, showing non-trivial grouping.\nWe also compare the TASK 2VEC embedding with a do-\nmain embedding baseline, which only exploits the input\ndistribution p(x)rather than the task distribution p(x,y).\nWhile some tasks are highly correlated with their domain\n(e.g., tasks from iNaturalist), other tasks differ only on the\nlabels ( e.g., all the attribute tasks of iMaterialist, which\nshare the same clothes domain). Accordingly, the domain'
 'lem of meta-learning has a long history with much re-\ncent work dedicated to problems such as neural architecture\nsearch and hyper-parameter estimation. Closely related to\nour problem is work on selecting from a library of classi-\nﬁers to solve a new task [33, 2, 20]. Unlike our approach,\nthese usually address the question via land-marking or ac-\ntive testing, in which a few different models are evaluated\nand performance of the remainder estimated by extension.\nThis can be viewed as a problem of completing a matrix\ndeﬁned by performance of each model on each task.\nA similar approach has been taken in computer vision for\nselecting a detector for a new category out of a large library\nof detectors [26, 40, 38].\n7. Discussion\nTASK 2VEC is an efﬁcient way to represent a task, or the\ncorresponding dataset, as a ﬁxed dimensional vector. It has\nseveral appealing properties, in particular its norm corre-\nlates with the test error obtained on the task, and the co-\nsine distance between embeddings correlates with natural\ndistances between tasks, when available, such as the taxo-\nnomic distance for species classiﬁcation, and the ﬁne-tuning\ndistance for transfer learning. Having a representation of\ntasks paves the way for a wide variety of meta-learning\ntasks. In this work, we focused on selection of an expert\nfeature extractor in order to solve a new task, especially\nwhen little training data is present, and showed that using\nTASK 2VEC to select an expert from a collection can sen-\nsibly improve test performance while adding only a small\noverhead to the training process.'
 'relatively few samples. In Fig. 4 we show how the perfor-\nmance of TASK 2VEC varies on a model selection task as the\nnumber of samples varies. At all sample sizes TASK 2VEC is\nclose to the optimum, and improves over selecting a generic\nexpert (ImageNet), both when ﬁne-tuning and when train-\ning only a classiﬁer. We observe that the best choice of ex-\nperts is not affected by the dataset size, and that even with\nfew examples TASK 2VEC is able to ﬁnd the optimal experts.\nChoice of probe network In Table 1 we show that\nDenseNet [15] and ResNet architectures [11] perform sig-\nniﬁcantly better when used as probe networks to compute\ntheTASK 2VEC embedding than a VGG [32] architecture.\n6. Related Work\nTask and Domain embedding. Tasks distinguished by\ntheir domain can be understood simply in terms of image\nstatistics. Due to the bias of different datasets, sometimes a\nbenchmark task may be identiﬁed just by looking at a few\nimages [34]. The question of determining what summary']","Task2Vec embedding utilizes the Fisher Information Matrix (FIM) computed from a probe network's parameters, capturing the task's complexity and semantic relationships by representing tasks as elements in a vector space.",0.8333333332916666,1.0,0.9320937554795713,1.0,0.8743917395394644,0.9261383867292864
How does Task2Vec's embedding relate to the difficulty and domain characteristics of a task?,"Task2Vec's embedding encodes the difficulty of a task through the norm of the embedding, which correlates with the test error obtained on that task. Additionally, the embedding captures characteristics of the input domain, allowing for a representation that reflects both the complexity and the nature of the task. This information can be valuable for selecting appropriate feature extractors for new tasks.","['Task embedding for iMaterialist In Fig. 1 we show a\nt-SNE visualization of the embedding for iMaterialist and\niNaturalist tasks. Task embedding yields interpretable re-\nsults: Tasks that are correlated in the dataset, such as binary\nclasses corresponding to the same categorical attribute, may\nend up far away from each other and close to other tasks that\nare semantically more similar ( e.g., the jeans category task\nis close to the ripped attribute and the denim material). This\nis reﬂected in the mixture of colors of semantically related\nnearby tasks, showing non-trivial grouping.\nWe also compare the TASK 2VEC embedding with a do-\nmain embedding baseline, which only exploits the input\ndistribution p(x)rather than the task distribution p(x,y).\nWhile some tasks are highly correlated with their domain\n(e.g., tasks from iNaturalist), other tasks differ only on the\nlabels ( e.g., all the attribute tasks of iMaterialist, which\nshare the same clothes domain). Accordingly, the domain'
 'TASK2VEC: Task Embedding for Meta-Learning\nAlessandro Achille\nUCLA and AWS\nachille@cs.ucla.eduMichael Lam\nAWS\nmichlam@amazon.comRahul Tewari\nAWS\ntewarir@amazon.comAvinash Ravichandran\nAWS\nravinash@amazon.com\nSubhransu Maji\nUMass and AWS\nsmmaji@amazon.comCharless Fowlkes\nUCI and AWS\nfowlkec@amazon.comStefano Soatto\nUCLA and AWS\nsoattos@amazon.comPietro Perona\nCaltech and AWS\nperonapp@amazon.com\nAbstract\nWe introduce a method to provide vectorial represen-\ntations of visual classiﬁcation tasks which can be used\nto reason about the nature of those tasks and their re-\nlations. Given a dataset with ground-truth labels and a\nloss function deﬁned over those labels, we process images\nthrough a “probe network” and compute an embedding\nbased on estimates of the Fisher information matrix asso-\nciated with the probe network parameters. This provides a\nﬁxed-dimensional embedding of the task that is independent\nof details such as the number of classes and does not require\nany understanding of the class label semantics. We demon-\nstrate that this embedding is capable of predicting task sim-\nilarities that match our intuition about semantic and tax-\nonomic relations between different visual tasks ( e.g., tasks\nbased on classifying different types of plants are similar).\nWe also demonstrate the practical value of this framework\nfor the meta-task of selecting a pre-trained feature extractor\nfor a new task. We present a simple meta-learning frame-\nwork for learning a metric on embeddings that is capable of\npredicting which feature extractors will perform well. Se-\nlecting a feature extractor with task embedding obtains a\nperformance close to the best available feature extractor,\nwhile costing substantially less than exhaustively training\nand evaluating on all available feature extractors.\n1. Introduction\nThe success of Deep Learning hinges in part on the fact\nthat models learned for one task can be used on other related\ntasks. Yet, no general framework exists to describe and\nlearn relations between tasks. We introduce the TASK 2VEC\nembedding, a technique to represent tasks as elements of a\nvector space based on the Fisher Information Matrix. The\nnorm of the embedding correlates with the complexity of\nthe task, while the distance between embeddings capturessemantic similarities between tasks (Fig. 1). When other\nnatural distances are available, such as the taxonomical dis-\ntance in biological classiﬁcation, we ﬁnd that the embed-\nding distance correlates positively with it (Fig. 2). More-\nover, we introduce an asymmetric distance on tasks which\ncorrelates with the transferability between tasks.\nComputation of the embedding leverages a duality be-\ntween network parameters (weights) and outputs (activa-\ntions) in a deep neural network (DNN): Just as the activa-\ntions of a DNN trained on a complex visual recognition task\nare a rich representation of the input images, we show that\nthe gradients of the weights relative to a task-speciﬁc loss\nare a rich representation of the task itself. Speciﬁcally, given\na task deﬁned by a dataset D={(xi,yi)}N\ni=1of labeled\nsamples, we feed the data through a pre-trained reference\nconvolutional neural network which we call “ probe net-\nwork ”, and compute the diagonal Fisher Information Ma-\ntrix (FIM) of the network ﬁlter parameters to capture the\nstructure of the task (Sect. 2). Since the architecture and\nweights of the probe network are ﬁxed, the FIM provides a\nﬁxed-dimensional representation of the task. We show this\nembedding encodes the “difﬁculty” of the task, character-\nistics of the input domain, and which features of the probe\nnetwork are useful to solve it (Sect. 2.1).\nOur task embedding can be used to reason about the\nspace of tasks and solve meta-tasks. As a motivating exam-\nple, we study the problem of selecting the best pre-trained\nfeature extractor to solve a new task. This can be particu-\nlarly valuable when there is insufﬁcient data to train or ﬁne-'
 'relatively few samples. In Fig. 4 we show how the perfor-\nmance of TASK 2VEC varies on a model selection task as the\nnumber of samples varies. At all sample sizes TASK 2VEC is\nclose to the optimum, and improves over selecting a generic\nexpert (ImageNet), both when ﬁne-tuning and when train-\ning only a classiﬁer. We observe that the best choice of ex-\nperts is not affected by the dataset size, and that even with\nfew examples TASK 2VEC is able to ﬁnd the optimal experts.\nChoice of probe network In Table 1 we show that\nDenseNet [15] and ResNet architectures [11] perform sig-\nniﬁcantly better when used as probe networks to compute\ntheTASK 2VEC embedding than a VGG [32] architecture.\n6. Related Work\nTask and Domain embedding. Tasks distinguished by\ntheir domain can be understood simply in terms of image\nstatistics. Due to the bias of different datasets, sometimes a\nbenchmark task may be identiﬁed just by looking at a few\nimages [34]. The question of determining what summary'
 'lem of meta-learning has a long history with much re-\ncent work dedicated to problems such as neural architecture\nsearch and hyper-parameter estimation. Closely related to\nour problem is work on selecting from a library of classi-\nﬁers to solve a new task [33, 2, 20]. Unlike our approach,\nthese usually address the question via land-marking or ac-\ntive testing, in which a few different models are evaluated\nand performance of the remainder estimated by extension.\nThis can be viewed as a problem of completing a matrix\ndeﬁned by performance of each model on each task.\nA similar approach has been taken in computer vision for\nselecting a detector for a new category out of a large library\nof detectors [26, 40, 38].\n7. Discussion\nTASK 2VEC is an efﬁcient way to represent a task, or the\ncorresponding dataset, as a ﬁxed dimensional vector. It has\nseveral appealing properties, in particular its norm corre-\nlates with the test error obtained on the task, and the co-\nsine distance between embeddings correlates with natural\ndistances between tasks, when available, such as the taxo-\nnomic distance for species classiﬁcation, and the ﬁne-tuning\ndistance for transfer learning. Having a representation of\ntasks paves the way for a wide variety of meta-learning\ntasks. In this work, we focused on selection of an expert\nfeature extractor in order to solve a new task, especially\nwhen little training data is present, and showed that using\nTASK 2VEC to select an expert from a collection can sen-\nsibly improve test performance while adding only a small\noverhead to the training process.']","The embedding's norm correlates with task difficulty, while its orientation and distance from other embeddings capture domain characteristics and task similarities.",0.999999999975,1.0,0.9408137277115051,1.0,0.4146304945155103,0.9085219780620413
How does Task2Vec differentiate itself from traditional domain embeddings and other task representation methods?,"Task2Vec differentiates itself by utilizing the task distribution \( p(x,y) \), rather than solely relying on the input distribution \( p(x) \) like traditional domain embeddings. This approach allows Task2Vec to capture the relationships between tasks based on their semantic similarities and complexities, which is not achievable with domain embeddings. Additionally, Task2Vec employs the Fisher Information Matrix to create a fixed-dimensional representation of tasks, making it more efficient for meta-learning tasks such as selecting feature extractors.","['TASK2VEC: Task Embedding for Meta-Learning\nAlessandro Achille\nUCLA and AWS\nachille@cs.ucla.eduMichael Lam\nAWS\nmichlam@amazon.comRahul Tewari\nAWS\ntewarir@amazon.comAvinash Ravichandran\nAWS\nravinash@amazon.com\nSubhransu Maji\nUMass and AWS\nsmmaji@amazon.comCharless Fowlkes\nUCI and AWS\nfowlkec@amazon.comStefano Soatto\nUCLA and AWS\nsoattos@amazon.comPietro Perona\nCaltech and AWS\nperonapp@amazon.com\nAbstract\nWe introduce a method to provide vectorial represen-\ntations of visual classiﬁcation tasks which can be used\nto reason about the nature of those tasks and their re-\nlations. Given a dataset with ground-truth labels and a\nloss function deﬁned over those labels, we process images\nthrough a “probe network” and compute an embedding\nbased on estimates of the Fisher information matrix asso-\nciated with the probe network parameters. This provides a\nﬁxed-dimensional embedding of the task that is independent\nof details such as the number of classes and does not require\nany understanding of the class label semantics. We demon-\nstrate that this embedding is capable of predicting task sim-\nilarities that match our intuition about semantic and tax-\nonomic relations between different visual tasks ( e.g., tasks\nbased on classifying different types of plants are similar).\nWe also demonstrate the practical value of this framework\nfor the meta-task of selecting a pre-trained feature extractor\nfor a new task. We present a simple meta-learning frame-\nwork for learning a metric on embeddings that is capable of\npredicting which feature extractors will perform well. Se-\nlecting a feature extractor with task embedding obtains a\nperformance close to the best available feature extractor,\nwhile costing substantially less than exhaustively training\nand evaluating on all available feature extractors.\n1. Introduction\nThe success of Deep Learning hinges in part on the fact\nthat models learned for one task can be used on other related\ntasks. Yet, no general framework exists to describe and\nlearn relations between tasks. We introduce the TASK 2VEC\nembedding, a technique to represent tasks as elements of a\nvector space based on the Fisher Information Matrix. The\nnorm of the embedding correlates with the complexity of\nthe task, while the distance between embeddings capturessemantic similarities between tasks (Fig. 1). When other\nnatural distances are available, such as the taxonomical dis-\ntance in biological classiﬁcation, we ﬁnd that the embed-\nding distance correlates positively with it (Fig. 2). More-\nover, we introduce an asymmetric distance on tasks which\ncorrelates with the transferability between tasks.\nComputation of the embedding leverages a duality be-\ntween network parameters (weights) and outputs (activa-\ntions) in a deep neural network (DNN): Just as the activa-\ntions of a DNN trained on a complex visual recognition task\nare a rich representation of the input images, we show that\nthe gradients of the weights relative to a task-speciﬁc loss\nare a rich representation of the task itself. Speciﬁcally, given\na task deﬁned by a dataset D={(xi,yi)}N\ni=1of labeled\nsamples, we feed the data through a pre-trained reference\nconvolutional neural network which we call “ probe net-\nwork ”, and compute the diagonal Fisher Information Ma-\ntrix (FIM) of the network ﬁlter parameters to capture the\nstructure of the task (Sect. 2). Since the architecture and\nweights of the probe network are ﬁxed, the FIM provides a\nﬁxed-dimensional representation of the task. We show this\nembedding encodes the “difﬁculty” of the task, character-\nistics of the input domain, and which features of the probe\nnetwork are useful to solve it (Sect. 2.1).\nOur task embedding can be used to reason about the\nspace of tasks and solve meta-tasks. As a motivating exam-\nple, we study the problem of selecting the best pre-trained\nfeature extractor to solve a new task. This can be particu-\nlarly valuable when there is insufﬁcient data to train or ﬁne-'
 'Task embedding for iMaterialist In Fig. 1 we show a\nt-SNE visualization of the embedding for iMaterialist and\niNaturalist tasks. Task embedding yields interpretable re-\nsults: Tasks that are correlated in the dataset, such as binary\nclasses corresponding to the same categorical attribute, may\nend up far away from each other and close to other tasks that\nare semantically more similar ( e.g., the jeans category task\nis close to the ripped attribute and the denim material). This\nis reﬂected in the mixture of colors of semantically related\nnearby tasks, showing non-trivial grouping.\nWe also compare the TASK 2VEC embedding with a do-\nmain embedding baseline, which only exploits the input\ndistribution p(x)rather than the task distribution p(x,y).\nWhile some tasks are highly correlated with their domain\n(e.g., tasks from iNaturalist), other tasks differ only on the\nlabels ( e.g., all the attribute tasks of iMaterialist, which\nshare the same clothes domain). Accordingly, the domain'
 'lem of meta-learning has a long history with much re-\ncent work dedicated to problems such as neural architecture\nsearch and hyper-parameter estimation. Closely related to\nour problem is work on selecting from a library of classi-\nﬁers to solve a new task [33, 2, 20]. Unlike our approach,\nthese usually address the question via land-marking or ac-\ntive testing, in which a few different models are evaluated\nand performance of the remainder estimated by extension.\nThis can be viewed as a problem of completing a matrix\ndeﬁned by performance of each model on each task.\nA similar approach has been taken in computer vision for\nselecting a detector for a new category out of a large library\nof detectors [26, 40, 38].\n7. Discussion\nTASK 2VEC is an efﬁcient way to represent a task, or the\ncorresponding dataset, as a ﬁxed dimensional vector. It has\nseveral appealing properties, in particular its norm corre-\nlates with the test error obtained on the task, and the co-\nsine distance between embeddings correlates with natural\ndistances between tasks, when available, such as the taxo-\nnomic distance for species classiﬁcation, and the ﬁne-tuning\ndistance for transfer learning. Having a representation of\ntasks paves the way for a wide variety of meta-learning\ntasks. In this work, we focused on selection of an expert\nfeature extractor in order to solve a new task, especially\nwhen little training data is present, and showed that using\nTASK 2VEC to select an expert from a collection can sen-\nsibly improve test performance while adding only a small\noverhead to the training process.'
 'relatively few samples. In Fig. 4 we show how the perfor-\nmance of TASK 2VEC varies on a model selection task as the\nnumber of samples varies. At all sample sizes TASK 2VEC is\nclose to the optimum, and improves over selecting a generic\nexpert (ImageNet), both when ﬁne-tuning and when train-\ning only a classiﬁer. We observe that the best choice of ex-\nperts is not affected by the dataset size, and that even with\nfew examples TASK 2VEC is able to ﬁnd the optimal experts.\nChoice of probe network In Table 1 we show that\nDenseNet [15] and ResNet architectures [11] perform sig-\nniﬁcantly better when used as probe networks to compute\ntheTASK 2VEC embedding than a VGG [32] architecture.\n6. Related Work\nTask and Domain embedding. Tasks distinguished by\ntheir domain can be understood simply in terms of image\nstatistics. Due to the bias of different datasets, sometimes a\nbenchmark task may be identiﬁed just by looking at a few\nimages [34]. The question of determining what summary']","Unlike domain embeddings that focus on input data distribution, Task2Vec uniquely represents tasks based on the joint input-output distribution's structure, offering a novel perspective on task similarity and complexity.",0.999999999975,1.0,0.9890351428499445,1.0,0.8775267802701304,0.9386785496519504
How does Task2Vec ensure its task embeddings are invariant to the number of classes and label semantics within a dataset?,"Task2Vec ensures its task embeddings are invariant to the number of classes and label semantics by using the Fisher Information Matrix (FIM) associated with the parameters of a fixed probe network. This approach computes a fixed-dimensional embedding that captures the structure of the task without relying on specific details such as class labels. Consequently, the embeddings focus on the task characteristics rather than the specific labels, allowing for a consistent representation across different task configurations.","['TASK2VEC: Task Embedding for Meta-Learning\nAlessandro Achille\nUCLA and AWS\nachille@cs.ucla.eduMichael Lam\nAWS\nmichlam@amazon.comRahul Tewari\nAWS\ntewarir@amazon.comAvinash Ravichandran\nAWS\nravinash@amazon.com\nSubhransu Maji\nUMass and AWS\nsmmaji@amazon.comCharless Fowlkes\nUCI and AWS\nfowlkec@amazon.comStefano Soatto\nUCLA and AWS\nsoattos@amazon.comPietro Perona\nCaltech and AWS\nperonapp@amazon.com\nAbstract\nWe introduce a method to provide vectorial represen-\ntations of visual classiﬁcation tasks which can be used\nto reason about the nature of those tasks and their re-\nlations. Given a dataset with ground-truth labels and a\nloss function deﬁned over those labels, we process images\nthrough a “probe network” and compute an embedding\nbased on estimates of the Fisher information matrix asso-\nciated with the probe network parameters. This provides a\nﬁxed-dimensional embedding of the task that is independent\nof details such as the number of classes and does not require\nany understanding of the class label semantics. We demon-\nstrate that this embedding is capable of predicting task sim-\nilarities that match our intuition about semantic and tax-\nonomic relations between different visual tasks ( e.g., tasks\nbased on classifying different types of plants are similar).\nWe also demonstrate the practical value of this framework\nfor the meta-task of selecting a pre-trained feature extractor\nfor a new task. We present a simple meta-learning frame-\nwork for learning a metric on embeddings that is capable of\npredicting which feature extractors will perform well. Se-\nlecting a feature extractor with task embedding obtains a\nperformance close to the best available feature extractor,\nwhile costing substantially less than exhaustively training\nand evaluating on all available feature extractors.\n1. Introduction\nThe success of Deep Learning hinges in part on the fact\nthat models learned for one task can be used on other related\ntasks. Yet, no general framework exists to describe and\nlearn relations between tasks. We introduce the TASK 2VEC\nembedding, a technique to represent tasks as elements of a\nvector space based on the Fisher Information Matrix. The\nnorm of the embedding correlates with the complexity of\nthe task, while the distance between embeddings capturessemantic similarities between tasks (Fig. 1). When other\nnatural distances are available, such as the taxonomical dis-\ntance in biological classiﬁcation, we ﬁnd that the embed-\nding distance correlates positively with it (Fig. 2). More-\nover, we introduce an asymmetric distance on tasks which\ncorrelates with the transferability between tasks.\nComputation of the embedding leverages a duality be-\ntween network parameters (weights) and outputs (activa-\ntions) in a deep neural network (DNN): Just as the activa-\ntions of a DNN trained on a complex visual recognition task\nare a rich representation of the input images, we show that\nthe gradients of the weights relative to a task-speciﬁc loss\nare a rich representation of the task itself. Speciﬁcally, given\na task deﬁned by a dataset D={(xi,yi)}N\ni=1of labeled\nsamples, we feed the data through a pre-trained reference\nconvolutional neural network which we call “ probe net-\nwork ”, and compute the diagonal Fisher Information Ma-\ntrix (FIM) of the network ﬁlter parameters to capture the\nstructure of the task (Sect. 2). Since the architecture and\nweights of the probe network are ﬁxed, the FIM provides a\nﬁxed-dimensional representation of the task. We show this\nembedding encodes the “difﬁculty” of the task, character-\nistics of the input domain, and which features of the probe\nnetwork are useful to solve it (Sect. 2.1).\nOur task embedding can be used to reason about the\nspace of tasks and solve meta-tasks. As a motivating exam-\nple, we study the problem of selecting the best pre-trained\nfeature extractor to solve a new task. This can be particu-\nlarly valuable when there is insufﬁcient data to train or ﬁne-'
 'Task embedding for iMaterialist In Fig. 1 we show a\nt-SNE visualization of the embedding for iMaterialist and\niNaturalist tasks. Task embedding yields interpretable re-\nsults: Tasks that are correlated in the dataset, such as binary\nclasses corresponding to the same categorical attribute, may\nend up far away from each other and close to other tasks that\nare semantically more similar ( e.g., the jeans category task\nis close to the ripped attribute and the denim material). This\nis reﬂected in the mixture of colors of semantically related\nnearby tasks, showing non-trivial grouping.\nWe also compare the TASK 2VEC embedding with a do-\nmain embedding baseline, which only exploits the input\ndistribution p(x)rather than the task distribution p(x,y).\nWhile some tasks are highly correlated with their domain\n(e.g., tasks from iNaturalist), other tasks differ only on the\nlabels ( e.g., all the attribute tasks of iMaterialist, which\nshare the same clothes domain). Accordingly, the domain'
 'lem of meta-learning has a long history with much re-\ncent work dedicated to problems such as neural architecture\nsearch and hyper-parameter estimation. Closely related to\nour problem is work on selecting from a library of classi-\nﬁers to solve a new task [33, 2, 20]. Unlike our approach,\nthese usually address the question via land-marking or ac-\ntive testing, in which a few different models are evaluated\nand performance of the remainder estimated by extension.\nThis can be viewed as a problem of completing a matrix\ndeﬁned by performance of each model on each task.\nA similar approach has been taken in computer vision for\nselecting a detector for a new category out of a large library\nof detectors [26, 40, 38].\n7. Discussion\nTASK 2VEC is an efﬁcient way to represent a task, or the\ncorresponding dataset, as a ﬁxed dimensional vector. It has\nseveral appealing properties, in particular its norm corre-\nlates with the test error obtained on the task, and the co-\nsine distance between embeddings correlates with natural\ndistances between tasks, when available, such as the taxo-\nnomic distance for species classiﬁcation, and the ﬁne-tuning\ndistance for transfer learning. Having a representation of\ntasks paves the way for a wide variety of meta-learning\ntasks. In this work, we focused on selection of an expert\nfeature extractor in order to solve a new task, especially\nwhen little training data is present, and showed that using\nTASK 2VEC to select an expert from a collection can sen-\nsibly improve test performance while adding only a small\noverhead to the training process.'
 'relatively few samples. In Fig. 4 we show how the perfor-\nmance of TASK 2VEC varies on a model selection task as the\nnumber of samples varies. At all sample sizes TASK 2VEC is\nclose to the optimum, and improves over selecting a generic\nexpert (ImageNet), both when ﬁne-tuning and when train-\ning only a classiﬁer. We observe that the best choice of ex-\nperts is not affected by the dataset size, and that even with\nfew examples TASK 2VEC is able to ﬁnd the optimal experts.\nChoice of probe network In Table 1 we show that\nDenseNet [15] and ResNet architectures [11] perform sig-\nniﬁcantly better when used as probe networks to compute\ntheTASK 2VEC embedding than a VGG [32] architecture.\n6. Related Work\nTask and Domain embedding. Tasks distinguished by\ntheir domain can be understood simply in terms of image\nstatistics. Due to the bias of different datasets, sometimes a\nbenchmark task may be identiﬁed just by looking at a few\nimages [34]. The question of determining what summary']","By basing the embeddings on the structure of the task via the FIM, Task2Vec creates representations that are independent of class count or label semantics, focusing on the task's intrinsic characteristics.",0.9999999999666667,0.875,0.9941414239669101,1.0,0.9828960813926608,0.9315843255706435
How does Task2Vec handle the variance in data size and complexity across different tasks in its embeddings?,"Task2Vec uses the norm of the task embedding to correlate with the complexity of the task, allowing it to capture variations in data size and task difficulty. It computes a fixed-dimensional representation via the diagonal Fisher Information Matrix, which encodes the task's characteristics regardless of the number of classes. This approach enables Task2Vec to maintain effectiveness for model selection even when faced with varying amounts of training data.","['TASK2VEC: Task Embedding for Meta-Learning\nAlessandro Achille\nUCLA and AWS\nachille@cs.ucla.eduMichael Lam\nAWS\nmichlam@amazon.comRahul Tewari\nAWS\ntewarir@amazon.comAvinash Ravichandran\nAWS\nravinash@amazon.com\nSubhransu Maji\nUMass and AWS\nsmmaji@amazon.comCharless Fowlkes\nUCI and AWS\nfowlkec@amazon.comStefano Soatto\nUCLA and AWS\nsoattos@amazon.comPietro Perona\nCaltech and AWS\nperonapp@amazon.com\nAbstract\nWe introduce a method to provide vectorial represen-\ntations of visual classiﬁcation tasks which can be used\nto reason about the nature of those tasks and their re-\nlations. Given a dataset with ground-truth labels and a\nloss function deﬁned over those labels, we process images\nthrough a “probe network” and compute an embedding\nbased on estimates of the Fisher information matrix asso-\nciated with the probe network parameters. This provides a\nﬁxed-dimensional embedding of the task that is independent\nof details such as the number of classes and does not require\nany understanding of the class label semantics. We demon-\nstrate that this embedding is capable of predicting task sim-\nilarities that match our intuition about semantic and tax-\nonomic relations between different visual tasks ( e.g., tasks\nbased on classifying different types of plants are similar).\nWe also demonstrate the practical value of this framework\nfor the meta-task of selecting a pre-trained feature extractor\nfor a new task. We present a simple meta-learning frame-\nwork for learning a metric on embeddings that is capable of\npredicting which feature extractors will perform well. Se-\nlecting a feature extractor with task embedding obtains a\nperformance close to the best available feature extractor,\nwhile costing substantially less than exhaustively training\nand evaluating on all available feature extractors.\n1. Introduction\nThe success of Deep Learning hinges in part on the fact\nthat models learned for one task can be used on other related\ntasks. Yet, no general framework exists to describe and\nlearn relations between tasks. We introduce the TASK 2VEC\nembedding, a technique to represent tasks as elements of a\nvector space based on the Fisher Information Matrix. The\nnorm of the embedding correlates with the complexity of\nthe task, while the distance between embeddings capturessemantic similarities between tasks (Fig. 1). When other\nnatural distances are available, such as the taxonomical dis-\ntance in biological classiﬁcation, we ﬁnd that the embed-\nding distance correlates positively with it (Fig. 2). More-\nover, we introduce an asymmetric distance on tasks which\ncorrelates with the transferability between tasks.\nComputation of the embedding leverages a duality be-\ntween network parameters (weights) and outputs (activa-\ntions) in a deep neural network (DNN): Just as the activa-\ntions of a DNN trained on a complex visual recognition task\nare a rich representation of the input images, we show that\nthe gradients of the weights relative to a task-speciﬁc loss\nare a rich representation of the task itself. Speciﬁcally, given\na task deﬁned by a dataset D={(xi,yi)}N\ni=1of labeled\nsamples, we feed the data through a pre-trained reference\nconvolutional neural network which we call “ probe net-\nwork ”, and compute the diagonal Fisher Information Ma-\ntrix (FIM) of the network ﬁlter parameters to capture the\nstructure of the task (Sect. 2). Since the architecture and\nweights of the probe network are ﬁxed, the FIM provides a\nﬁxed-dimensional representation of the task. We show this\nembedding encodes the “difﬁculty” of the task, character-\nistics of the input domain, and which features of the probe\nnetwork are useful to solve it (Sect. 2.1).\nOur task embedding can be used to reason about the\nspace of tasks and solve meta-tasks. As a motivating exam-\nple, we study the problem of selecting the best pre-trained\nfeature extractor to solve a new task. This can be particu-\nlarly valuable when there is insufﬁcient data to train or ﬁne-'
 'relatively few samples. In Fig. 4 we show how the perfor-\nmance of TASK 2VEC varies on a model selection task as the\nnumber of samples varies. At all sample sizes TASK 2VEC is\nclose to the optimum, and improves over selecting a generic\nexpert (ImageNet), both when ﬁne-tuning and when train-\ning only a classiﬁer. We observe that the best choice of ex-\nperts is not affected by the dataset size, and that even with\nfew examples TASK 2VEC is able to ﬁnd the optimal experts.\nChoice of probe network In Table 1 we show that\nDenseNet [15] and ResNet architectures [11] perform sig-\nniﬁcantly better when used as probe networks to compute\ntheTASK 2VEC embedding than a VGG [32] architecture.\n6. Related Work\nTask and Domain embedding. Tasks distinguished by\ntheir domain can be understood simply in terms of image\nstatistics. Due to the bias of different datasets, sometimes a\nbenchmark task may be identiﬁed just by looking at a few\nimages [34]. The question of determining what summary'
 'about the structure of the task. We may therefore expect\nthat the distance between two embeddings correlate posi-\ntively with natural metrics on the space of tasks. However,\nthere are two problems in using the Euclidean distance be-\ntween embeddings: the parameters of the network have dif-\nferent scales, and the norm of the embedding is affected by\ncomplexity of the task and the number of samples used to\ncompute the embedding.\nSymmetric TASK 2VEC distance To make the distance\ncomputation robust, we propose to use the cosine distance\nbetween normalized embeddings:\ndsym(Fa,Fb) =dcos(Fa\nFa+Fb,Fb\nFa+Fb)\n,\nwheredcosis the cosine distance, FaandFbare the two\ntask embeddings ( i.e., the diagonal of the Fisher Informa-\ntion computed on the same probe network), and the division\nis element-wise. This is a symmetric distance which we ex-\npect to capture semantic similarity between two tasks. For\nexample, we show in Fig. 2 that it correlates well with the\ntaxonomical distance between species on iNaturalist.\nOn the other hand, precisely for this reason, this distance\nis ill-suited for tasks such as model selection, where the (in-\ntrinsically asymmetric) transfer distance is more relevant.\nAsymmetric TASK 2VEC distance In a ﬁrst approxima-\ntion, that does not consider either the model or the training\nprocedure used, positive transfer between two tasks depends\nboth on the similarity between two tasks and on the com-\nplexity of the ﬁrst. Indeed, pre-training on a general but\ncomplex task such as ImageNet often yields a better result\nthan ﬁne-tuning from a close dataset of comparable com-\nplexity. In our case, complexity can be measured as the dis-\ntance from the trivial embedding. This suggests the follow-\ning asymmetric score, again improperly called a “distance”\ndespite being asymmetric and possibly negative:\ndasym(ta→tb) =dsym(ta,tb)−αdsym(ta,t0),\nwheret0is the trivial embedding, and αis an hyperparam-\neter. This has the effect of bring more complex models\ncloser. The hyper-parameter αcan be selected based on\nthe meta-task. In our experiments, we found that the best\nvalue ofα(α= 0.15when using a ResNet-34 pretrained\non ImageNet as the probe network) is robust to the choice\nof meta-tasks.\n4.MODEL 2VEC: task/model co-embedding\nBy construction, the TASK 2VEC distance ignores details\nof the model and only relies on the task. If we know what\ntask a model was trained on, we can represent the model by\nthe embedding of that task. However, in general we may\nnot have such information ( e.g., black-box models or hand-\nconstructed feature extractors). We may also have multiplemodels trained on the same task with different performance\ncharacteristics. To model the joint interaction between task\nand model ( i.e., architecture and training algorithm), we aim\nto learn a joint embedding of the two.\nWe consider for concreteness the problem of learning\na joint embedding for model selection. In order to em-\nbed models in the task space so that those near a task\nare likely to perform well on that task, we formulate the\nfollowing meta-learning problem: Given kmodels, their\nMODEL 2VEC embedding are the vectors mi=Fi+bi,\nwhereFiis the task embedding of the task used to train\nmodelmi(if available, else we set it to zero), and biis a\nlearned “model bias” that perturbs the task embedding to\naccount for particularities of the model. We learn biby opti-\nmizing ak-way cross entropy loss to predict the best model\ngiven the task distance (see Supplementary Material):\nL=E[−logp(m|dasym(t,m0),...,d asym(t,mk))].\nAfter training, given a novel query task t, we can then pre-\ndict the best model for it as the arg maxidasym(t,mi), that\nis, the model miembedded closest to the query task.\n5. Experiments\nWe test TASK 2VEC on a large collection of tasks and\nmodels, related to different degrees. Our experiments aim to\ntest both qualitative properties of the embedding and its per-\nformance on meta-learning tasks. We use an off-the-shelf'
 '102103104\nNumber of samples-10%0%10%Error relative to brute force\n(lower is better)\nBrute force fixed\nImageNet fixed\nTask2Vec fixedImageNet finetune\nTask2Vec finetuneFigure 4: TASK 2VEC improves results at different\ndataset sizes and training conditions: Performance of\nmodel selection on a subset of 4 tasks as a function of\nthe number of samples available to train relative to opti-\nmal model selection (dashed orange). Training a classiﬁer\non the feature extractor selected by TASK 2VEC (solid red) is\nalways better than using a generic ImageNet feature extrac-\ntor (dashed red). The same holds when allowed to ﬁne-tune\nthe feature extractor (blue curves). Also notice that in the\nlow-data regime ﬁne-tuning the ImageNet feature extractor\nis more expensive and has a worse performance than accu-\nrately selecting a good ﬁxed feature extractor.\nProbe network Top-10 All\nChance +13.95% +59.52%\nVGG-13 +4.82% +38.03%\nDenseNet-121 +0.30% +10.63%\nResNet-13 +0.00% +9.97%\nTable 1: Choice of probe network. Mean relative error\nincrease over the ground-truth optimum on the iNat+CUB\nmeta-task for different choices of the probe-network. We\nalso report the performance on the top 10 tasks with more\nsamples to show how data size affect different architectures.\nembedding recovers similar clusters on iNaturalist. How-\never, on iMaterialst domain embedding collapses all tasks\nto a single uninformative cluster (not a single point due to\nslight noise in embedding computation).\nTask Embedding encodes task difﬁculty The scatter-\nplot in Fig. 3 compares the norm of embedding vectors vs.\nperformance of the best expert (or task speciﬁc model for\ncases where we have the diagonal computed). As shown\nanalytically for the two-layers model, the norm of the task\nembedding correlates with the complexity of the task also\non real tasks and architectures.\n5.2. Model Selection\nGiven a task, our aim is to select an expert feature extrac-\ntor that maximizes the classiﬁcation performance on that\ntask. We propose two strategies: (1) embed the task andselect the feature extractor trained on the most similar task,\nand (2) jointly embed the models and tasks, and select a\nmodel using the learned metric (see Section 4). Notice that\n(1) does not use knowledge of the model performance on\nvarious tasks, which makes it more widely applicable but\nrequires we know what task a model was trained for and\nmay ignore the fact that models trained on slightly differ-\nent tasks may still provide an overall better feature extrac-\ntor (for example by over-ﬁtting less to the task they were\ntrained on).\nIn Table 2 we compare the overall results of the various\nproposed metrics on the model selection meta-tasks. On\nboth the iNat+CUB and Mixed meta-tasks, the Asymmetric\nTASK 2VEC model selection is close to the ground-truth op-\ntimal, and signiﬁcantly improves over both chance, and over\nusing an generic ImageNet expert. Notice that our method\nhasO(1)complexity, while searching over a collection of\nNexperts isO(N).\nError distribution In Fig. 3 we show in detail the error\ndistribution of the experts on multiple tasks. It is interesting\nto notice that the classiﬁcation error obtained using most ex-\nperts clusters around some mean value, and little improve-\nment is observed over using a generic expert. On the other\nhand, a few optimal experts can obtain a largely better per-\nformance on the task than a generic expert. This conﬁrms\nthe importance of having access to a large collection of ex-\nperts when solving a new task, especially if few training\ndata are available. But this collection can only be efﬁciently\nexploited if an algorithm is given to efﬁciently ﬁnd one of\nthe few experts for the task, which we propose.\nDependence on task dataset size Finding experts is es-\npecially important when the task we are interested in has\nrelatively few samples. In Fig. 4 we show how the perfor-\nmance of TASK 2VEC varies on a model selection task as the\nnumber of samples varies. At all sample sizes TASK 2VEC is']","Through robust Fisher computation, Task2Vec accommodates variance in data size and complexity, ensuring embeddings remain informative by adjusting for irregularities in the loss landscape.",0.999999999975,1.0,0.9353412308781252,1.0,,0.9257324670728113
"How does GLM-130B's architecture differ from traditional GPT-style models, and what are its key features?","GLM-130B adopts a bidirectional architecture using the General Language Model (GLM) algorithm, contrasting with the unidirectional approach of traditional GPT-style models. Its key features include a bilingual capability supporting both English and Chinese, a bidirectional attention mechanism, and the use of an autoregressive blank infilling objective for training. Additionally, it employs strategies like embedding gradient shrink to enhance training stability.","['Published as a conference paper at ICLR 2023\nGPT-3 175BPaLM 540BBLOOM 176BOPT-175BGLM-130BLanguage Ability EvaluationBias & Toxicity Evaluation\nFigure 1: A summary of the performance evaluation and ethical studies.\nTable 1: A comparison between GLM-130B and other 100B-scale LLMs and PaLM 540B. (LN:\nlayer norm.; FPF: floating-point format; MIP: multi-task instruction pre-training; CN : Chinese)\nArchitecture & Data Training Inference\nModelOpen-\nsource Objective LN Major Lang. FPF Stabilization Quantization GPU Needed\nGPT-3 175B × English FP16 undisclosed undisclosed undisclosed\nOPT-175B ✓ English FP16 Manual Adjusting INT8 8 ×3090\nBLOOM-176B ✓GPT Pre-LN\nMulti-lingual BF16 Embedding Norm INT8 8 ×3090\nPaLM 540B × GPT Pre-LN English BF16 Manual Adjusting undisclosed undisclosed\nGLM-130B ✓GLM (Blank\nInfilling & MIP)Deep-\nNormBilingual\n(EN & CN)FP16Embedding\nGradient ShrinkINT44×3090 or\n8×1080 Ti\nIn this work, we introduce the pre-training of a 100B-scale model—GLM-130B, in terms of engi-\nneering efforts, model design choices, training strategies for efficiency and stability, and quantization\nfor affordable inference. As it has been widely realized that it is computationally unaffordable to\nempirically enumerate all possible designs for training 100B-scale LLMs, we present not only the\nsuccessful part for training GLM-130B but also many of the failed options and lessons learned.\nParticularly, the training stability is the decisive factor in the success of training models of such a\nscale. Different from practices such as manually adjusting learning rates in OPT-175B and using\nembedding norm in the sacrifice of performance in BLOOM-176B, we experiment with various op-\ntions and find the strategy of embedding gradient shrink can significantly stabilize the training of\nGLM-130B.\nSpecifically, GLM-130B is a bilingual (English and Chinese) bidirectional dense model with 130 bil-\nlion parameters, pre-trained over 400 billion tokens on a cluster of 96 NVIDIA DGX-A100 (8 ×40G)\nGPU nodes between May 6 and July 3, 2022. Instead of using the GPT-style architecture, we adopt\nthe General Language Model (GLM) algorithm (Du et al., 2022) to leverage its bidirectional at-\ntention advantage and autoregressive blank infilling objective. Table 1 summarizes the comparison\nbetween GLM-130B, GPT-3 and another two open-source efforts—OPT-175B and BLOOM-176B,\nas well as PaLM 540B (Chowdhery et al., 2022)—a 4 ×larger model—as a reference.\nAltogether, the conceptual uniqueness and engineering efforts enable GLM-130B to exhibit perfor-\nmance that surpasses the level of GPT-3 on a wide range of benchmarks (in total 112 tasks) and also\noutperforms PaLM 540B in many cases, while outperformance over GPT-3 has not been observed in\nOPT-175B and BLOOM-176B (Cf. Figure 1 left). For zero-shot performance, GLM-130B is better\nthan GPT-3 175B (+5.0%), OPT-175B (+6.5%), and BLOOM-176B (+13.0%) on LAMBADA (Pa-\nperno et al., 2016), and achieves 3 ×better performance than GPT-3 on Big-bench-lite (Srivastava\net al., 2022). For the 5-shot MMLU (Hendrycks et al., 2021) tasks, it is better than GPT-3 175B\n(+0.9%) and BLOOM-176B (+12.7%). As a bilingual LLM also in Chinese, it offers significantly\nbetter results than ERNIE TITAN 3.0 260B (Wang et al., 2021)—the largest Chinese LLM—on 7\nzero-shot CLUE (Xu et al., 2020) datasets (+24.26%) and 5 zero-shot FewCLUE (Xu et al., 2021)\nones (+12.75%). Importantly, as summarized in Figure 1 right, GLM-130B as an open model is\nassociated with significantly less bias and generation toxicity than its 100B-scale counterparts .\nFinally, we design GLM-130B to empower as many people as possible to conduct 100B-scale LLM\nstudies. First, instead of using 175B+ parameters as OPT and BLOOM, the 130B size is decided be-\ncause such a size supports inference on a single A100 (8 ×40G) server. Second, to further lower the\nGPU requirements, we quantize GLM-130B into INT4 precision without post training while OPT'
 'Published as a conference paper at ICLR 2023\nexact dataset setting in T0 (Sanh et al., 2022) and the information extraction datasets in GLM-130B\nto allow the correct evaluation on some types of tasks (e.g., NLI).\nFigure 14 shows the ablation results. On the 8 datasets we test, we find that the GLM objective\nis a major contributor to the improvement (from GLM (uni) to GLM + MIP (bi)). For exam-\nple, it accounts for 73% improvement in LAMBADA and 90% improvement in MMLU, which\nare very widely adopted challenging benchmarks for LLMs. As for MIP, on some datasets (e.g.,\nWiC, ReCoRD, Hellaswag), MIP may even harm the performance. While for datasets related to text\nsimilarity and coreference (e.g., WSC, BoolQ, ANLI R1), MIP is the main contributor. It is likely\nbecause the text similarity and coreference challenges, which people usually construct intentionally\nto test language models’ ability, are seldom seen in the self-supervised corpus that makes up peo-\nple’s daily written texts. Thus, MIP training mainly helps to bridge the gap between self-supervised\npre-training and these tasks.\nB.10 L ESSONS LEARNED\nLesson 1 (Bidirectional Architecture). The bidirectional-attention GLM is a strong architec-\nture alternative, in addition to GPTs.\nLesson 2 (Platform-aware Configuration). Configure LLMs based on the cluster and parallel\nstrategy used to squeeze hardware potential.\nLesson 3 (Improved Post-LN). Counter-stereotypically, DeepNorm, a type of Post-LN, is the\noption to stabilize GLM-130B.\nLesson 4 (Training Stability Categorization). Unexpected training instability that LLMs\nsuffer from arouses systematically and numerically.\nLesson 5 (Systematical Instability: FP16). Though FP16 induces more instability, it enables\ntraining and inference on diverse platforms.\nLesson 6 (Numerical Instability: Embedding Gradient Shrink). Shrinking embedding\nlayer’s gradient to its 0.1 can solve most numerical instability problems.\nLesson 7 (GLM’s INT4 Quantization Scaling Law). GLM has a unique INT4 weight quan-\ntization scaling law unobserved in GPT-style BLOOM.\nLesson 8 (Future Direction). To create powerful LLMs, the main focus can be on 1) more and\nbetter data, 2) better architectures and pre-training objectives, and 3) more sufficient training.\n30'
 'Published as a conference paper at ICLR 2023\nF A B RIEF HISTORY OF GLM-130B\nThe GLM-130B project16was conceived in Dec. 2021 in a brainstorming meeting at Tsinghua KEG.\nWe firmly believe that it is of value to pre-train a highly accurate language model, in particular for\nboth Chinese and English. Though GPT-3 (Brown et al., 2020) is the pioneer for this effort, it is not\navailable to most people in the world. In addition, it supports English only. We therefore decide to\ninitialize the project GLM-130B. Please note that the WuDao 1.75T model we built last year is a\nsparse model with 480 mixture-of-experts (MoE), rather than a dense one as GPT-3. Our goal then\nis to train a bilingual pre-trained dense model with high accuracy on downstream tasks, and to make\nit open to everyone in the world-anyone, anywhere can download it and use it on a single server with\nappropriate GPUs.\nThe ambitious project soon faced several important challenges:\n•Lack of computational resources : No organization is willing to sponsor such a big project and\nfreely make it public.\n•Lack of a robust pre-training algorithm : Despite GPT-3’s success on English corpus, it is\nunclear how to train a high-accurate bilingual model for both English and Chinese.\n•Lack of fast inference solutions : Since the goal is to have the model public to everyone, we need\nto design fast inference solutions with low resource requirements to run the model.\nFor the pre-training algorithm, we finally chose GLM (Du et al., 2022) due to its high performance\nin practice. We eventually decided to train a GLM model of 130 billion parameters after several\nrounds of discussions and exploration, because such a size makes it possible to run the inference on\na single A100 (40G * 8) server.\nOur first attempt at training the model was in January 2022, shortly after we received a small sponsor\nof GPUs for test running. However, we soon realized that we had significantly underestimated the\ntechnical difficulties of pre-training a model at such a scale (>100B). It seems that pre-training a\nhighly accurate 100B-scale model is quite different from training a 10B-scale one. Due to frequent\nrandom hardware failures, model gradients exploding, unexpected excessive memory usage in the\nalgorithm, debug for the 3D pipeline in the new Megatron and DeepSpeed frameworks, inability to\nrecover from optimizer states, blocked TCP responses between processes, and many many unex-\npected “bugs”, the project was delayed for many times. The Tsinghua PACMAN team gave us a\nhand at this difficult time and together we successfully fixed most of the “bugs”.\nBy March, we were still short on computational resources, but fortunately got a chance to try test\nruns on several other platforms, including Ascend 910, Hygon DCU, NVIDIA, and Sunway. The\nimmediate challenge was for us to adapt our training code to these different platforms, as the under-\nlying operators are quite different. Also, it introduced many new issues: the element-wise operators\nnot supporting fast computation for large-dimension vectors, various issues that hindered conver-\ngence—the large gradient norms of input embeddings, native Post-LN, Pre-LN, and Sandwich-LN,\ndataloader state seeds, and computation precision choices in Softmax and Attention — as well as\nnumerous mistakes we ourselves made. With tremendous help from all of our generous partners, we\nfinally succeeded in making our pre-training algorithms runnable across all the platforms—frankly,\na surprising achievement for this project. The timeline of GLM-130B in Figure 21 covers most of\nthe issues we have encountered and addressed as of this writing.\nOn April 26th, we received a generous computing sponsorship from Zhipu.AI — an AI startup that\naims to teach machines to think like humans. After another week of testing, we finally kicked off\nthe training of the GLM-130B model on its 96 A100 (40G * 8) servers on May 6th. Additionally,'
 'ence. Altogether, it contributes to the high quality of GLM-130B in terms of both language perfor-\nmance on 112 tasks and ethical results on bias and toxicity benchmarks. Our experiences of both\nsuccess and failure are condensed into the lessons for training 100B-scale LLMs, attached in the\nAppendix B.10.\n9']","Unlike GPT-style architectures, GLM-130B is based on the General Language Model (GLM) framework with bidirectional attention and autoregressive blank infilling, enabling more effective context comprehension.",0.9999999999666667,1.0,0.9202126588600018,1.0,0.6999617258401979,0.9536930572069453
How does GLM-130B's performance compare to other 100B-scale models and PaLM 540B across English benchmarks?,"GLM-130B outperforms GPT-3 on a wide range of benchmarks, showing a +5.0% improvement on LAMBADA and achieving 3x better performance than GPT-3 on Big-bench-lite. It also surpasses OPT-175B and BLOOM-176B, with an average performance increase of +0.9% over GPT-3 on 5-shot MMLU tasks. In comparisons with PaLM 540B, GLM-130B demonstrates superior results on many benchmarks, although specific performance metrics against PaLM are not detailed.","['Published as a conference paper at ICLR 2023\nGPT-3 175BPaLM 540BBLOOM 176BOPT-175BGLM-130BLanguage Ability EvaluationBias & Toxicity Evaluation\nFigure 1: A summary of the performance evaluation and ethical studies.\nTable 1: A comparison between GLM-130B and other 100B-scale LLMs and PaLM 540B. (LN:\nlayer norm.; FPF: floating-point format; MIP: multi-task instruction pre-training; CN : Chinese)\nArchitecture & Data Training Inference\nModelOpen-\nsource Objective LN Major Lang. FPF Stabilization Quantization GPU Needed\nGPT-3 175B × English FP16 undisclosed undisclosed undisclosed\nOPT-175B ✓ English FP16 Manual Adjusting INT8 8 ×3090\nBLOOM-176B ✓GPT Pre-LN\nMulti-lingual BF16 Embedding Norm INT8 8 ×3090\nPaLM 540B × GPT Pre-LN English BF16 Manual Adjusting undisclosed undisclosed\nGLM-130B ✓GLM (Blank\nInfilling & MIP)Deep-\nNormBilingual\n(EN & CN)FP16Embedding\nGradient ShrinkINT44×3090 or\n8×1080 Ti\nIn this work, we introduce the pre-training of a 100B-scale model—GLM-130B, in terms of engi-\nneering efforts, model design choices, training strategies for efficiency and stability, and quantization\nfor affordable inference. As it has been widely realized that it is computationally unaffordable to\nempirically enumerate all possible designs for training 100B-scale LLMs, we present not only the\nsuccessful part for training GLM-130B but also many of the failed options and lessons learned.\nParticularly, the training stability is the decisive factor in the success of training models of such a\nscale. Different from practices such as manually adjusting learning rates in OPT-175B and using\nembedding norm in the sacrifice of performance in BLOOM-176B, we experiment with various op-\ntions and find the strategy of embedding gradient shrink can significantly stabilize the training of\nGLM-130B.\nSpecifically, GLM-130B is a bilingual (English and Chinese) bidirectional dense model with 130 bil-\nlion parameters, pre-trained over 400 billion tokens on a cluster of 96 NVIDIA DGX-A100 (8 ×40G)\nGPU nodes between May 6 and July 3, 2022. Instead of using the GPT-style architecture, we adopt\nthe General Language Model (GLM) algorithm (Du et al., 2022) to leverage its bidirectional at-\ntention advantage and autoregressive blank infilling objective. Table 1 summarizes the comparison\nbetween GLM-130B, GPT-3 and another two open-source efforts—OPT-175B and BLOOM-176B,\nas well as PaLM 540B (Chowdhery et al., 2022)—a 4 ×larger model—as a reference.\nAltogether, the conceptual uniqueness and engineering efforts enable GLM-130B to exhibit perfor-\nmance that surpasses the level of GPT-3 on a wide range of benchmarks (in total 112 tasks) and also\noutperforms PaLM 540B in many cases, while outperformance over GPT-3 has not been observed in\nOPT-175B and BLOOM-176B (Cf. Figure 1 left). For zero-shot performance, GLM-130B is better\nthan GPT-3 175B (+5.0%), OPT-175B (+6.5%), and BLOOM-176B (+13.0%) on LAMBADA (Pa-\nperno et al., 2016), and achieves 3 ×better performance than GPT-3 on Big-bench-lite (Srivastava\net al., 2022). For the 5-shot MMLU (Hendrycks et al., 2021) tasks, it is better than GPT-3 175B\n(+0.9%) and BLOOM-176B (+12.7%). As a bilingual LLM also in Chinese, it offers significantly\nbetter results than ERNIE TITAN 3.0 260B (Wang et al., 2021)—the largest Chinese LLM—on 7\nzero-shot CLUE (Xu et al., 2020) datasets (+24.26%) and 5 zero-shot FewCLUE (Xu et al., 2021)\nones (+12.75%). Importantly, as summarized in Figure 1 right, GLM-130B as an open model is\nassociated with significantly less bias and generation toxicity than its 100B-scale counterparts .\nFinally, we design GLM-130B to empower as many people as possible to conduct 100B-scale LLM\nstudies. First, instead of using 175B+ parameters as OPT and BLOOM, the 130B size is decided be-\ncause such a size supports inference on a single A100 (8 ×40G) server. Second, to further lower the\nGPU requirements, we quantize GLM-130B into INT4 precision without post training while OPT'
 'ence. Altogether, it contributes to the high quality of GLM-130B in terms of both language perfor-\nmance on 112 tasks and ethical results on bias and toxicity benchmarks. Our experiences of both\nsuccess and failure are condensed into the lessons for training 100B-scale LLMs, attached in the\nAppendix B.10.\n9'
 'Jurassic-1 GPT-3 GLM-130B\nAvg. BPB 0.650 0.742 0.634Pile. The Pile test-set (Gao et al., 2020) includes a series\nof benchmarks for language modeling. On average, GLM-\n130B performs the best on its 18 shared test sets in terms\nof weighted BPB when compared to GPT-3 and Jurassic-\n1 (Lieber et al., 2021) whose results are directly adopted\nfrom the latter, demonstrating its strong language capability (Cf. Appendix C.4 for details).\n1Results in OPT-175B’s paper are reported as applications to access it have not been approved for months.\n7'
 'Published as a conference paper at ICLR 2023\nF A B RIEF HISTORY OF GLM-130B\nThe GLM-130B project16was conceived in Dec. 2021 in a brainstorming meeting at Tsinghua KEG.\nWe firmly believe that it is of value to pre-train a highly accurate language model, in particular for\nboth Chinese and English. Though GPT-3 (Brown et al., 2020) is the pioneer for this effort, it is not\navailable to most people in the world. In addition, it supports English only. We therefore decide to\ninitialize the project GLM-130B. Please note that the WuDao 1.75T model we built last year is a\nsparse model with 480 mixture-of-experts (MoE), rather than a dense one as GPT-3. Our goal then\nis to train a bilingual pre-trained dense model with high accuracy on downstream tasks, and to make\nit open to everyone in the world-anyone, anywhere can download it and use it on a single server with\nappropriate GPUs.\nThe ambitious project soon faced several important challenges:\n•Lack of computational resources : No organization is willing to sponsor such a big project and\nfreely make it public.\n•Lack of a robust pre-training algorithm : Despite GPT-3’s success on English corpus, it is\nunclear how to train a high-accurate bilingual model for both English and Chinese.\n•Lack of fast inference solutions : Since the goal is to have the model public to everyone, we need\nto design fast inference solutions with low resource requirements to run the model.\nFor the pre-training algorithm, we finally chose GLM (Du et al., 2022) due to its high performance\nin practice. We eventually decided to train a GLM model of 130 billion parameters after several\nrounds of discussions and exploration, because such a size makes it possible to run the inference on\na single A100 (40G * 8) server.\nOur first attempt at training the model was in January 2022, shortly after we received a small sponsor\nof GPUs for test running. However, we soon realized that we had significantly underestimated the\ntechnical difficulties of pre-training a model at such a scale (>100B). It seems that pre-training a\nhighly accurate 100B-scale model is quite different from training a 10B-scale one. Due to frequent\nrandom hardware failures, model gradients exploding, unexpected excessive memory usage in the\nalgorithm, debug for the 3D pipeline in the new Megatron and DeepSpeed frameworks, inability to\nrecover from optimizer states, blocked TCP responses between processes, and many many unex-\npected “bugs”, the project was delayed for many times. The Tsinghua PACMAN team gave us a\nhand at this difficult time and together we successfully fixed most of the “bugs”.\nBy March, we were still short on computational resources, but fortunately got a chance to try test\nruns on several other platforms, including Ascend 910, Hygon DCU, NVIDIA, and Sunway. The\nimmediate challenge was for us to adapt our training code to these different platforms, as the under-\nlying operators are quite different. Also, it introduced many new issues: the element-wise operators\nnot supporting fast computation for large-dimension vectors, various issues that hindered conver-\ngence—the large gradient norms of input embeddings, native Post-LN, Pre-LN, and Sandwich-LN,\ndataloader state seeds, and computation precision choices in Softmax and Attention — as well as\nnumerous mistakes we ourselves made. With tremendous help from all of our generous partners, we\nfinally succeeded in making our pre-training algorithms runnable across all the platforms—frankly,\na surprising achievement for this project. The timeline of GLM-130B in Figure 21 covers most of\nthe issues we have encountered and addressed as of this writing.\nOn April 26th, we received a generous computing sponsorship from Zhipu.AI — an AI startup that\naims to teach machines to think like humans. After another week of testing, we finally kicked off\nthe training of the GLM-130B model on its 96 A100 (40G * 8) servers on May 6th. Additionally,']","GLM-130B surpasses GPT-3 and other 100B-scale models in a range of English benchmarks and performs competitively against PaLM 540B, demonstrating its effectiveness in language understanding.",0.9999999999666667,1.0,0.9230849457026448,1.0,0.532244156973755,0.92897662789502
What record-setting performance did Megatron-LM achieve in terms of parameter count and sustained PetaFLOPs on NVIDIA V100 GPUs?,"Megatron-LM achieved a record-setting performance of scaling the model to 8.3 billion parameters on 512 GPUs, sustaining up to 15.1 PetaFLOPs per second. This demonstrates a scaling efficiency of 76% compared to the single GPU case.","['Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nFigure 1. Model (blue) and model+data (green) parallel FLOPS\nas a function of number of GPUs. Model parallel (blue): up to\n8-way model parallel weak scaling with approximately 1 billion\nparameters per GPU (e.g. 2 billion for 2 GPUs and 4 billion for\n4 GPUs). Model+data parallel (green): similar conﬁguration as\nmodel parallel combined with 64-way data parallel.\na baseline by training a model of 1.2 billion parameters\non a single NVIDIA V100 32GB GPU, that sustains 39\nTeraFLOPs. This is 30% of the theoretical peak FLOPS\nfor a single GPU as conﬁgured in a DGX-2H server, and\nis thus a strong baseline. Scaling the model to 8.3 billion\nparameters on 512 GPUs with 8-way model parallelism,\nwe achieve up to 15.1 PetaFLOPs per second sustained\nover the entire application. This is 76% scaling efﬁciency\ncompared to the single GPU case. Figure 1 shows more\ndetailed scaling results.\nTo analyze the effect of model size scaling on accuracy,\nwe train both left-to-right GPT-2 (Radford et al., 2019) lan-\nguage models as well as BERT (Devlin et al., 2018) bidi-\nrectional transformers and evaluate them on several down-\nstream tasks. We show that the existing BERT architecture\nresults in model degradation as the size increases. We over-\ncome this challenge by rearranging the layer normalization\nand residual connection in the transformer layers and show\nthat with this change, results for the downstream tasks on\ndevelopment sets improve monotonically as the model size\nincreases. In addition, we show that our models achieve\ntest set state of the art (SOTA) results on WikiText103,\ncloze-style prediction accuracy on LAMBADA, and reading\ncomprehension RACE datasets.\nIn summary, our contributions are as follows:\n•We implement a simple and efﬁcient model parallel\napproach by making only a few targeted modiﬁcations\nto an existing PyTorch transformer implementation.\n•We perform an in-depth empirical analysis of our\nmodel and data parallel technique and demonstrate\nup to 76% scaling efﬁciency using 512 GPUs.•We show that careful attention to the placement of\nlayer normalization in BERT-like models is critical to\nachieving increased accuracies as the model grows.\n•We demonstrate that scaling the model size results in\nimproved accuracies for both GPT-2 (studied up to\n8.3 billion parameters) and BERT (studied up to 3.9B\nparameters) models.\n•We showcase that our models achieve state of the art\nresults on test sets: perplexity on WikiText103 (10.8\nppl), accuracy on LAMBADA (66.5%), and accuracy\non RACE (90.9%).\n•We open source our code along with the training\nand evaluation pipelines at https://github .com/\nNVIDIA/Megatron-LM\n2. Background and Challenges\n2.1. Neural Language Model Pretraining\nPretrained language models have become an indispensable\npart of NLP researchers’ toolkits. Leveraging large corpus\npretraining to learn robust neural representations of lan-\nguage is an active area of research that has spanned the\npast decade. Early examples of pretraining and transferring\nneural representations of language demonstrated that pre-\ntrained word embedding tables improve downstream task\nresults compared to word embedding tables learned from\nscratch (Mikolov et al., 2013; Pennington et al., 2014; Turian\net al., 2010). Later work advanced research in this area by\nlearning and transferring neural models that capture contex-\ntual representations of words (Melamud et al., 2016; Mc-\nCann et al., 2017; Peters et al., 2018; Radford et al., 2017;\n2019). Recent parallel work (Ramachandran et al., 2016;\nHoward & Ruder, 2018; Radford et al., 2018; Devlin et al.,\n2018; Liu et al., 2019b; Dai et al., 2019; Yang et al., 2019;\nLiu et al., 2019a; Lan et al., 2019) further builds upon these\nideas by not just transferring the language model to extract\ncontextual word representations, but by also ﬁnetuning the\nlanguage model in an end to end fashion on downstream'
 'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nContext : The 36th International Conference on Machine\nLearning (ICML 2019) will be held in Long Beach, CA,\nUSA from June 10th to June 15th, 2019. The conference\nwill consist of one day of tutorials (June 10), followed\nby three days of main conference sessions (June 11-13),\nfollowed by two days of workshops (June 14-15).\nMegatron-LM : With a broad scope, the conference ad-\ndresses the challenges and opportunities in machine learning\nfor practitioners and researchers. The conference program\nwill feature sessions that cover different machine learning\ntopics, from reinforcement learning to computational biol-\nogy and from AI to deep learning. It is the world’s premier\nforum for exploring and presenting advances in machine\nlearning.\nICML 2019 will offer an impressive range of sessions,\nwhich will be of interest to practitioners, researchers, and\nstudents. Some of the topics we will cover include:\nReinforcement learning\nSoft Computing\nComputational biology\nConvolutional networks\nDeep learning\nStatistical learning theory\nStatistical learning machine\nNatural language processing\nBiology\nAwards\nSince its ﬁrst edition, the conference has grown and evolved.\nToday, it is the premier forum for showcasing state-of-the-\nart research in machine learning. The conference is widely\nrecognized as the preeminent venue for showcasing the\nlatest research in machine learning, and a place where prac-\ntitioners, researchers, and students can exchange ideas and\ninsights.\nThe conference is a peer-reviewed venue that draws its pres-\ntige and inﬂuence from the excellence of the presentations\nand the quality of the invited speakers. With over 50% of all\ntalks selected based on their content by an expert committee,\nit is clear that the conference’s high standards of research\nand quality keep a very high bar.Context : Rescue workers on Tuesday found a ﬂight\nrecorder from the Russian plane that crashed into the Black\nSea over the weekend, the defense ministry said. All 92\npeople aboard the Russian military’s Tu-154 plane are be-\nlieved to have died Sunday morning when it crashed two\nminutes after taking off from the southern Russian city of\nSochi.\nMegatron-LM : A child holds up a portrait of a dead relative\nat a makeshift morgue in a building belonging to a local\ngovernment in the town of Petropavlovsk-Kamchatsky. A\nRussian Defense Ministry Tu-154 plane with 92 people\non board crashed into the Black Sea on Sunday morning\nshortly after taking off from the southern Russian city of\nSochi. (Yuri Kochetkov/EPA)\nAccording to the ministry, the plane was found at a depth\nof more than 1,000 feet and the recorder was damaged.\n”The defense ministry has decided to hand over the recorder\nto the Interstate Aviation Committee, which is the main\ninvestigative body in the air crash,” the statement said.\nThe search for the plane’s black boxes is being hampered by\nstrong winds and rough seas. Investigators have launched a\ncriminal probe into the crash, which ofﬁcials said was likely\ncaused by a technical fault.\nRussian President Vladimir Putin declared Wednesday a\nday of mourning for the victims.\nD. Further Scaling Analysis\nIn this section we study the effect of number of attention\nheads on the scaling results. We also present strong scaling\nresults for our 1.2 billion parameter model.\nD.1. Attention Heads and Scaling\nThis section studies the effect of attention heads on model\nparallel scaling. To this end, we consider the 8.3 billion\nparameter conﬁguration with 8-way model parallelism and\nvary the number of heads from 16 to 32. The results are\npresented in Table 7. As the number of attention heads\nincreases, some of the GEMMS inside the self-attention\nlayer become smaller and also the number of elements in\nthe self attention softmax increases. This results in a slight\ndecrease in scaling efﬁciency. Future research should be\nwary of this hyperparameter to design large transformer'
 'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\ninvestigation that will further test existing deep learning\nhardware and software. To realize this, improvements in\nthe efﬁciency and memory footprint of optimizers will be\nneeded. In addition, training a model with more than 16\nbillion parameters will demand more memory than is avail-\nable within 16 GPUs of a DGX-2H box. For such models, a\nhybrid intra-layer and inter-layer model parallelism along\nwith inter-node model parallelism would be more suitable.\nThree other directions of investigation include (a) pretrain-\ning different model families (XLNet, T5), (b) evaluating per-\nformance of large models across more difﬁcult and diverse\ndownstream tasks (e.g. Generative Question Answering,\nSummarization, and Conversation), and (c) using knowl-\nedge distillation to train small student models from these\nlarge pretrained teacher models.\nReferences\nAbadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z.,\nCitro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M.,\nGhemawat, S., Goodfellow, I., Harp, A., Irving, G., Is-\nard, M., Jia, Y ., Jozefowicz, R., Kaiser, L., Kudlur, M.,\nLevenberg, J., Man ´e, D., Monga, R., Moore, S., Mur-\nray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B.,\nSutskever, I., Talwar, K., Tucker, P., Vanhoucke, V ., Va-\nsudevan, V ., Vi ´egas, F., Vinyals, O., Warden, P., Watten-\nberg, M., Wicke, M., Yu, Y ., and Zheng, X. TensorFlow:\nLarge-scale machine learning on heterogeneous systems,\n2015. URL http://tensorflow .org/ . Software\navailable from tensorﬂow.org.\nBa, J. L., Kiros, J. R., and Hinton, G. E. Layernorm. CoRR ,\nabs/1607.06450, 2016. URL http://arxiv .org/\nabs/1607.06450 .\nChen, C.-C., Yang, C.-L., and Cheng, H.-Y . Efﬁcient and\nrobust parallel dnn training through model parallelism on\nmulti-gpu platform. arXiv:1809.02839 , 2018.\nChen, T., Xu, B., Zhang, C., and Guestrin, C. Train-\ning deep nets with sublinear memory cost. CoRR ,\nabs/1604.06174, 2016. URL http://arxiv .org/\nabs/1604.06174 .\nDai, Z., Yang, Z., Yang, Y ., Carbonell, J. G., Le, Q. V .,\nand Salakhutdinov, R. Transformer-xl: Attentive lan-\nguage models beyond a ﬁxed-length context. CoRR ,\nabs/1901.02860, 2019. URL http://arxiv .org/\nabs/1901.02860 .\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding, 2018.\nGoyal, P., Doll ´ar, P., Girshick, R. B., Noordhuis, P.,\nWesolowski, L., Kyrola, A., Tulloch, A., Jia, Y ., andHe, K. Accurate, large minibatch SGD: training imagenet\nin 1 hour. CoRR , abs/1706.02677, 2017.\nHarlap, A., Narayanan, D., Phanishayee, A., Se-\nshadri, V ., Devanur, N., Ganger, G., and Gibbons, P.\nPipedream: Fast and efﬁcient pipeline parallel dnn train-\ning.arXiv:1806.03377 , 2018.\nHendrycks, D. and Gimpel, K. Bridging nonlinearities\nand stochastic regularizers with gaussian error linear\nunits. CoRR , abs/1606.08415, 2016. URL http:\n//arxiv.org/abs/1606 .08415 .\nHoward, J. and Ruder, S. Fine-tuned language models for\ntext classiﬁcation. CoRR , abs/1801.06146, 2018.\nHuang, Y ., Cheng, Y ., Chen, D., Lee, H., Ngiam, J., Le,\nQ. V ., and Chen, Z. Gpipe: Efﬁcient training of gi-\nant neural networks using pipeline parallelism. CoRR ,\nabs/1811.06965, 2018. URL http://arxiv .org/\nabs/1811.06965 .\nJia, Z., Zaharia, M., and Aiken, A. Beyond data and model\nparallelism for deep neural networks. arXiv:1807.05358 ,\n2018.\nJoshi, M., Chen, D., Liu, Y ., Weld, D. S., Zettlemoyer,\nL., and Levy, O. Spanbert: Improving pre-training by\nrepresenting and predicting spans. arXiv:1907.10529 ,\n2019.\nKeskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy,\nM., and Tang, P. T. P. On large- batch training for deep\nlearning: Generalization gap and sharp minima. ICLR ,\n2017.\nKhandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and\nLewis, M. Generalization through memorization: Nearest\nneighbor language models. arXiv:1911.00172 , 2019.'
 'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\ntions. Our learning rate of 1.5e-4 utilizes a warmup period\nof 3k iterations before following a single cycle cosine decay\nover the remaining 297k iterations. We stop the decay at a\nminimum learning rate of 1e-5.\nFor BERT models, we largely follow the training process\ndescribed in (Lan et al., 2019). We use the original BERT\ndictionary with vocab size of 30,522. In addition, we re-\nplace the next sentence prediction head with sentence order\nprediction as suggested by (Lan et al., 2019) and use whole\nword n-gram masking of (Joshi et al., 2019). For all cases,\nwe set the batch size to 1024 and use a learning rate of 1.0e-\n4 warmed up over 10,000 iterations and decayed linearly\nover 2 million iterations. Other training parameters are kept\nthe same as (Devlin et al., 2018).\n5. Experiments\nAll of our experiments use up to 32 DGX-2H servers (a total\nof 512 Tesla V100 SXM3 32GB GPUs). Our infrastruc-\nture is optimized for multi-node deep learning applications,\nwith 300 GB/sec bandwidth between GPUs inside a server\nvia NVSwitch and 100 GB/sec of interconnect bandwidth\nbetween servers using 8 InﬁniBand adapters per server.\n5.1. Scaling Analysis\nTo test the scalability of our implementation, we consider\nGPT-2 models with four sets of parameters detailed in Table\n1. To have consistent GEMM sizes in the self attention layer,\nthe hidden size per attention head is kept constant at 96\nwhile the number of heads and layers are varied to obtain\nconﬁgurations ranging from 1 billion to 8 billion parameters.\nThe conﬁguration with 1.2 billion parameters ﬁts on a single\nGPU whereas the 8 billion parameter model requires 8-way\nmodel parallelism (8 GPUs). The original vocabulary size\nwas 50,257, however, to have efﬁcient GEMMs for the logit\nlayer, it is beneﬁcial for the per-GPU vocabulary size to\nbe a multiple of 128. Since we study up to 8-way model\nparallelism, we pad the vocabulary such that it is divisible\nby128×8 = 1024 , resulting in a padded vocabulary size\nof 51,200. We study both model and model+data parallel\nscaling. For the model parallel scaling, a ﬁxed batch size of\n8 is used across all conﬁgurations. Data parallel scaling is\nnecessary for training many state of the art models which\ntypically use a much larger global batch size. To this end,\nfor the model+data parallel cases we ﬁx the global batch\nsize to 512 for all experiments which corresponds to 64-way\ndata parallelism.\n5.1.1. M ODEL AND DATA PARALLELISM\nThroughout this section, we will showcase weak scaling\nwith respect to the model parameters for both model parallel\nand model+data parallel cases. Weak scaling is typicallyTable 1. Parameters used for scaling studies. Hidden size per atten-\ntion head is kept constant at 96.\nNumber Number Model Model\nHidden Attention of of parallel +data\nSize heads layers parameters GPUs parallel\n(billions) GPUs\n1536 16 40 1.2 1 64\n1920 20 54 2.5 2 128\n2304 24 64 4.2 4 256\n3072 32 72 8.3 8 512\n100%95%82%77%96%83%79%74%0%20%40%60%80%100%\n1248…64128256512Weak ScalingNumber of GPUSModel ParallelModel + Data Parallel\nFigure 5. Model and model + data parallel weak scaling efﬁciency\nas a function of the number of GPUs.\ndone by scaling the batch-size, however, this approach does\nnot address training large models that do not ﬁt on a single\nGPU and it leads to training convergence degradation for\nlarge batch sizes. In contrast, here we use weak scaling to\ntrain larger models that were not possible otherwise. The\nbaseline for all the scaling numbers is the ﬁrst conﬁguration\n(1.2 billion parameters) in Table 1 running on a single GPU.\nThis is a strong baseline as it achieves 39 TeraFLOPS during\nthe overall training process, which is 30% of the theoretical\npeak FLOPS for a single GPU in a DGX-2H server.\nFigure 5 shows scaling values for both model and\nmodel+data parallelism. We observe excellent scaling num-\nbers in both settings. For example, the 8.3 billion parame-']","Megatron-LM trained models up to 8.3 billion parameters, sustaining 15.1 PetaFLOPs across 512 GPUs with 76% scaling efficiency.",0.7499999999625,0.8,0.9132030511478989,1.0,0.8052434702918687,0.9710086933382893
What computational approach does PAL use to integrate programmatic reasoning within natural language tasks?,"PAL integrates programmatic reasoning by using a large language model (LLM) to read natural language problems and generate programs as intermediate reasoning steps, while offloading the solution step to a Python interpreter. This approach allows the LLM to focus on decomposing the problem into runnable steps, ensuring accurate calculations and logical reasoning by leveraging the interpreter. As a result, PAL achieves higher accuracy in various reasoning tasks compared to traditional methods that rely solely on LLMs.","['PAL: Program-aided Language Models 9\n8. Conclusion\nWe introduce PAL, a new method for natural language rea-\nsoning, using programs as intermediate reasoning steps.\nDifferently from existing LM-based reasoning approaches,\nthe main idea is to ofﬂoad solving and calculating to an\nexternal Python interpreter, instead of using the LLM for\nboth understanding the problem andsolving. This results\nin a ﬁnal answer that is guaranteed to be accurate, given the\ncorrectly predicted programmatic steps. We demonstrate\nthis seamless synergy between an LLM and a Python in-\nterpreter across 13 tasks from BIG-Bench Hard and other\nbenchmarks. In all these benchmarks, PALoutperforms\nlarger LLMs such as PaLM-540 Bwhich use the popular\n“chain-of-thought” method and sets new state-of-the-art ac-\ncuracy on all of them. We believe that these results unlock\nexciting directions for future neuro-symbolic AI reasoners.\nReferences\nAhn, M., Brohan, A., Brown, N., Chebotar, Y ., Cortes, O.,\nDavid, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman,\nK., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B.,\nIrpan, A., Jang, E., Ruano, R. J., Jeffrey, K., Jesmonth,\nS., Joshi, N. J., Julian, R., Kalashnikov, D., Kuang, Y .,\nLee, K.-H., Levine, S., Lu, Y ., Luu, L., Parada, C., Pastor,\nP., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D.,\nSermanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke,\nV ., Xia, F., Xiao, T., Xu, P., Xu, S., Yan, M., and Zeng,\nA. Do as I Can, not as I Say: Grounding Language in\nRobotic Affordances. arXiv preprint arXiv:2204.01691 ,\n2022.\nAmini, A., Gabriel, S., Lin, S., Koncel-Kedziorski, R., Choi,\nY ., and Hajishirzi, H. MathQA: Towards Interpretable\nMath Word Problem Solving with Operation-Based For-\nmalisms. In ACL, 2019.\nAndor, D., He, L., Lee, K., and Pitler, E. Giving bert a cal-\nculator: Finding operations and arguments with reading\ncomprehension. arXiv preprint arXiv:1909.00109 , 2019.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,\nHenighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,\nJ., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,\nGray, S., Chess, B., Clark, J., Berner, C., McCandlish,\nS., Radford, A., Sutskever, I., and Amodei, D. Language\nModels are Few-Shot Learners. In NeurIPS , 2020.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y ., Joseph, N., Brockman,\nG., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf,\nH., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N.,\nPavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter,C., Tillet, P., Such, F. P., Cummings, D., Plappert, M.,\nChantzis, F., Barnes, E., Herbert-V oss, A., Guss, W. H.,\nNichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin,\nI., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr,\nA. N., Leike, J., Achiam, J., Misra, V ., Morikawa, E.,\nRadford, A., Knight, M., Brundage, M., Murati, M.,\nMayer, K., Welinder, P., McGrew, B., Amodei, D., Mc-\nCandlish, S., Sutskever, I., and Zaremba, W. Evaluating\nLarge Language Models Trained on Code. arXiv preprint\narXiv:2107.03374 , 2021a.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y ., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374 , 2021b.\nChen, W., Ma, X., Wang, X., and Cohen, W. W. Program\nof thoughts prompting: Disentangling computation from\nreasoning for numerical reasoning tasks. arXiv preprint\narXiv:2211.12588 , 2022.\nCheng, Z., Xie, T., Shi, P., Li, C., Nadkarni, R., Hu, Y .,\nXiong, C., Radev, D., Ostendorf, M., Zettlemoyer, L.,\nSmith, N. A., and Yu, T. Binding language models in\nsymbolic languages. arXiv preprint arXiv:2210.02875 ,\n2022.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton,\nC., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,'
 'PAL: Program-aided Language Models\nLuyu Gao* 1Aman Madaan* 1Shuyan Zhou* 1Uri Alon1Pengfei Liu1 2Yiming Yang1Jamie Callan1\nGraham Neubig1 2\n{luyug,amadaan,shuyanzh,ualon,pliu3,yiming,callan,gneubig }@cs.cmu.edu\nAbstract\nLarge language models (LLMs) have recently\ndemonstrated an impressive ability to perform\narithmetic and symbolic reasoning tasks, when\nprovided with a few examples at test time (“few-\nshot prompting”). Much of this success can be\nattributed to prompting methods such as “chain-\nof-thought”, which employ LLMs for both under-\nstanding the problem description by decomposing\nit into steps, as well as solving each step of the\nproblem. While LLMs seem to be adept at this\nsort of step-by-step decomposition, LLMs often\nmake logical and arithmetic mistakes in the solu-\ntion part, even when the problem is decomposed\ncorrectly. In this paper, we present Program-\nAided Language models ( PAL): a novel approach\nthat uses the LLM to read natural language prob-\nlems and generate programs as the intermediate\nreasoning steps, but ofﬂoads the solution step to a\nruntime such as a Python interpreter. With PAL,\ndecomposing the natural language problem into\nrunnable steps remains the only learning task for\nthe LLM, while solving is delegated to the inter-\npreter. We demonstrate this synergy between a\nneural LLM and a symbolic interpreter across 13\nmathematical, symbolic, and algorithmic reason-\ning tasks from BIG-Bench Hard and other bench-\nmarks. In all these natural language reasoning\ntasks, generating code using an LLM and rea-\nsoning using a Python interpreter leads to more\naccurate results than much larger models. For ex-\nample, PALusing CODEX achieves state-of-the-\nart few-shot accuracy on the GSM 8Kbenchmark\nof math word problems, surpassing PaLM-540 B\nwhich uses chain-of-thought by absolute 15% top-\n1. Our code and data are publicly available at\nhttp://reasonwithpal.com .\n*The ﬁrst three authors contributed equally.1Language Tech-\nnologies Institute, Carnegie Mellon University, USA2Inspired\nCognition, USA.1. Introduction\nUntil as recently as two years ago, reasoning was considered\nto be one of the most signiﬁcant challenges that large lan-\nguage models (LLMs) had not yet overcome (Marcus, 2018;\n2020; Garcez & Lamb, 2020). Recently, LLMs have shown\nimpressive success on a wide range of tasks, including com-\nmonsense (Wei et al., 2021; Sanh et al., 2021; Madaan\net al., 2022), mathematical (Lewkowycz et al., 2022; Wu\net al., 2022; Mishra et al., 2022), and symbolic reason-\ning (Yao et al., 2022; Ahn et al., 2022), using few-shot\nprompting (Brown et al., 2020).\nThis process has been accelerated by methods that require\nLLMs to generate their explicit reasoning steps, such as\n“chain-of-thought” (Wei et al., 2022), “scratchpads” (Nye\net al., 2021), and “least-to-most” (Zhou et al., 2022) prompt-\ning. In particular, the widely used chain-of-thought ( COT)\nmethod presents the model with the explicit intermediate\nsteps that are required to reach the ﬁnal answer. Then, the\nmodel is expected to apply a similar decomposition to the ac-\ntual test example, and consecutively reach an accurate ﬁnal\nanswer (Ling et al., 2017; Amini et al., 2019). Nevertheless,\nwhile LLMs can decompose natural language problems into\nsteps and perform simple arithmetic operations, their perfor-\nmance falls dramatically when dealing with complex arith-\nmetic (Hendrycks et al., 2021; Madaan & Yazdanbakhsh,\n2022) or large numbers (Nogueira et al., 2021; Qian et al.,\n2022). In fact, even when ﬁne-tuning a PaLM-based model\non 164B tokens of explicit mathematical content, its two\nmost common failures are reportedly “incorrect reasoning”\nand “incorrect calculation” (Lewkowycz et al., 2022).\nIn this paper, we propose Program- Aided Language\nmodel ( PAL): a novel method that uses an LLM to read\nnatural language problems and generate programs as rea-\nsoning steps, but ofﬂoads the solution step to a Python inter-'
 'model ( PAL): a novel method that uses an LLM to read\nnatural language problems and generate programs as rea-\nsoning steps, but ofﬂoads the solution step to a Python inter-\npreter, as illustrated in Figure 1. This ofﬂoading leverages an\nLLM that can decompose a natural language problem into\nprogrammatic steps, which is fortunately available using\ncontemporary state-of-the-art LLMs that are pre-trained on\nboth natural language and programming languages (Brown\net al., 2020; Chen et al., 2021a; Chowdhery et al., 2022).\nWhile natural language understanding and decompositionarXiv:2211.10435v2  [cs.CL]  27 Jan 2023'
 'PAL: Program-aided Language Models 8\nColored Objects Date Penguins60708090100\n84.4\n64.879.295.2\n76.293.391.1\n69.191.3\n79.9\n63.491.9COT PAL PAL−comment PAL−var\n−comment\nFigure 9: Ablation study of PALprompt formats. We consider the original PALprompt, it with natural language comments\nremoved ( PAL−comment ), and further variable names replaced with random character ( PAL−var\n−comment ). As a reference, we also\nshow the C OT performance (blue).\n2021; Reif et al., 2021; Wei et al., 2021; Sanh et al., 2021)\nto code-generation (Chen et al., 2021b). Methods such as\nchain-of-thought prompting ( COT) have further unlocked a\nvariety of reasoning tasks, boosting the performance of mod-\nels on a variety of benchmarks. Nevertheless, all previous\napproaches suffer from inaccuracy in arithmetic calculation\nand incorrect reasoning (Lewkowycz et al., 2022; Hendrycks\net al., 2021; Madaan & Yazdanbakhsh, 2022). PALavoids\nthese problems by ofﬂoading the calculation and some of\nthe reasoning to a Python interpreter, which is correct by\nconstruction, given the right program. Further, not only\nthatPALcan improve the standard chain-of-thought, it can\nimprove least-to-most prompting (Zhou et al., 2022) as well,\nas we show in Appendix I.\nLMs with external tools Several prior works have\nequipped neural models with specialized modules. For ex-\nample, Cobbe et al. (2021) employ a calculator for arith-\nmetic operations as a post hoc processing, and Demeter\n& Downey (2020) add specialized modules for generating\ncities and dates. Unlike these works, PALgenerates code\nfor a Python interpreter, which is general enough to handle\nboth arithmetic calculations and dates, without specialized\nmodules and ad-hoc ﬁxes. Chowdhery et al. (2022) and Wei\net al. (2022) have also experimented with external calcula-\ntors; however, the calculator had improved Codex by only\n2.3% (absolute) on GSM 8Kand improved PaLM-540 Bby\n1.7%, while PALimproves Codex by 6.4% on the same\nbenchmark (Section 5.1). Similarly to our work, Chowd-\nhery et al. (2022) have also experimented with generating\nPython code for solving the GSM 8Kbenchmark, but their\nexperiments resulted in lower accuracy than the standard\nPaLM-540 Bthat uses chain-of-thought. Pi et al. (2022)\npretrain the model on execution results of random expres-\nsions on a calculator, instead of using the solver at test time\nas well. While their model can hypothetically perform arith-\nmetic better than other pretrained LMs, their results on the\nSV AMP benchmark are much lower: 57.4% using a T5-11Bmodel, while PAL achieves 79.4% on the same benchmark\nwithout any specialized pretraining.\nShortly after a preprint of our work was submitted to arXiv,\nanother related work on “program of thought prompting”\n(Chen et al., 2022) was also submitted to arXiv. Their\nmethod is conceptually similar to ours, but PoT (1) only\ndemonstrates efﬁcacy on mathematical problems, whereas\nwe demonstrate gains on symbolic and algorithmic bench-\nmarks as well, and (2) chose benchmark-speciﬁc prompt\nexamples, while we used the same prompt examples as pre-\nvious work, to disentangled the beneﬁt of our approach from\nthe beneﬁt of the choice of examples.\nSemantic parsing Our work can also be seen as a very\ngeneral form of semantic parsing, where instead of parsing\ninto strict domain-speciﬁc languages, the model generates\nfree-form Python code. Some works constrain the decoder\nusing a Context-Free Grammar (CFG) to generate a domain-\nspeciﬁc meaning representation (Shin & Van Durme, 2021)\nor a canonical utterance, which can be converted to a Lisp-\nlike meaning representation (Shin et al., 2021). In contrast,\nPALdoes not require any constraining or domain-speciﬁc\nrepresentations other than Python code. Further, LMs that\nwere pretrained on Python are abundant compared to other\ndomain-speciﬁc languages, making Python code a much\nmore preferable representation. Andor et al. (2019) generate\ntask-speciﬁc arithmetic operations for reading comprehen-']","PAL leverages large language models (LLMs) to generate programs as intermediate reasoning steps for natural language problems, offloading solution steps to a Python interpreter, ensuring accuracy by focusing LLMs on decomposing problems into executable code.",0.999999999975,0.8888888888888888,0.9480081520966505,1.0,0.9844302623332153,0.937721049332861
How does PAL handle tasks involving large numbers differently than models relying on chain-of-thought methodologies?,"PAL handles tasks involving large numbers by using multi-step programming and code-based reasoning chains, which provide a clear, systematic breakdown of the reasoning process. In contrast, models relying on chain-of-thought methodologies may show only modest gains in performance when prompted with larger numbers, indicating that they struggle with consistent and accurate reasoning in these contexts. This difference suggests that the explicit nature of programming in PAL allows for more robust handling of complex numerical tasks.","['PAL: Program-aided Language Models 14\nthat even with a prompt that matches the numbers, there are only modest gains in performance. These results show that the\ngains achieved by using code-based reasoning chains may not be achieved simply by using better few-shot examples for\nCOT.\nRegular Prompt Prompt with Larger Numbers\nCOT 23.3 23.8\nTable 5: GSM-hard results, when the prompts also had examples of larger numbers.\nSuccinct Code The programs used in few-shot examples by PALare multi-step, and show a step-by-step breakdown of\nthe reasoning process. Is this breakdown necessary? Alternatively, can we return a single line expression (see Figure 11b) to\ncalculate the result? Results in Table 6 ( 4throw) shows that is not the case. With single-line expressions, the performance of\nPAL falls to the level of direct prompting.\nGenerating the answer directly PALﬁrst generates a reasoning chain in the form of a Python program, and passes the\ngenerated program to a runtime to obtain an answer. Is PALbetter only because of the program-style intermediate reasoning\nchains, or are the improvements derived from ofﬂoading execution to the Python runtime? To investigate this, we experiment\nwith a variant that forces the LLM to generate the answer after generating the reasoning chain (Figure 11e). This setting\ncompels the LLM to condition on the generated code-based reasoning to generate an answer, simulating the runtime. The\nresults in Table 6 ( 5throw) show that the solve rate drops to near DIRECT levels. This reinforces our hypothesis that while\ncurrent LLMs can be excellent at specifying a high-level plan to solve a task—they are still incapable of executing them.\nAblation Solve Rate\nDIRECT (no intermediate reasoning) 19.7\nCOT 65.6\nPAL 72.0\nSuccinct Code 47.8\nLLM Simulating Runtime 23.2\nTable 6: Solve Rates for Ablations\nC. Effect of Using Language Models of Code\nIn our experiments, we focused on evaluating the performance of a language model for code. We aimed to investigate\nwhether the additional performance boost observed in our results was due to the use of models like Codex, or whether our\nformulation was useful even for text-based models. To this end, we conducted additional experiments using text-based\nlanguage models. Our ﬁndings indicate that the PAL approach is not restricted to working solely with Codex, but can also\nbe applied to natural language (NL) models, as long as the model is sufﬁciently strong. Speciﬁcally, our results showed that\nin the text-davinci-001 model, the use of the CoT approach resulted in better performance.\nModel CoT PaL\ntext-davinci-001 26.5 8.6\ntext-davinci-002 46.9 65.8\ntext-davinci-003 65.3 69.8\nD. Analyzing the Effect of Increasing Number of Samples on P AL\nIn Section 5.1, we show that PALoutperforms strong baselines both for a single sample and by drawing 40 samples and\nusing majority voting. Figure 12 illustrates the trends for cases when the number of samples drawn are between 1 and 40,\nand the interpolation estimates demonstrate that P AL remains competitive throughout the number of samples.'
 'PAL: Program-aided Language Models 20\nG. Closer Look into Token-level Behaviors of Different Mechanisms\nBeyond empirical results, we make initial attempts to gain a deeper understanding of the behavior of LLMs with different\nreasoning mechanisms by looking into the token-level log-likelihood of reasoning chains produced by COTandPAL.\nWe randomly selected 20 questions from the COLORED OBJECTS dataset, along with their corresponding COTandPAL\nsolutions. We then manually compared the two mechanisms by focusing on tokens with a low log-likelihood.\nOur analysis reveals that COToften has lower conﬁdence in tokens related to numbers and quantitative information, the\ngrounded position of spatial adjectives ( e.g., right-most), properties such as the color of objects, and nouns that refer to the\nobjects. Speciﬁcally, we found that this occurred in seven, six, two, and six examples out of the 20 we examined. In contrast,\nPALuses list manipulations, such as len(objects) , and accesses objects and their associated properties through list\nindexing ( e.g.,object[3][0] ). We found that the LLM is typically conﬁdent in producing these programs. Furthermore,\nwe observed that while COTrequires different expressions for the same concept in different contexts, PALalmost always\nuses the same expression, which is presumably more robust. For example, when there are ﬁve objects, COTpredicts “the\nright-most thing is the ﬁfth item on the list”, and “the right-most thing is the third item on the list” when the number of\nobjects is three. Occasionally, COTalso predicts “the right-most thing is lastitem on the list” which does not provide more\nconcrete information. On the contrary, PALconﬁdently predicts objects[-1] consistently. The more consistent and\nuniform use of expressions in PALcan be attributed to the explicit and deﬁned nature of programming languages, which\nallows for clear and accurate expressions.\nH. Datasets\nIn the following tables (Table 8,Table 9, Table 10), we presents statistics and examples for the datasets we considered.\nDataset N Example\nReasoning about Colored Objects 2000 On the table, you see a bunch of objects arranged in a row: a purple\npaperclip, a pink stress ball, a brown keychain, a green scrunchiephone\ncharger, a mauve ﬁdget spinner, and a burgundy pen. What is the color\nof the object directly to the right of the stress ball?\nPenguins in a Table 149 Here is a table where the ﬁrst line is a header and each subsequent line is\na penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard,\n5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of\nLouis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80\ncm. We now add a penguin to the table: James, 12, 90, 12 How many\npenguins are less than 8 years old?\nDate Understanding 369 2015 is coming in 36 hours. What is the date one week from today in\nMM/DD/YYYY?\nTable 8: Reasoning datasets about everyday objects and concepts.\nDataset N Example\nObject Counting 1000 I have a chair, two potatoes, a cauliﬂower, a lettuce head, two tables, a\ncabbage, two onions, and three fridges. How many vegetables do I have?\nRepeat Copy 32 Repeat the word duck four times, but halfway through also say quack.\nTable 9: Reasoning datasets about algorithmic problems.'
 'critical. Notably, it is also possible to incrementally run\nthe PL segments and feed the execution results back to the\nLLM to generate the following blocks. For simplicity, in\nour experiments, we used a single, post-hoc, execution.\nThis work focuses on COT-style reasoning chain, but in\nAppendix I we show that PALalso improves Least-to-\nMost (Zhou et al., 2022) prompts, which introduce rea-\nsoning chains that decompose a question into sub-questions.\n4. Experimental Setup\nData and in-context examples We experiment with three\nbroad classes of reasoning tasks: (1) mathematical prob-\nlems ( §4.1) from a wide range of datasets including\nGSM 8K(Cobbe et al., 2021), SVAMP (Patel et al., 2021),\nASDIV (Miao et al., 2020), and MAWPS (Koncel-Kedziorski\net al., 2016); (2) symbolic reasoning ( §4.2) from BIG-Bench\nHard (Suzgun et al., 2022); (3) and algorithmic problems\n(§4.3) from BIG-Bench Hard as well. Details of all datasets\nare shown in Appendix H. For all of the experiments for\nwhich COTprompts were available, we use the same in-\ncontext examples as used by previous work. Otherwise, we\nrandomly sampled a ﬁxed set of in-context examples, and'
 'PAL: Program-aided Language Models 17\nE. Standard Deviations Across Multiple Order of Prompts\nFor each math reasoning task, we run inference using three random orderings of the prompts. As shown in Table 7, the\nstandard deviation between the results obtained from the three different seeds is minimal.\nCOT P AL\nAverage Standard Deviation Average Standard Deviation\nGSM 8K 65.6 1.10 72.0 0.16\nSVAMP 74.8 0.19 79.4 0.20\nASDIV 76.9 0.65 79.6 0.14\nGSM -HARD 23.3 0.49 61.2 0.91\nMAWPS -SingleEq 89.1 0.54 96.1 0.30\nMAWPS -SingleOp 91.9 0.55 94.6 0.36\nMAWPS -AddSub 86.0 0.62 92.5 0.34\nMAWPS -MultiArith 95.9 0.51 99.2 0.48\nTable 7: Standard deviations for three runs for the math reasoning datasets.\nF. PAL Beyond Benchmarks\nWe argue that symbolic reasoning is a crucial component in solving a wide range of tasks. In this section, we demonstrate\nexamples of tasks that may not initially appear to require using programs as intermediate reasoning steps, but can be\nimproved through the use of PAL-style reasoning. We demonstrate these examples using the ChatGPT tool.1In contrast to\nthe in-context-learning methods we used in the main paper, here we instruct ChatGPT to perform program-aided reasoning\nthrough one of the user utterances.\nIn Figure 13, in COT-style reasoning, while the reasoning chain is correct, the ﬁnal answer is wrong. In contrast, PAL-style\nreasoning could not only accurately extract the color of objects from the question but also produce the correct lines of code\nto branch to different situations that yield their corresponding correct answers.\nA more intriguing example is letting an LLM count the number of letters in the word “intriguing”. In Figure 14a, while the\nstep-by-step explanation appears reasonable by splitting the letters by spaces, ChatGPT does not change the answer after\nthis explicit reasoning and insists on the wrong answer. Explicitly instructing the model to perform step-by-step reasoning\nbefore answering the question still yields the wrong answer. In contrast, PAL-style reasoning only takes a few lines of code,\nand the execution does produce the correct answer, in this case. These examples indicate that PALcan beneﬁt even an\nostensibly powerful model like ChatGPT.\n1chat.openai.com']","For tasks with large numbers, PAL demonstrates robust performance by leveraging the Python interpreter for accurate calculations, contrasting with chain-of-thought methodologies where performance drops due to LLMs' computational limitations.",0.9166666666361111,1.0,0.9914668118195508,1.0,0.7730862061781147,0.9105266428942775
Describe the process and tools provided by CodeNet for transforming code samples into machine-learning-friendly formats.,CodeNet provides several pre-processing tools to transform source code into intermediate representations that are suitable for machine learning models. The dataset includes documented tools that help users access the code samples and make tailored selections based on their needs. This transformation process facilitates the extraction of relevant features and improves the usability of the dataset for various AI tasks.,"['Given the success of non-AI tools for code, why should we consider AI to augment or possibly\nreplace them? Firstly, AI can help reﬁne and re-tune the heuristics used by traditional coding tools.\nSecondly, based on the training data from past experience, AI can help prioritize when there is more\nthan one sound answer [ 5]. Thirdly, an AI-based tool may handle incomplete or invalid code more\nrobustly, thus expanding its scope. Finally, AI can incorporate signals usually ignored by traditional\ntools for code, such as the natural language in identiﬁers or comments.\nIn the enterprise environment, developers often face code written by large teams over many years\nand geographies. Developers must manipulate such code to modernize it, ﬁx bugs, improve its\nperformance, evolve it when requirements change, make it more secure, and/or comply with regu-\nlations. These tasks are challenging, and it is crucial to provide tool support for developers to be\nmore productive at performing them. It is well known that the latest advancements in deep learning\nalgorithms rely on best-of-breed datasets, such as ImageNet, to create increasingly complex and\npowerful models. In this paper, we present ""CodeNet"", a ﬁrst-of-its-kind dataset in scale, diversity,\nand quality, to accelerate the algorithmic advances in AI for Code.\nTo promote widespread adoption of CodeNet, we will be launching contests involving use cases based\non the dataset. The ﬁrst contest [ 6] will focus on diversity, inclusion and spurring interest among\naspiring data scientists. We are partnering with the Global Women in Data Science organization (with\npresence in over 50 countries) founded by Stanford University [ 7] and targeting teams with at least\nﬁfty percent women. We are planning follow-up contests that target experienced AI practitioners.\nThe rest of the paper is organized as follows. Section 2 introduces the CodeNet dataset. Related\ndatasets are discussed in Section 3, and the differentiation of CodeNet with respect to these related\ndatasets is elaborated in Section 4. Section 5 describes how CodeNet was curated and Section 6\nenumerates the usability features of CodeNet with several pre-processing tools to transform source\ncodes into representations that can be readily used as inputs into machine learning models. Section 7\ndiscusses the upcoming CodeNet contest and Section 8 describes important baseline experiments\nwith the CodeNet dataset. Section 9 presents further uses of the CodeNet dataset and Section 10\nconcludes the paper.\n2 The CodeNet Dataset\nThe CodeNet dataset consists of a large collection of code samples with extensive metadata. It\nalso contains documented tools to transform code samples into intermediate representations and to\naccess the dataset and make tailored selections. Our goal is to provide the community with a large,\nhigh-quality curated dataset that can be used to advance AI techniques for source code.\nCodeNet is derived from the data available on two online judge websites: AIZU [ 8] and AtCoder [ 9].\nOnline judge websites pose programming problems in the form of courses and contests. The dataset\nconsists of submissions to these problems, which are judged by an automated review process for\ncorrectness. Problem descriptions, submission outcomes, and associated metadata are available via\nvarious REST APIs.\nScale and Statistics. CodeNet contains a total of 13,916,868 submissions, divided into 4053\nproblems. Among the submissions, 53.6% (7,460,588) are accepted (compilable and pass the\nprescribed tests), 29.5% are marked with wrong answer, and the remaining rejected due to their\nfailure to meet run time or memory requirements. To our knowledge, this is the largest dataset so\nfar among similar kinds. Submissions are in 55 different languages; 95% of them are coded in C++,\nPython, Java, C, Ruby, and C#. C++ is the most common language, with 8,008,527 submissions (57%'
 'CodeNet: A Large-Scale AI for Code Dataset for\nLearning a Diversity of Coding Tasks\nRuchir Puri1, David S. Kung1, Geert Janssen1, Wei Zhang1,\nGiacomo Domeniconi1,Vladimir Zolotov1,Julian Dolby1,Jie Chen2,1,\nMihir Choudhury1,Lindsey Decker1,Veronika Thost2,1,Luca Buratti1,\nSaurabh Pujar1,Shyam Ramji1,Ulrich Finkler1,Susan Malaika3,Frederick Reiss1\n1IBM Research\n2MIT-IBM Watson AI Lab\n3IBM Worldwide Ecosystems\nAbstract\nOver the last several decades, software has been woven into the fabric of every\naspect of our society. As software development surges and code infrastructure of\nenterprise applications ages, it is now more critical than ever to increase software\ndevelopment productivity and modernize legacy applications. Advances in deep\nlearning and machine learning algorithms have enabled breakthroughs in computer\nvision, speech recognition, natural language processing and beyond, motivating\nresearchers to leverage AI techniques to improve software development efﬁciency.\nThus, the fast-emerging research area of “AI for Code” has garnered new interest\nand gathered momentum. In this paper, we present a large-scale dataset CodeNet ,\nconsisting of over 14 million code samples and about 500 million lines of code\nin 55 different programming languages, which is aimed at teaching AI to code.\nIn addition to its large scale, CodeNet has a rich set of high-quality annotations\nto benchmark and help accelerate research in AI techniques for a variety of crit-\nical coding tasks, including code similarity and classiﬁcation, code translation\nbetween a large variety of programming languages, and code performance (runtime\nand memory) improvement techniques. Additionally, CodeNet provides sample\ninput and output test sets for 98.5% of the code samples, which can be used as\nan oracle for determining code correctness and potentially guide reinforcement\nlearning for code quality improvements. As a usability feature, we provide several\npre-processing tools in CodeNet to transform source code into representations that\ncan be readily used as inputs into machine learning models. Results of code classi-\nﬁcation and code similarity experiments using the CodeNet dataset are provided as\na reference. We hope that the scale, diversity and rich, high-quality annotations of\nCodeNet will offer unprecedented research opportunities at the intersection of AI\nand Software Engineering.\n1 Introduction\nThere is a growing trend towards leveraging AI for building tools that support software engineering\nand development [ 1,2]. AI can manipulate and generate computer code, but can it do so with\nhigh quality? Many researchers are fascinated by this possibility, encouraged by AI successes in\nother domains and tantalized by the vision of computers programming computers. Some recent\ndeep-learning models [ 3,4] for code have received a lot of publicity: trained on vast amounts of\ndata and using novel architectures with billions of parameters, they sometimes generate surprisingly\nplausible code.\nPreprint. Under review.arXiv:2105.12655v2  [cs.SE]  29 Aug 2021'
 '9 Further Uses of CodeNet\nThe rich metadata and language diversity open CodeNet to a plethora of use cases. The problem-\nsubmission relationship in CodeNet corresponds to type-4 similarity [ 43] and can be used for code\nsearch and clone detection. The code samples in CodeNet are labeled with their acceptance status\nso we can readily extract pairs of buggy and ﬁxed code for code repair [ 49,50]. A large number\nof code samples come with inputs so that we can execute the code to extract the CPU run time and\nmemory footprint, which can be used for regression studies and prediction.\nCodeNet may also be used for program translation, given its wealth of programs written in a multitude\nof languages. Translation between two programming languages is born out of a practical need to port\nlegacy codebases to modern languages in order to increase accessibility and lower maintenance costs.\nWith the help of neural networks, machine translation models developed for natural languages [ 51]\nwere adapted to programming languages, producing pivotal success [ 4]. One considerable challenge of\nneural machine translation is that model training depends on large, parallel corpora that are expensive\nto curate [ 52], especially for low-resource languages (e.g., legacy code). Recently, monolingual\napproaches [ 53,4] were developed to mitigate the reliance on parallel data, paving ways to build\nmodels for languages with little translation. Compared with current popular data sets (e.g., [ 4,54]),\nCodeNet covers a much richer set of languages with ample training instances.\n10 Conclusion\nArtiﬁcial intelligence has made great strides in understanding human language. Computer scientists\nhave been fascinated by the possibility and tantalized by the vision of computers (AI) programming\ncomputers. In this paper, we presented ""CodeNet"", a ﬁrst-of-its-kind very large-scale, diverse and\nhigh-quality dataset to accelerate the algorithmic advances in AI for Code. This dataset is not\nonly unique in its scale, but also in the diversity of coding tasks it can help benchmark: from code\nsimilarity and classiﬁcation for advances in code recommendation algorithms, and code translation\nbetween a large variety of programming languages, to advances in code performance improvement\ntechniques. We hope that the scale, diversity and rich, high-quality annotations of CodeNet will offer\nunprecedented research opportunities at the intersection of AI and Software Engineering.\n11 Acknowledgements\nWe would like to acknowledge AIZU and AtCoder for making the code submissions publicly available.\nWe would like to thank the IBM Data Asset eXchange team for providing a platform to host the\nCodeNet dataset. We would like to thank the Women in Data Science team at Stanford University\nand the IBM Call for Code team for their collaboration in launching the CodeNet challenge.\n12 Bibliography\n[1]Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. A survey of\nmachine learning for big code and naturalness. ACM Computing Surveys (CSUR) , 51(4):1–37,\n2018.\n[2]Yanming Yang, Xin Xia, David Lo, and John Grundy. A survey on deep learning for software\nengineering. arXiv preprint arXiv:2011.14597 , 2020.\n[3]Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul\nPuri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke\nChan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad\nBavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias\nPlappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-V oss, William Hebgen Guss, Alex\nNichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra,'
 'far among similar kinds. Submissions are in 55 different languages; 95% of them are coded in C++,\nPython, Java, C, Ruby, and C#. C++ is the most common language, with 8,008,527 submissions (57%\nof the total), of which 4,353,049 are accepted. With the abundance of code samples, users can extract\nlarge benchmark datasets that are customized to their downstream use. See Figure 1 for a summary.\nDiversity. The problems in CodeNet are mainly pedagogical and range from elementary exercises\nto sophisticated problems that require advanced algorithms. The submitters range from beginners\nto experienced coders. Some submissions are correct while others contain different types of errors,\naccordingly labeled. The submissions are in many different languages.\nCode Samples. Each code sample is a single ﬁle and includes inputting the test cases and printing out\nthe computed results. The ﬁle name uses standard extensions that denote the programming language,\ne.g.,.pyfor Python. The majority of code samples contain only one function, although submissions\nto more complex problems might have several functions.\n2']","CodeNet offers pre-processing tools for tokenization, simplified parse tree generation, and code graph formation, easing the transformation of code into representations for machine learning models.",0.9999999999666667,0.5,0.9096777344970813,1.0,0.6963615140754393,0.9104460563017576
What specific challenges do the tasks in SuperGLUE address in natural language processing?,"The tasks in SuperGLUE address challenges in natural language processing that require understanding and reasoning about texts in English, focusing on tasks that are beyond the capabilities of current state-of-the-art systems. These tasks are designed to be solvable by most college-educated English speakers while excluding those requiring domain-specific knowledge. Overall, SuperGLUE aims to evaluate the performance of models on a range of language understanding tasks with varying input complexities and contexts.","['Table 1: The tasks included in SuperGLUE. WSD stands for word sense disambiguation, NLI is\nnatural language inference, coref. is coreference resolution, and QAis question answering. For\nMultiRC, we list the number of total answers for 456/83/166 train/dev/test questions.\nCorpus |Train | | Dev | | Test |Task Metrics Text Sources\nBoolQ 9427 3270 3245 QA acc. Google queries, Wikipedia\nCB 250 57 250 NLI acc./F1 various\nCOPA 400 100 500 QA acc. blogs, photography encyclopedia\nMultiRC 5100 953 1800 QA F1 a/EM various\nReCoRD 101k 10k 10k QA F1/EM news (CNN, Daily Mail)\nRTE 2500 278 300 NLI acc. news, Wikipedia\nWiC 6000 638 1400 WSD acc. WordNet, VerbNet, Wiktionary\nWSC 554 104 146 coref. acc. ﬁction books\n(Collobert and Weston, 2008; Dai and Le, 2015; Kiros et al., 2015; Hill et al., 2016; Conneau and\nKiela, 2018; McCann et al., 2017; Peters et al., 2018). GLUE was presented as a formal challenge\naffording straightforward comparison between such task-agnostic transfer learning techniques. Other\nsimilarly-motivated benchmarks include SentEval (Conneau and Kiela, 2018), which speciﬁcally\nevaluates ﬁxed-size sentence embeddings, and DecaNLP (McCann et al., 2018), which recasts a set\nof target tasks into a general question-answering format and prohibits task-speciﬁc parameters. In\ncontrast, GLUE provides a lightweight classiﬁcation API and no restrictions on model architecture or\nparameter sharing, which seems to have been well-suited to recent work in this area.\nSince its release, GLUE has been used as a testbed and showcase by the developers of several\ninﬂuential models, including GPT (Radford et al., 2018) and BERT (Devlin et al., 2019). As shown\nin Figure 1, progress on GLUE since its release has been striking. On GLUE, GPT and BERT\nachieved scores of 72.8 and 80.2 respectively, relative to 66.5 for an ELMo-based model (Peters\net al., 2018) and 63.7 for the strongest baseline with no multitask learning or pretraining above the\nword level. Recent models (Liu et al., 2019d; Yang et al., 2019) have clearly surpassed estimates of\nnon-expert human performance on GLUE (Nangia and Bowman, 2019). The success of these models\non GLUE has been driven by ever-increasing model capacity, compute power, and data quantity, as\nwell as innovations in model expressivity (from recurrent to bidirectional recurrent to multi-headed\ntransformer encoders) and degree of contextualization (from learning representation of words in\nisolation to using uni-directional contexts and ultimately to leveraging bidirectional contexts).\nIn parallel to work scaling up pretrained models, several studies have focused on complementary\nmethods for augmenting performance of pretrained models. Phang et al. (2018) show that BERT can\nbe improved using two-stage pretraining, i.e., ﬁne-tuning the pretrained model on an intermediate\ndata-rich supervised task before ﬁne-tuning it again on a data-poor target task. Liu et al. (2019d,c) and\nBach et al. (2018) get further improvements respectively via multi-task ﬁnetuning and using massive\namounts of weak supervision. Clark et al. (2019b) demonstrate that knowledge distillation (Hinton\net al., 2015; Furlanello et al., 2018) can lead to student networks that outperform their teachers.\nOverall, the quantity and quality of research contributions aimed at the challenges posed by GLUE\nunderline the utility of this style of benchmark for machine learning researchers looking to evaluate\nnew application-agnostic methods on language understanding.\nLimits to current approaches are also apparent via the GLUE suite. Performance on the GLUE\ndiagnostic entailment dataset, at 0.42 R3, falls far below the average human performance of 0.80\nR3reported in the original GLUE publication, with models performing near, or even below, chance\non some linguistic phenomena (Figure 2, Appendix B). While some initially difﬁcult categories\nsaw gains from advances on GLUE (e.g., double negation), others remain hard (restrictivity) or'
 'Table 2: Development set examples from the tasks in SuperGLUE. Bold text represents part of the\nexample format for each task. Text in italics is part of the model input. Underlined text is specially\nmarked in the input. Text in a monospaced font represents the expected model output.BoolQPassage: Barq’s – Barq’s is an American soft drink. Its brand of root beer is notable for having caffeine.\nBarq’s, created by Edward Barq and bottled since the turn of the 20th century, is owned by the Barq\nfamily but bottled by the Coca-Cola Company. It was known as Barq’s Famous Olde Tyme Root Beer\nuntil 2012.\nQuestion: is barq’s root beer a pepsi product Answer: NoCBText: B: And yet, uh, I we-, I hope to see employer based, you know, helping out. You know, child, uh,\ncare centers at the place of employment and things like that, that will help out. A: Uh-huh. B: What do\nyou think, do you think we are, setting a trend?\nHypothesis: they are setting a trend Entailment: UnknownCOPAPremise: My body cast a shadow over the grass. Question: What’s the CAUSE for this?\nAlternative 1: The sun was rising. Alternative 2: The grass was cut.\nCorrect Alternative: 1MultiRCParagraph: Susan wanted to have a birthday party. She called all of her friends. She has ﬁve friends.\nHer mom said that Susan can invite them all to the party. Her ﬁrst friend could not go to the party\nbecause she was sick. Her second friend was going out of town. Her third friend was not so sure if her\nparents would let her. The fourth friend said maybe. The ﬁfth friend could go to the party for sure. Susan\nwas a little sad. On the day of the party, all ﬁve friends showed up. Each friend had a present for Susan.\nSusan was happy and sent each friend a thank you card the next week\nQuestion: Did Susan’s sick friend recover? Candidate answers: Yes, she recovered (T),No(F),Yes\n(T),No, she didn’t recover (F),Yes, she was at Susan’s party (T)ReCoRDParagraph: (CNN )Puerto Rico on Sunday overwhelmingly voted for statehood. But Congress, the only\nbody that can approve new states, will ultimately decide whether the status of the UScommonwealth\nchanges. Ninety-seven percent of the votes in the nonbinding referendum favored statehood, an increase\nover the results of a 2012 referendum, ofﬁcial results from the State Electorcal Commission show. It\nwas the ﬁfth such vote on statehood. ""Today, we the people of Puerto Rico are sending a strong and\nclear message to the US Congress ... and to the world ... claiming our equal rights as American citizens,\nPuerto Rico Gov. Ricardo Rossello said in a news release. @highlight Puerto Rico voted Sunday in\nfavor of US statehood\nQuery For one, they can truthfully say, “Don’t blame me, I didn’t vote for them, ” when discussing the\n<placeholder> presidency Correct Entities: USRTEText: Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44,\naccording to the Christopher Reeve Foundation.\nHypothesis: Christopher Reeve had an accident. Entailment: FalseWiCContext 1: Room and board .Context 2: He nailed boards across the windows.\nSense match: FalseWSCText: Mark told Pete many lies about himself, which Pete included in his book. Heshould have been\nmore truthful. Coreference: False\n3 SuperGLUE Overview\n3.1 Design Process\nThe goal of SuperGLUE is to provide a simple, robust evaluation metric of any method capable of\nbeing applied to a broad range of language understanding tasks. To that end, in designing SuperGLUE,\nwe identify the following desiderata of tasks in the benchmark:\nTask substance: Tasks should test a system’s ability to understand and reason about texts in English.\nTask difﬁculty: Tasks should be beyond the scope of current state-of-the-art systems, but solvable by\nmost college-educated English speakers. We exclude tasks that require domain-speciﬁc knowledge,\ne.g. medical notes or scientiﬁc papers.\nEvaluability: Tasks must have an automatic performance metric that corresponds well to human'
 'SuperGLUE: A Stickier Benchmark for\nGeneral-Purpose Language Understanding Systems\nAlex Wang∗\nNew York UniversityYada Pruksachatkun∗\nNew York UniversityNikita Nangia∗\nNew York University\nAmanpreet Singh∗\nFacebook AI ResearchJulian Michael\nUniversity of WashingtonFelix Hill\nDeepMindOmer Levy\nFacebook AI Research\nSamuel R. Bowman\nNew York University\nAbstract\nIn the last year, new models and methods for pretraining and transfer learning have\ndriven striking performance improvements across a range of language understand-\ning tasks. The GLUE benchmark, introduced a little over one year ago, offers\na single-number metric that summarizes progress on a diverse set of such tasks,\nbut performance on the benchmark has recently surpassed the level of non-expert\nhumans, suggesting limited headroom for further research. In this paper we present\nSuperGLUE, a new benchmark styled after GLUE with a new set of more difﬁ-\ncult language understanding tasks, a software toolkit, and a public leaderboard.\nSuperGLUE is available at super.gluebenchmark.com .\n1 Introduction\nRecently there has been notable progress across many natural language processing (NLP) tasks, led\nby methods such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT\n(Devlin et al., 2019). The unifying theme of these methods is that they couple self-supervised learning\nfrom massive unlabelled text corpora with effective adapting of the resulting model to target tasks.\nThe tasks that have proven amenable to this general approach include question answering, textual\nentailment, and parsing, among many others (Devlin et al., 2019; Kitaev et al., 2019, i.a.).\nIn this context, the GLUE benchmark (Wang et al., 2019a) has become a prominent evaluation\nframework for research towards general-purpose language understanding technologies. GLUE is\na collection of nine language understanding tasks built on existing public datasets, together with\nprivate test data, an evaluation server, a single-number target metric, and an accompanying expert-\nconstructed diagnostic set. GLUE was designed to provide a general-purpose evaluation of language\nunderstanding that covers a range of training data volumes, task genres, and task formulations. We\nbelieve it was these aspects that made GLUE particularly appropriate for exhibiting the transfer-\nlearning potential of approaches like OpenAI GPT and BERT.\nThe progress of the last twelve months has eroded headroom on the GLUE benchmark dramatically.\nWhile some tasks (Figure 1) and some linguistic phenomena (Figure 2 in Appendix B) measured\nin GLUE remain difﬁcult, the current state of the art GLUE Score as of early July 2019 (88.4 from\nYang et al., 2019) surpasses human performance (87.1 from Nangia and Bowman, 2019) by 1.3\npoints, and in fact exceeds this human performance estimate on four tasks. Consequently, while there\n∗Equal contribution. Correspondence: glue-benchmark-admin@googlegroups.com\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.arXiv:1905.00537v3  [cs.CL]  13 Feb 2020'
 'Public data: We require that tasks have existing public training data in order to minimize the risks\ninvolved in newly-created datasets. We also prefer tasks for which we have access to (or could create)\na test set with private labels.\nTask format: We prefer tasks that had relatively simple input and output formats, to avoid incentiviz-\ning the users of the benchmark to create complex task-speciﬁc model architectures. Still, while GLUE\nis restricted to tasks involving single sentence or sentence pair inputs, for SuperGLUE we expand\nthe scope to consider tasks with longer inputs. This yields a set of tasks that requires understanding\nindividual tokens in context, complete sentences, inter-sentence relations, and entire paragraphs.\nLicense: Task data must be available under licences that allow use and redistribution for research\npurposes.\nTo identify possible tasks for SuperGLUE, we disseminated a public call for task proposals to the\nNLP community, and received approximately 30 proposals. We ﬁltered these proposals according\nto our criteria. Many proposals were not suitable due to licensing issues, complex formats, and\ninsufﬁcient headroom; we provide examples of such tasks in Appendix D. For each of the remaining\ntasks, we ran a BERT-based baseline and a human baseline, and ﬁltered out tasks which were either\ntoo challenging for humans without extensive training or too easy for our machine baselines.\n3.2 Selected Tasks\nFollowing this process, we arrived at eight tasks to use in SuperGLUE. See Tables 1 and 2 for details\nand speciﬁc examples of each task.\nBoolQ (Boolean Questions, Clark et al., 2019a) is a QA task where each example consists of a short\npassage and a yes/no question about the passage. The questions are provided anonymously and\nunsolicited by users of the Google search engine, and afterwards paired with a paragraph from a\nWikipedia article containing the answer. Following the original work, we evaluate with accuracy.\nCB(CommitmentBank, de Marneffe et al., 2019) is a corpus of short texts in which at least one\nsentence contains an embedded clause. Each of these embedded clauses is annotated with the degree\nto which it appears the person who wrote the text is committed to the truth of the clause. The resulting\ntask framed as three-class textual entailment on examples that are drawn from the Wall Street Journal,\nﬁction from the British National Corpus, and Switchboard. Each example consists of a premise\ncontaining an embedded clause and the corresponding hypothesis is the extraction of that clause.\nWe use a subset of the data that had inter-annotator agreement above 80%. The data is imbalanced\n(relatively fewer neutral examples), so we evaluate using accuracy and F1, where for multi-class F1\nwe compute the unweighted average of the F1 per class.\nCOPA (Choice of Plausible Alternatives, Roemmele et al., 2011) is a causal reasoning task in which\na system is given a premise sentence and must determine either the cause or effect of the premise\nfrom two possible choices. All examples are handcrafted and focus on topics from blogs and a\nphotography-related encyclopedia. Following the original work, we evaluate using accuracy.\nMultiRC (Multi-Sentence Reading Comprehension, Khashabi et al., 2018) is a QA task where each\nexample consists of a context paragraph, a question about that paragraph, and a list of possible\nanswers. The system must predict which answers are true and which are false. While many QA\ntasks exist, we use MultiRC because of a number of desirable properties: (i) each question can have\nmultiple possible correct answers, so each question-answer pair must be evaluated independent of\nother pairs, (ii) the questions are designed such that answering each question requires drawing facts\nfrom multiple context sentences, and (iii) the question-answer pair format more closely matches\nthe API of other tasks in SuperGLUE than the more popular span-extractive QA format does. The']","The tasks challenge models on various fronts, including deep textual understanding, commonsense reasoning, fine-grained word sense disambiguation, and the ability to handle ambiguity and complexity in language, reflecting a broad spectrum of NLP challenges.",0.999999999975,1.0,0.991863131291734,1.0,0.7896707081398551,0.8729685468451345
"How does SuperGLUE's scoring system work, and what does it aim to achieve?","SuperGLUE uses a single-number performance metric to evaluate systems across a diverse set of language understanding tasks, aiming to provide a robust measure of progress in NLP. It includes a public leaderboard and improves upon the original GLUE benchmark by featuring more challenging tasks and diverse formats. The goal is to encourage significant advancements in machine learning methods like sample-efficient, transfer, multitask, and self-supervised learning.","['SuperGLUE: A Stickier Benchmark for\nGeneral-Purpose Language Understanding Systems\nAlex Wang∗\nNew York UniversityYada Pruksachatkun∗\nNew York UniversityNikita Nangia∗\nNew York University\nAmanpreet Singh∗\nFacebook AI ResearchJulian Michael\nUniversity of WashingtonFelix Hill\nDeepMindOmer Levy\nFacebook AI Research\nSamuel R. Bowman\nNew York University\nAbstract\nIn the last year, new models and methods for pretraining and transfer learning have\ndriven striking performance improvements across a range of language understand-\ning tasks. The GLUE benchmark, introduced a little over one year ago, offers\na single-number metric that summarizes progress on a diverse set of such tasks,\nbut performance on the benchmark has recently surpassed the level of non-expert\nhumans, suggesting limited headroom for further research. In this paper we present\nSuperGLUE, a new benchmark styled after GLUE with a new set of more difﬁ-\ncult language understanding tasks, a software toolkit, and a public leaderboard.\nSuperGLUE is available at super.gluebenchmark.com .\n1 Introduction\nRecently there has been notable progress across many natural language processing (NLP) tasks, led\nby methods such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT\n(Devlin et al., 2019). The unifying theme of these methods is that they couple self-supervised learning\nfrom massive unlabelled text corpora with effective adapting of the resulting model to target tasks.\nThe tasks that have proven amenable to this general approach include question answering, textual\nentailment, and parsing, among many others (Devlin et al., 2019; Kitaev et al., 2019, i.a.).\nIn this context, the GLUE benchmark (Wang et al., 2019a) has become a prominent evaluation\nframework for research towards general-purpose language understanding technologies. GLUE is\na collection of nine language understanding tasks built on existing public datasets, together with\nprivate test data, an evaluation server, a single-number target metric, and an accompanying expert-\nconstructed diagnostic set. GLUE was designed to provide a general-purpose evaluation of language\nunderstanding that covers a range of training data volumes, task genres, and task formulations. We\nbelieve it was these aspects that made GLUE particularly appropriate for exhibiting the transfer-\nlearning potential of approaches like OpenAI GPT and BERT.\nThe progress of the last twelve months has eroded headroom on the GLUE benchmark dramatically.\nWhile some tasks (Figure 1) and some linguistic phenomena (Figure 2 in Appendix B) measured\nin GLUE remain difﬁcult, the current state of the art GLUE Score as of early July 2019 (88.4 from\nYang et al., 2019) surpasses human performance (87.1 from Nangia and Bowman, 2019) by 1.3\npoints, and in fact exceeds this human performance estimate on four tasks. Consequently, while there\n∗Equal contribution. Correspondence: glue-benchmark-admin@googlegroups.com\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.arXiv:1905.00537v3  [cs.CL]  13 Feb 2020'
 'Table 2: Development set examples from the tasks in SuperGLUE. Bold text represents part of the\nexample format for each task. Text in italics is part of the model input. Underlined text is specially\nmarked in the input. Text in a monospaced font represents the expected model output.BoolQPassage: Barq’s – Barq’s is an American soft drink. Its brand of root beer is notable for having caffeine.\nBarq’s, created by Edward Barq and bottled since the turn of the 20th century, is owned by the Barq\nfamily but bottled by the Coca-Cola Company. It was known as Barq’s Famous Olde Tyme Root Beer\nuntil 2012.\nQuestion: is barq’s root beer a pepsi product Answer: NoCBText: B: And yet, uh, I we-, I hope to see employer based, you know, helping out. You know, child, uh,\ncare centers at the place of employment and things like that, that will help out. A: Uh-huh. B: What do\nyou think, do you think we are, setting a trend?\nHypothesis: they are setting a trend Entailment: UnknownCOPAPremise: My body cast a shadow over the grass. Question: What’s the CAUSE for this?\nAlternative 1: The sun was rising. Alternative 2: The grass was cut.\nCorrect Alternative: 1MultiRCParagraph: Susan wanted to have a birthday party. She called all of her friends. She has ﬁve friends.\nHer mom said that Susan can invite them all to the party. Her ﬁrst friend could not go to the party\nbecause she was sick. Her second friend was going out of town. Her third friend was not so sure if her\nparents would let her. The fourth friend said maybe. The ﬁfth friend could go to the party for sure. Susan\nwas a little sad. On the day of the party, all ﬁve friends showed up. Each friend had a present for Susan.\nSusan was happy and sent each friend a thank you card the next week\nQuestion: Did Susan’s sick friend recover? Candidate answers: Yes, she recovered (T),No(F),Yes\n(T),No, she didn’t recover (F),Yes, she was at Susan’s party (T)ReCoRDParagraph: (CNN )Puerto Rico on Sunday overwhelmingly voted for statehood. But Congress, the only\nbody that can approve new states, will ultimately decide whether the status of the UScommonwealth\nchanges. Ninety-seven percent of the votes in the nonbinding referendum favored statehood, an increase\nover the results of a 2012 referendum, ofﬁcial results from the State Electorcal Commission show. It\nwas the ﬁfth such vote on statehood. ""Today, we the people of Puerto Rico are sending a strong and\nclear message to the US Congress ... and to the world ... claiming our equal rights as American citizens,\nPuerto Rico Gov. Ricardo Rossello said in a news release. @highlight Puerto Rico voted Sunday in\nfavor of US statehood\nQuery For one, they can truthfully say, “Don’t blame me, I didn’t vote for them, ” when discussing the\n<placeholder> presidency Correct Entities: USRTEText: Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44,\naccording to the Christopher Reeve Foundation.\nHypothesis: Christopher Reeve had an accident. Entailment: FalseWiCContext 1: Room and board .Context 2: He nailed boards across the windows.\nSense match: FalseWSCText: Mark told Pete many lies about himself, which Pete included in his book. Heshould have been\nmore truthful. Coreference: False\n3 SuperGLUE Overview\n3.1 Design Process\nThe goal of SuperGLUE is to provide a simple, robust evaluation metric of any method capable of\nbeing applied to a broad range of language understanding tasks. To that end, in designing SuperGLUE,\nwe identify the following desiderata of tasks in the benchmark:\nTask substance: Tasks should test a system’s ability to understand and reason about texts in English.\nTask difﬁculty: Tasks should be beyond the scope of current state-of-the-art systems, but solvable by\nmost college-educated English speakers. We exclude tasks that require domain-speciﬁc knowledge,\ne.g. medical notes or scientiﬁc papers.\nEvaluability: Tasks must have an automatic performance metric that corresponds well to human'
 'BiLSTM+ELMo+Attn\nOpenAI GPT\nBERT + Single-task Adapters\nBERT (Large)\nBERT on STILTs\nBERT + BAM\nSemBERT\nSnorkel MeTaL\nALICE (Large)\nMT-DNN (ensemble)\nXLNet-Large (ensemble)0.50.60.70.80.91.01.11.2\nGLUE Score\nHuman Performance\nCoLA\nSST-2MRPC\nSTS-B\nQQP\nMNLIQNLI\nRTE\nWNLIFigure 1: GLUE benchmark performance for submitted systems, rescaled to set human performance\nto 1.0, shown as a single number score, and broken down into the nine constituent task performances.\nFor tasks with multiple metrics, we use an average of the metrics. More information on the tasks\nincluded in GLUE can be found in Wang et al. (2019a) and in Warstadt et al. (2019, CoLA), Socher\net al. (2013, SST-2), Dolan and Brockett (2005, MRPC), Cer et al. (2017, STS-B), and Williams et al.\n(2018, MNLI), and Rajpurkar et al. (2016, the original data source for QNLI).\nremains substantial scope for improvement towards GLUE’s high-level goals, the original version of\nthe benchmark is no longer a suitable metric for quantifying such progress.\nIn response, we introduce SuperGLUE, a new benchmark designed to pose a more rigorous test of\nlanguage understanding. SuperGLUE has the same high-level motivation as GLUE: to provide a\nsimple, hard-to-game measure of progress toward general-purpose language understanding technolo-\ngies for English. We anticipate that signiﬁcant progress on SuperGLUE should require substantive\ninnovations in a number of core areas of machine learning, including sample-efﬁcient, transfer,\nmultitask, and unsupervised or self-supervised learning.\nSuperGLUE follows the basic design of GLUE: It consists of a public leaderboard built around\neight language understanding tasks, drawing on existing data, accompanied by a single-number\nperformance metric, and an analysis toolkit. However, it improves upon GLUE in several ways:\nMore challenging tasks: SuperGLUE retains the two hardest tasks in GLUE. The remaining tasks\nwere identiﬁed from those submitted to an open call for task proposals and were selected based on\ndifﬁculty for current NLP approaches.\nMore diverse task formats: The task formats in GLUE are limited to sentence- and sentence-pair\nclassiﬁcation. We expand the set of task formats in SuperGLUE to include coreference resolution\nand question answering (QA).\nComprehensive human baselines: We include human performance estimates for all benchmark\ntasks, which verify that substantial headroom exists between a strong BERT-based baseline and\nhuman performance.\nImproved code support: SuperGLUE is distributed with a new, modular toolkit for work on\npretraining, multi-task learning, and transfer learning in NLP, built around standard tools including\nPyTorch (Paszke et al., 2017) and AllenNLP (Gardner et al., 2017).\nReﬁned usage rules: The conditions for inclusion on the SuperGLUE leaderboard have been\nrevamped to ensure fair competition, an informative leaderboard, and full credit assignment to data\nand task creators.\nThe SuperGLUE leaderboard, data, and software tools are available at super.gluebenchmark.com .\n2 Related Work\nMuch work prior to GLUE demonstrated that training neural models with large amounts of available\nsupervision can produce representations that effectively transfer to a broad range of NLP tasks\n2'
 'Table 4: Baseline performance on the SuperGLUE development.\nModel Avg BoolQ CB COPA MultiRC ReCoRD RTE WiC WSC\nMetrics Acc. Acc./F1 Acc. F1 a/EM F1/EM Acc. Acc. Acc.\nMost Frequent Class 47.7 62.2 50.0/22.2 55.0 59.9/ 0.8 32.4/31.5 52.7 50.0 63.5\nCBOW 47.7 62.4 71.4/49.6 63.0 20.3/ 0.3 14.4/13.8 54.2 55.3 61.5\nBERT 72.2 77.7 94.6/93.7 69.0 70.5/24.7 70.6/69.8 75.8 74.9 68.3\nBERT++ 74.6 80.1 96.4/95.0 78.0 70.5/24.7 70.6/69.8 82.3 74.9 68.3\nA Development Set Results\nIn Table 4, we present results of the baselines on the SuperGLUE tasks development sets.\nB Performance on GLUE Diagnostics\nFigure 2 shows the performance on the GLUE diagnostics dataset for systems submitted to the public\nleaderboard.\nDisjunction Downward Monotone Restrictivity Double Negation Prepositional Phrases60\n40\n20\n020406080\nChance\nBiLSTM+ELMo+Attn\nOpenAI GPT\nBERT + Single-task Adapters\nBERT (Large)\nBERT on STILTsBERT + BAM\nSemBERT\nSnorkel MeTaL\nALICE (Large)\nMT-DNN (ensemble)\nXLNet-Large (ensemble)\nFigure 2: Performance of GLUE submissions on selected diagnostic categories, reported using the\nR3metric scaled up by 100, as in Wang et al. (2019a, see paper for a description of the categories).\nSome initially difﬁcult categories, like double negation, saw gains from advances on GLUE, but\nothers remain hard (restrictivity) or even adversarial (disjunction, downward monotone).\nC Human Performance Estimation\nFor collecting data to establish human performance on the SuperGLUE tasks, we follow a two\nstep procedure where we ﬁrst provide some training to the crowd workers before they proceed to\nannotation. For both steps and all tasks, the average pay rate is $23.75/hr.7\nIn the training phase, workers are provided with instructions on the task, linked to an FAQ page, and\nare asked to annotate up to 30 examples from the development set. After answering each example,\nworkers are also asked to check their work against the provided ground truth label. After the training\nphase is complete, we provide the qualiﬁcation to work on the annotation phase to all workers\nwho annotated a minimum of ﬁve examples, i.e. completed ﬁve HITs during training and achieved\nperformance at, or above the median performance across all workers during training.\n7This estimate is taken from https://turkerview.com .\n16']"," SuperGLUE uses a single-number metric averaging task scores to simplify comparison across models, aiming to provide a clear, comprehensive measure of progress on a wide range of language understanding tasks.",0.999999999975,1.0,0.8587506042588903,1.0,0.49819196132609495,0.9339443158926151
"What practical meta-task is Task2Vec particularly designed to optimize, and how does it achieve this?","Task2Vec is particularly designed to optimize the selection of a pre-trained feature extractor for a new task, especially when there is insufficient training data. It achieves this by providing a fixed-dimensional representation of tasks using the Fisher Information Matrix, allowing for the prediction of task similarities and the selection of the most suitable feature extractor based on these embeddings. This approach improves test performance while minimizing the overhead in the training process.","['lem of meta-learning has a long history with much re-\ncent work dedicated to problems such as neural architecture\nsearch and hyper-parameter estimation. Closely related to\nour problem is work on selecting from a library of classi-\nﬁers to solve a new task [33, 2, 20]. Unlike our approach,\nthese usually address the question via land-marking or ac-\ntive testing, in which a few different models are evaluated\nand performance of the remainder estimated by extension.\nThis can be viewed as a problem of completing a matrix\ndeﬁned by performance of each model on each task.\nA similar approach has been taken in computer vision for\nselecting a detector for a new category out of a large library\nof detectors [26, 40, 38].\n7. Discussion\nTASK 2VEC is an efﬁcient way to represent a task, or the\ncorresponding dataset, as a ﬁxed dimensional vector. It has\nseveral appealing properties, in particular its norm corre-\nlates with the test error obtained on the task, and the co-\nsine distance between embeddings correlates with natural\ndistances between tasks, when available, such as the taxo-\nnomic distance for species classiﬁcation, and the ﬁne-tuning\ndistance for transfer learning. Having a representation of\ntasks paves the way for a wide variety of meta-learning\ntasks. In this work, we focused on selection of an expert\nfeature extractor in order to solve a new task, especially\nwhen little training data is present, and showed that using\nTASK 2VEC to select an expert from a collection can sen-\nsibly improve test performance while adding only a small\noverhead to the training process.'
 'TASK2VEC: Task Embedding for Meta-Learning\nAlessandro Achille\nUCLA and AWS\nachille@cs.ucla.eduMichael Lam\nAWS\nmichlam@amazon.comRahul Tewari\nAWS\ntewarir@amazon.comAvinash Ravichandran\nAWS\nravinash@amazon.com\nSubhransu Maji\nUMass and AWS\nsmmaji@amazon.comCharless Fowlkes\nUCI and AWS\nfowlkec@amazon.comStefano Soatto\nUCLA and AWS\nsoattos@amazon.comPietro Perona\nCaltech and AWS\nperonapp@amazon.com\nAbstract\nWe introduce a method to provide vectorial represen-\ntations of visual classiﬁcation tasks which can be used\nto reason about the nature of those tasks and their re-\nlations. Given a dataset with ground-truth labels and a\nloss function deﬁned over those labels, we process images\nthrough a “probe network” and compute an embedding\nbased on estimates of the Fisher information matrix asso-\nciated with the probe network parameters. This provides a\nﬁxed-dimensional embedding of the task that is independent\nof details such as the number of classes and does not require\nany understanding of the class label semantics. We demon-\nstrate that this embedding is capable of predicting task sim-\nilarities that match our intuition about semantic and tax-\nonomic relations between different visual tasks ( e.g., tasks\nbased on classifying different types of plants are similar).\nWe also demonstrate the practical value of this framework\nfor the meta-task of selecting a pre-trained feature extractor\nfor a new task. We present a simple meta-learning frame-\nwork for learning a metric on embeddings that is capable of\npredicting which feature extractors will perform well. Se-\nlecting a feature extractor with task embedding obtains a\nperformance close to the best available feature extractor,\nwhile costing substantially less than exhaustively training\nand evaluating on all available feature extractors.\n1. Introduction\nThe success of Deep Learning hinges in part on the fact\nthat models learned for one task can be used on other related\ntasks. Yet, no general framework exists to describe and\nlearn relations between tasks. We introduce the TASK 2VEC\nembedding, a technique to represent tasks as elements of a\nvector space based on the Fisher Information Matrix. The\nnorm of the embedding correlates with the complexity of\nthe task, while the distance between embeddings capturessemantic similarities between tasks (Fig. 1). When other\nnatural distances are available, such as the taxonomical dis-\ntance in biological classiﬁcation, we ﬁnd that the embed-\nding distance correlates positively with it (Fig. 2). More-\nover, we introduce an asymmetric distance on tasks which\ncorrelates with the transferability between tasks.\nComputation of the embedding leverages a duality be-\ntween network parameters (weights) and outputs (activa-\ntions) in a deep neural network (DNN): Just as the activa-\ntions of a DNN trained on a complex visual recognition task\nare a rich representation of the input images, we show that\nthe gradients of the weights relative to a task-speciﬁc loss\nare a rich representation of the task itself. Speciﬁcally, given\na task deﬁned by a dataset D={(xi,yi)}N\ni=1of labeled\nsamples, we feed the data through a pre-trained reference\nconvolutional neural network which we call “ probe net-\nwork ”, and compute the diagonal Fisher Information Ma-\ntrix (FIM) of the network ﬁlter parameters to capture the\nstructure of the task (Sect. 2). Since the architecture and\nweights of the probe network are ﬁxed, the FIM provides a\nﬁxed-dimensional representation of the task. We show this\nembedding encodes the “difﬁculty” of the task, character-\nistics of the input domain, and which features of the probe\nnetwork are useful to solve it (Sect. 2.1).\nOur task embedding can be used to reason about the\nspace of tasks and solve meta-tasks. As a motivating exam-\nple, we study the problem of selecting the best pre-trained\nfeature extractor to solve a new task. This can be particu-\nlarly valuable when there is insufﬁcient data to train or ﬁne-'
 'ple, we study the problem of selecting the best pre-trained\nfeature extractor to solve a new task. This can be particu-\nlarly valuable when there is insufﬁcient data to train or ﬁne-\ntune a generic model, and transfer of knowledge is essen-\ntial. TASK 2VEC depends solely on the task, and ignores\ninteractions with the model which may however play an\nimportant role. To address this, we learn a joint task and\nmodel embedding, called MODEL 2VEC, in such a way that\nmodels whose embeddings are close to a task exhibit good\nperfmormance on the task. We use this to select an expert\nfrom a given collection, improving performance relative to\n1arXiv:1902.03545v1  [cs.LG]  10 Feb 2019'
 'Meta-task Optimal Chance ImageNet TASK 2VEC Asymmetric TASK 2VEC MODEL 2VEC\niNat + CUB 31.24 +59.52% +30.18% +42.54% +9.97% +6.81%\nMixed 22.90 +112.49% +75.73% +40.30% +29.23% +27.81%\nTable 2: Model selection performance of different metrics. Average optimal error obtained on two meta-learning tasks\nby exhaustive search over the best expert, and relative error increase when using cheaper model selection methods. Always\npicking a ﬁxed good general model ( e.g., a model pretrained on ImageNet) performs better than picking an expert at random\n(chance). However, picking an expert using the Asymmetric TASK 2VEC distance can achieve an overall better performance\nthan using a general model. Notice also the improvement over the Symmetric version, especially on iNat + CUB, where\nexperts trained on very similar tasks may be too simple to yield good transfer, and should be avoided.\nstatistics are useful (analogous to our choice of probe net-\nwork) has also been considered, for example [9] train an\nautoencoder that learns to extract ﬁxed dimensional sum-\nmary statistics that can reproduce many different datasets\naccurately. However, for general vision tasks which apply\nto all natural images, the domain is the same across tasks.\nTaskonomy [39] explores the structure of the space of\ntasks, focusing on the question of effective knowledge\ntransfer in a curated collection of 26 visual tasks, ranging\nfrom classiﬁcation to 3D reconstruction, deﬁned on a com-\nmon domain. They compute pairwise transfer distances be-\ntween pairs of tasks and use the results to compute a di-\nrected hierarchy. Introducing novel tasks requires comput-\ning the pairwise distance with tasks in the library. In con-\ntrast, we focus on a larger library of 1,460 ﬁne-grained clas-\nsiﬁcation tasks both on same and different domains, and\nshow that it is possible to represent tasks in a topological\nspace with a constant-time embedding. The large task col-\nlection and cheap embedding costs allow us to tackle new\nmeta-learning problems.\nFisher kernels Our work takes inspiration from Jaakkola\nand Hausler [16]. They propose the “Fisher Kernel”, which\nuses the gradients of a generative model score function as a\nrepresentation of similarity between data items\nK(x(1),x(2)) =∇θlogP(x(1)|θ)TF−1∇θlogP(x(2)|θ).\nHereP(x|θ)is a parameterized generative model and Fis\nthe Fisher information matrix. This provides a way to utilize\ngenerative models in the context of discriminative learning.\nVariants of the Fisher kernel have found wide use as a repre-\nsentation of images [28, 29], and other structured data such\nas protein molecules [17] and text [30]. Since the genera-\ntive model can be learned on unlabelled data, several works\nhave investigated the use of Fisher kernel for unsupervised\nlearning [14, 31]. [35] learns a metric on the Fisher kernel\nrepresentation similar to our metric learning approach. Our\napproach differs in that we use the FIM as a representation\nof a whole dataset (task) rather than using model gradients\nas representations of individual data items.\nFisher Information for CNNs Our approach to task em-\nbedding makes use of the Fisher Information matrix of aneural network as a characterization of the task. Use of\nFisher information for neural networks was popularized by\nAmari [6] who advocated optimization using natural gra-\ndient descent which leverages the fact that the FIM is an\nappropriate parameterization-independent metric on statis-\ntical models. Recent work has focused on approximates of\nFIM appropriate in this setting (see e.g., [12, 10, 25]). FIM\nhas also been proposed for various regularization schemes\n[5, 8, 22, 27], analyze learning dynamics of deep networks\n[4], and to overcome catastrophic forgetting [19].\nMeta-learning and Model Selection The general prob-\nlem of meta-learning has a long history with much re-\ncent work dedicated to problems such as neural architecture\nsearch and hyper-parameter estimation. Closely related to']",Task2Vec is designed for selecting the most suitable pre-trained feature extractor for a new task by learning a metric on task embeddings to predict feature extractor performance.,0.999999999975,1.0,0.9510579176070802,1.0,0.7003979815123274,0.9554380798954629
In what way does Task2Vec's asymmetric distance measure contribute to task similarity assessment and model selection?,"Task2Vec's asymmetric distance measure accounts for both task similarity and the complexity of the initial task, enhancing the assessment of task similarity. By incorporating the complexity of tasks through a hyperparameter, it allows for a more nuanced evaluation of transferability between tasks, which is crucial for model selection. This approach leads to better predictions of model performance on new tasks by positioning models closer to relevant tasks in the embedding space.","['about the structure of the task. We may therefore expect\nthat the distance between two embeddings correlate posi-\ntively with natural metrics on the space of tasks. However,\nthere are two problems in using the Euclidean distance be-\ntween embeddings: the parameters of the network have dif-\nferent scales, and the norm of the embedding is affected by\ncomplexity of the task and the number of samples used to\ncompute the embedding.\nSymmetric TASK 2VEC distance To make the distance\ncomputation robust, we propose to use the cosine distance\nbetween normalized embeddings:\ndsym(Fa,Fb) =dcos(Fa\nFa+Fb,Fb\nFa+Fb)\n,\nwheredcosis the cosine distance, FaandFbare the two\ntask embeddings ( i.e., the diagonal of the Fisher Informa-\ntion computed on the same probe network), and the division\nis element-wise. This is a symmetric distance which we ex-\npect to capture semantic similarity between two tasks. For\nexample, we show in Fig. 2 that it correlates well with the\ntaxonomical distance between species on iNaturalist.\nOn the other hand, precisely for this reason, this distance\nis ill-suited for tasks such as model selection, where the (in-\ntrinsically asymmetric) transfer distance is more relevant.\nAsymmetric TASK 2VEC distance In a ﬁrst approxima-\ntion, that does not consider either the model or the training\nprocedure used, positive transfer between two tasks depends\nboth on the similarity between two tasks and on the com-\nplexity of the ﬁrst. Indeed, pre-training on a general but\ncomplex task such as ImageNet often yields a better result\nthan ﬁne-tuning from a close dataset of comparable com-\nplexity. In our case, complexity can be measured as the dis-\ntance from the trivial embedding. This suggests the follow-\ning asymmetric score, again improperly called a “distance”\ndespite being asymmetric and possibly negative:\ndasym(ta→tb) =dsym(ta,tb)−αdsym(ta,t0),\nwheret0is the trivial embedding, and αis an hyperparam-\neter. This has the effect of bring more complex models\ncloser. The hyper-parameter αcan be selected based on\nthe meta-task. In our experiments, we found that the best\nvalue ofα(α= 0.15when using a ResNet-34 pretrained\non ImageNet as the probe network) is robust to the choice\nof meta-tasks.\n4.MODEL 2VEC: task/model co-embedding\nBy construction, the TASK 2VEC distance ignores details\nof the model and only relies on the task. If we know what\ntask a model was trained on, we can represent the model by\nthe embedding of that task. However, in general we may\nnot have such information ( e.g., black-box models or hand-\nconstructed feature extractors). We may also have multiplemodels trained on the same task with different performance\ncharacteristics. To model the joint interaction between task\nand model ( i.e., architecture and training algorithm), we aim\nto learn a joint embedding of the two.\nWe consider for concreteness the problem of learning\na joint embedding for model selection. In order to em-\nbed models in the task space so that those near a task\nare likely to perform well on that task, we formulate the\nfollowing meta-learning problem: Given kmodels, their\nMODEL 2VEC embedding are the vectors mi=Fi+bi,\nwhereFiis the task embedding of the task used to train\nmodelmi(if available, else we set it to zero), and biis a\nlearned “model bias” that perturbs the task embedding to\naccount for particularities of the model. We learn biby opti-\nmizing ak-way cross entropy loss to predict the best model\ngiven the task distance (see Supplementary Material):\nL=E[−logp(m|dasym(t,m0),...,d asym(t,mk))].\nAfter training, given a novel query task t, we can then pre-\ndict the best model for it as the arg maxidasym(t,mi), that\nis, the model miembedded closest to the query task.\n5. Experiments\nWe test TASK 2VEC on a large collection of tasks and\nmodels, related to different degrees. Our experiments aim to\ntest both qualitative properties of the embedding and its per-\nformance on meta-learning tasks. We use an off-the-shelf'
 '0 25 50 75 100 125\nSize k of neighborhood1.01.52.02.53.0Avg. top-k tax. distanceTask2Vec distance\nTax. distance\n0.4 0.6 0.8\nL1 norm of task embedding1e80%10%20%30%40%50%60%Test error on task (%)\nFigure 2: Distance between species classiﬁcation tasks. (Left) Task similarity matrix ordered by hierarchical clustering.\nNote that the dendrogram produced by the task similarity matches the taxonomic clusters (indicated by color bar). (Center)\nFor tasks extracted from iNaturalist and CUB, we compare the cosine distance between tasks to their taxonomical distance.\nAs the size of the task embedding neighborhood increases (measured by number of tasks in the neighborhood), we plot the\naverage taxonomical distance of tasks from the neighborhood center. While the task distance does not perfectly match the\ntaxonomical distance (whose curve is shown in orange), it shows a good correlation. Difference are both due to the fact that\ntaxonomically close species may need very different features to be classiﬁed, creating a mismatch between the two notions\nof distance, and because for some tasks in iNaturalist too few samples are provided to compute a good embedding. (Right)\nCorrelation between L1norm of the task embedding (distance from origin) and test error obtained on the task.\nnear the decision boundary since p(1−p)is maximized at\np= 1/2. Compare this to the covariance matrix of the data,\nC0, to which all data points contribute equally. Instead, in\nTASK 2VEC information on the domain is based on data near\nthe decision boundary (task-weighted domain embedding).\nEncoding useful features for the task: The FIM de-\npends on the curvature of the loss function with the diagonal\nentries capturing the sensitivity of the loss to model param-\neters. Speciﬁcally, in the two-layer model one can see that,\nif a given feature is uncorrelated with y, the correspond-\ning blocks of Fare zero. In contrast, a domain embedding\nbased on feature activations of the probe network (e.g., C1)\nonly reﬂects which features vary over the dataset without\nindication of whether they are relevant to the task.\n3. Similarity Measures on the Space of Tasks\nWhat metric should be used on the space of tasks? This\ndepends critically on the meta-task we are considering. As a\nmotivation, we concentrate on the meta-task of selecting the\npre-trained feature extractor from a set in order to obtain the\nbest performance on a new training task. There are several\nnatural metrics that may be considered for this meta-task.\nIn this work, we mainly consider:\nTaxonomic distance For some tasks, there is a natural no-\ntion of semantic similarity, for instance deﬁned by sets of\ncategories organized in a taxonomic hierarchy where each\ntask is classiﬁcation inside a subtree of the hierarchy ( e.g.,\nwe may say that classifying breeds of dogs is closer to clas-siﬁcation of cats than it is to classiﬁcation of species of\nplants). In this setting, we can deﬁne\nDtax(ta,tb) = min\ni∈Sa,j∈Sbd(i,j),\nwhereSa,Sbare the sets of categories in task ta,tband\nd(i,j)is an ultrametric or graph distance in the taxonomy\ntree. Notice that this is a proper distance, and in particular\nit is symmetric.\nTransfer distance. We deﬁne the transfer (or ﬁne-tuning)\ngain from a task tato a tasktb(which we improperly call\ndistance, but is not necessarily symmetric or positive) as\nthe difference in expected performance between a model\ntrained for task tbfrom a ﬁxed initialization (random or pre-\ntrained), and the performance of a model ﬁne-tuned for task\ntbstarting from a solution of task ta:\nDft(ta→tb) =E[ℓa→b]−E[ℓb]\nE[ℓb],\nwhere the expectations are taken over all trainings with the\nselected architecture, training procedure and network ini-\ntialization,ℓbis the ﬁnal test error obtained by training on\ntaskbfrom the chosen initialization, and ℓa→bis the error\nobtained instead when starting from a solution to task aand\nthen ﬁne-tuning (with the selected procedure) on task tb.\n3.1. Symmetric and asymmetric TASK 2VECmetrics'
 'obtained instead when starting from a solution to task aand\nthen ﬁne-tuning (with the selected procedure) on task tb.\n3.1. Symmetric and asymmetric TASK 2VECmetrics\nBy construction, the Fisher embedding on which\nTASK 2VEC is based captures fundamental information'
 'Meta-task Optimal Chance ImageNet TASK 2VEC Asymmetric TASK 2VEC MODEL 2VEC\niNat + CUB 31.24 +59.52% +30.18% +42.54% +9.97% +6.81%\nMixed 22.90 +112.49% +75.73% +40.30% +29.23% +27.81%\nTable 2: Model selection performance of different metrics. Average optimal error obtained on two meta-learning tasks\nby exhaustive search over the best expert, and relative error increase when using cheaper model selection methods. Always\npicking a ﬁxed good general model ( e.g., a model pretrained on ImageNet) performs better than picking an expert at random\n(chance). However, picking an expert using the Asymmetric TASK 2VEC distance can achieve an overall better performance\nthan using a general model. Notice also the improvement over the Symmetric version, especially on iNat + CUB, where\nexperts trained on very similar tasks may be too simple to yield good transfer, and should be avoided.\nstatistics are useful (analogous to our choice of probe net-\nwork) has also been considered, for example [9] train an\nautoencoder that learns to extract ﬁxed dimensional sum-\nmary statistics that can reproduce many different datasets\naccurately. However, for general vision tasks which apply\nto all natural images, the domain is the same across tasks.\nTaskonomy [39] explores the structure of the space of\ntasks, focusing on the question of effective knowledge\ntransfer in a curated collection of 26 visual tasks, ranging\nfrom classiﬁcation to 3D reconstruction, deﬁned on a com-\nmon domain. They compute pairwise transfer distances be-\ntween pairs of tasks and use the results to compute a di-\nrected hierarchy. Introducing novel tasks requires comput-\ning the pairwise distance with tasks in the library. In con-\ntrast, we focus on a larger library of 1,460 ﬁne-grained clas-\nsiﬁcation tasks both on same and different domains, and\nshow that it is possible to represent tasks in a topological\nspace with a constant-time embedding. The large task col-\nlection and cheap embedding costs allow us to tackle new\nmeta-learning problems.\nFisher kernels Our work takes inspiration from Jaakkola\nand Hausler [16]. They propose the “Fisher Kernel”, which\nuses the gradients of a generative model score function as a\nrepresentation of similarity between data items\nK(x(1),x(2)) =∇θlogP(x(1)|θ)TF−1∇θlogP(x(2)|θ).\nHereP(x|θ)is a parameterized generative model and Fis\nthe Fisher information matrix. This provides a way to utilize\ngenerative models in the context of discriminative learning.\nVariants of the Fisher kernel have found wide use as a repre-\nsentation of images [28, 29], and other structured data such\nas protein molecules [17] and text [30]. Since the genera-\ntive model can be learned on unlabelled data, several works\nhave investigated the use of Fisher kernel for unsupervised\nlearning [14, 31]. [35] learns a metric on the Fisher kernel\nrepresentation similar to our metric learning approach. Our\napproach differs in that we use the FIM as a representation\nof a whole dataset (task) rather than using model gradients\nas representations of individual data items.\nFisher Information for CNNs Our approach to task em-\nbedding makes use of the Fisher Information matrix of aneural network as a characterization of the task. Use of\nFisher information for neural networks was popularized by\nAmari [6] who advocated optimization using natural gra-\ndient descent which leverages the fact that the FIM is an\nappropriate parameterization-independent metric on statis-\ntical models. Recent work has focused on approximates of\nFIM appropriate in this setting (see e.g., [12, 10, 25]). FIM\nhas also been proposed for various regularization schemes\n[5, 8, 22, 27], analyze learning dynamics of deep networks\n[4], and to overcome catastrophic forgetting [19].\nMeta-learning and Model Selection The general prob-\nlem of meta-learning has a long history with much re-\ncent work dedicated to problems such as neural architecture\nsearch and hyper-parameter estimation. Closely related to']","The asymmetric distance accounts for task complexity and transferability, aiding in selecting feature extractors by evaluating their proximity to a task in the embedding space.",0.9166666666361111,,0.9490820486228055,1.0,0.8744472475218894,0.9263604186589864
Describe the computational approach to obtaining Task2Vec embeddings using a probe network.,"The Task2Vec embeddings are obtained by processing labeled samples through a pre-trained ""probe network"" to compute the diagonal Fisher Information Matrix (FIM) of the network's parameters. This involves feeding the dataset through the network, estimating the FIM, and averaging the Fisher Information for all weights in each filter to create a fixed-size representation. The resulting embedding captures the structure of the task, including its complexity and informative features, while remaining independent of the specifics of the dataset.","['TASK2VEC: Task Embedding for Meta-Learning\nAlessandro Achille\nUCLA and AWS\nachille@cs.ucla.eduMichael Lam\nAWS\nmichlam@amazon.comRahul Tewari\nAWS\ntewarir@amazon.comAvinash Ravichandran\nAWS\nravinash@amazon.com\nSubhransu Maji\nUMass and AWS\nsmmaji@amazon.comCharless Fowlkes\nUCI and AWS\nfowlkec@amazon.comStefano Soatto\nUCLA and AWS\nsoattos@amazon.comPietro Perona\nCaltech and AWS\nperonapp@amazon.com\nAbstract\nWe introduce a method to provide vectorial represen-\ntations of visual classiﬁcation tasks which can be used\nto reason about the nature of those tasks and their re-\nlations. Given a dataset with ground-truth labels and a\nloss function deﬁned over those labels, we process images\nthrough a “probe network” and compute an embedding\nbased on estimates of the Fisher information matrix asso-\nciated with the probe network parameters. This provides a\nﬁxed-dimensional embedding of the task that is independent\nof details such as the number of classes and does not require\nany understanding of the class label semantics. We demon-\nstrate that this embedding is capable of predicting task sim-\nilarities that match our intuition about semantic and tax-\nonomic relations between different visual tasks ( e.g., tasks\nbased on classifying different types of plants are similar).\nWe also demonstrate the practical value of this framework\nfor the meta-task of selecting a pre-trained feature extractor\nfor a new task. We present a simple meta-learning frame-\nwork for learning a metric on embeddings that is capable of\npredicting which feature extractors will perform well. Se-\nlecting a feature extractor with task embedding obtains a\nperformance close to the best available feature extractor,\nwhile costing substantially less than exhaustively training\nand evaluating on all available feature extractors.\n1. Introduction\nThe success of Deep Learning hinges in part on the fact\nthat models learned for one task can be used on other related\ntasks. Yet, no general framework exists to describe and\nlearn relations between tasks. We introduce the TASK 2VEC\nembedding, a technique to represent tasks as elements of a\nvector space based on the Fisher Information Matrix. The\nnorm of the embedding correlates with the complexity of\nthe task, while the distance between embeddings capturessemantic similarities between tasks (Fig. 1). When other\nnatural distances are available, such as the taxonomical dis-\ntance in biological classiﬁcation, we ﬁnd that the embed-\nding distance correlates positively with it (Fig. 2). More-\nover, we introduce an asymmetric distance on tasks which\ncorrelates with the transferability between tasks.\nComputation of the embedding leverages a duality be-\ntween network parameters (weights) and outputs (activa-\ntions) in a deep neural network (DNN): Just as the activa-\ntions of a DNN trained on a complex visual recognition task\nare a rich representation of the input images, we show that\nthe gradients of the weights relative to a task-speciﬁc loss\nare a rich representation of the task itself. Speciﬁcally, given\na task deﬁned by a dataset D={(xi,yi)}N\ni=1of labeled\nsamples, we feed the data through a pre-trained reference\nconvolutional neural network which we call “ probe net-\nwork ”, and compute the diagonal Fisher Information Ma-\ntrix (FIM) of the network ﬁlter parameters to capture the\nstructure of the task (Sect. 2). Since the architecture and\nweights of the probe network are ﬁxed, the FIM provides a\nﬁxed-dimensional representation of the task. We show this\nembedding encodes the “difﬁculty” of the task, character-\nistics of the input domain, and which features of the probe\nnetwork are useful to solve it (Sect. 2.1).\nOur task embedding can be used to reason about the\nspace of tasks and solve meta-tasks. As a motivating exam-\nple, we study the problem of selecting the best pre-trained\nfeature extractor to solve a new task. This can be particu-\nlarly valuable when there is insufﬁcient data to train or ﬁne-'
 'relatively few samples. In Fig. 4 we show how the perfor-\nmance of TASK 2VEC varies on a model selection task as the\nnumber of samples varies. At all sample sizes TASK 2VEC is\nclose to the optimum, and improves over selecting a generic\nexpert (ImageNet), both when ﬁne-tuning and when train-\ning only a classiﬁer. We observe that the best choice of ex-\nperts is not affected by the dataset size, and that even with\nfew examples TASK 2VEC is able to ﬁnd the optimal experts.\nChoice of probe network In Table 1 we show that\nDenseNet [15] and ResNet architectures [11] perform sig-\nniﬁcantly better when used as probe networks to compute\ntheTASK 2VEC embedding than a VGG [32] architecture.\n6. Related Work\nTask and Domain embedding. Tasks distinguished by\ntheir domain can be understood simply in terms of image\nstatistics. Due to the bias of different datasets, sometimes a\nbenchmark task may be identiﬁed just by looking at a few\nimages [34]. The question of determining what summary'
 '2.1. TASK 2VECembedding using a probe network\nWhile the network activations capture the information in\nthe input image which are needed to infer the image label,\nthe FIM indicates the set of feature maps which are more\ninformative for solving the current task. Following this in-\ntuition, we use the FIM to represent the task itself. How-\never, the FIMs computed on different networks are not di-\nrectly comparable. To address this, we use single “probe”\nnetwork pre-trained on ImageNet as a feature extractor and\nre-train only the classiﬁer layer on any given task, which\nusually can be done efﬁciently. After training is complete,\nwe compute the FIM for the feature extractor parameters.\nSince the full FIM is unmanageably large for rich probe\nnetworks based on CNNs, we make two additional approxi-\nmations. First, we only consider the diagonal entries, which\nimplicitly assumes that correlations between different ﬁlters\nin the probe network are not important. Second, since the\nweights in each ﬁlter are usually not independent, we aver-\nage the Fisher Information for all weights in the same ﬁlter.\nThe resulting representation thus has ﬁxed size, equal to the\nnumber of ﬁlters in the probe network. We call this embed-\nding method TASK 2VEC.\nRobust Fisher computation Since the FIM is a local\nquantity, it is affected by the local geometry of the training\nloss landscape, which is highly irregular in many deep net-\nwork architectures [21], and may be too noisy when trained\nwith few samples. To avoid this problem, instead of a direct\ncomputation, we use a more robust estimator that leverages\nconnections to variational inference. Assume we perturb\nthe weights ˆwof the network with Gaussian noise N(0,Λ)\nwith precision matrix Λ, and we want to ﬁnd the optimal Λ\nwhich yields a good expected error, while remaining close\nto an isotropic prior N( ˆw,λ2I). That is, we want to ﬁnd Λ\nthat minimizes:\nL( ˆw; Λ) = Ew∼N( ˆw,Λ)[Hpw,ˆpp(y|x)]\n+βKL (N(0,Λ)∥N(0,λ2I)),\nwhereHis the cross-entropy loss and βcontrols the weight\nof the prior. Notice that for β= 1 this reduces to the Evi-\ndence Lower-Bound (ELBO) commonly used in variational\ninference. Approximating to the second order, the optimal\nvalue of Λsatisﬁes (see Supplementary Material):\nβ\n2NΛ =F+βλ2\n2NI.\nTherefore,β\n2NΛ∼F+o(1)can be considered as an estima-\ntor of the FIM F, biased towards the prior λ2Iin the low-\ndata regime instead of being degenerate. In case the task is\ntrivial (the loss is constant or there are too few samples) the\nembedding will coincide with the prior λ2I, which we will\nrefer to as the trivial embedding . This estimator has theadvantage of being easy to compute by directly minimizing\nthe lossL( ˆw; Σ) through Stochastic Gradient Variational\nBayes [18], while being less sensitive to irregularities of\nthe loss landscape than direct computation, since the value\nof the loss depends on the cross-entropy in a neighborhood\nofˆwof size Λ−1. As in the standard Fisher computation,\nwe estimate one parameter per ﬁlter, rather than per weight,\nwhich in practice means that we constrain Λii= Λjjwhen-\neverwiandwjbelongs to the same ﬁlter. In this case, opti-\nmization ofL( ˆw; Λ)can be done efﬁciently using the local\nreparametrization trick of [18].\n2.2. Properties of the TASK 2VECembedding\nThe task embedding we just deﬁned has a number of\nuseful properties. For illustrative purposes, consider a two-\nlayer sigmoidal network for which an analytic expression\ncan be derived (see Supplementary Materials). The FIM\nof the feature extractor parameters can be written using the\nKronecker product as\nF=Ex,y∼ˆp(x)pw(y|x)[(y−p)2·S⊗xxT]\nwherep=pw(y= 1|x)and the matrix S=wwT⊙zzT⊙\n(1−z)(1−z)Tis an element-wise product of classiﬁer\nweightswand ﬁrst layer feature activations z. It is informa-\ntive to compare this expression to an embedding based only\non the dataset domain statistics, such as the (non-centered)\ncovarianceC0=E[\nxxT]\nof the input data or the covari-\nanceC1=E[\nzzT]\nof the feature activations. One could'
 '102103104\nNumber of samples-10%0%10%Error relative to brute force\n(lower is better)\nBrute force fixed\nImageNet fixed\nTask2Vec fixedImageNet finetune\nTask2Vec finetuneFigure 4: TASK 2VEC improves results at different\ndataset sizes and training conditions: Performance of\nmodel selection on a subset of 4 tasks as a function of\nthe number of samples available to train relative to opti-\nmal model selection (dashed orange). Training a classiﬁer\non the feature extractor selected by TASK 2VEC (solid red) is\nalways better than using a generic ImageNet feature extrac-\ntor (dashed red). The same holds when allowed to ﬁne-tune\nthe feature extractor (blue curves). Also notice that in the\nlow-data regime ﬁne-tuning the ImageNet feature extractor\nis more expensive and has a worse performance than accu-\nrately selecting a good ﬁxed feature extractor.\nProbe network Top-10 All\nChance +13.95% +59.52%\nVGG-13 +4.82% +38.03%\nDenseNet-121 +0.30% +10.63%\nResNet-13 +0.00% +9.97%\nTable 1: Choice of probe network. Mean relative error\nincrease over the ground-truth optimum on the iNat+CUB\nmeta-task for different choices of the probe-network. We\nalso report the performance on the top 10 tasks with more\nsamples to show how data size affect different architectures.\nembedding recovers similar clusters on iNaturalist. How-\never, on iMaterialst domain embedding collapses all tasks\nto a single uninformative cluster (not a single point due to\nslight noise in embedding computation).\nTask Embedding encodes task difﬁculty The scatter-\nplot in Fig. 3 compares the norm of embedding vectors vs.\nperformance of the best expert (or task speciﬁc model for\ncases where we have the diagonal computed). As shown\nanalytically for the two-layers model, the norm of the task\nembedding correlates with the complexity of the task also\non real tasks and architectures.\n5.2. Model Selection\nGiven a task, our aim is to select an expert feature extrac-\ntor that maximizes the classiﬁcation performance on that\ntask. We propose two strategies: (1) embed the task andselect the feature extractor trained on the most similar task,\nand (2) jointly embed the models and tasks, and select a\nmodel using the learned metric (see Section 4). Notice that\n(1) does not use knowledge of the model performance on\nvarious tasks, which makes it more widely applicable but\nrequires we know what task a model was trained for and\nmay ignore the fact that models trained on slightly differ-\nent tasks may still provide an overall better feature extrac-\ntor (for example by over-ﬁtting less to the task they were\ntrained on).\nIn Table 2 we compare the overall results of the various\nproposed metrics on the model selection meta-tasks. On\nboth the iNat+CUB and Mixed meta-tasks, the Asymmetric\nTASK 2VEC model selection is close to the ground-truth op-\ntimal, and signiﬁcantly improves over both chance, and over\nusing an generic ImageNet expert. Notice that our method\nhasO(1)complexity, while searching over a collection of\nNexperts isO(N).\nError distribution In Fig. 3 we show in detail the error\ndistribution of the experts on multiple tasks. It is interesting\nto notice that the classiﬁcation error obtained using most ex-\nperts clusters around some mean value, and little improve-\nment is observed over using a generic expert. On the other\nhand, a few optimal experts can obtain a largely better per-\nformance on the task than a generic expert. This conﬁrms\nthe importance of having access to a large collection of ex-\nperts when solving a new task, especially if few training\ndata are available. But this collection can only be efﬁciently\nexploited if an algorithm is given to efﬁciently ﬁnd one of\nthe few experts for the task, which we propose.\nDependence on task dataset size Finding experts is es-\npecially important when the task we are interested in has\nrelatively few samples. In Fig. 4 we show how the perfor-\nmance of TASK 2VEC varies on a model selection task as the\nnumber of samples varies. At all sample sizes TASK 2VEC is']","Task2Vec embeddings are obtained by processing images through a pre-trained probe network, retraining the classifier layer for the specific task, and computing the FIM of the network filter parameters.",0.999999999975,1.0,,1.0,0.942059436323928,0.9447083335310063
What limitations does Task2Vec face regarding its ability to capture the full complexity of tasks in real-world applications?,"Task2Vec relies solely on task-specific information and does not account for interactions with the model, which may play a significant role in task performance. Additionally, while it captures the general characteristics of tasks, it may not fully represent the nuanced complexities and variations present in real-world applications. As a result, its effectiveness can be limited when tasks exhibit unique features not encapsulated by the embedding.","['lem of meta-learning has a long history with much re-\ncent work dedicated to problems such as neural architecture\nsearch and hyper-parameter estimation. Closely related to\nour problem is work on selecting from a library of classi-\nﬁers to solve a new task [33, 2, 20]. Unlike our approach,\nthese usually address the question via land-marking or ac-\ntive testing, in which a few different models are evaluated\nand performance of the remainder estimated by extension.\nThis can be viewed as a problem of completing a matrix\ndeﬁned by performance of each model on each task.\nA similar approach has been taken in computer vision for\nselecting a detector for a new category out of a large library\nof detectors [26, 40, 38].\n7. Discussion\nTASK 2VEC is an efﬁcient way to represent a task, or the\ncorresponding dataset, as a ﬁxed dimensional vector. It has\nseveral appealing properties, in particular its norm corre-\nlates with the test error obtained on the task, and the co-\nsine distance between embeddings correlates with natural\ndistances between tasks, when available, such as the taxo-\nnomic distance for species classiﬁcation, and the ﬁne-tuning\ndistance for transfer learning. Having a representation of\ntasks paves the way for a wide variety of meta-learning\ntasks. In this work, we focused on selection of an expert\nfeature extractor in order to solve a new task, especially\nwhen little training data is present, and showed that using\nTASK 2VEC to select an expert from a collection can sen-\nsibly improve test performance while adding only a small\noverhead to the training process.'
 'relatively few samples. In Fig. 4 we show how the perfor-\nmance of TASK 2VEC varies on a model selection task as the\nnumber of samples varies. At all sample sizes TASK 2VEC is\nclose to the optimum, and improves over selecting a generic\nexpert (ImageNet), both when ﬁne-tuning and when train-\ning only a classiﬁer. We observe that the best choice of ex-\nperts is not affected by the dataset size, and that even with\nfew examples TASK 2VEC is able to ﬁnd the optimal experts.\nChoice of probe network In Table 1 we show that\nDenseNet [15] and ResNet architectures [11] perform sig-\nniﬁcantly better when used as probe networks to compute\ntheTASK 2VEC embedding than a VGG [32] architecture.\n6. Related Work\nTask and Domain embedding. Tasks distinguished by\ntheir domain can be understood simply in terms of image\nstatistics. Due to the bias of different datasets, sometimes a\nbenchmark task may be identiﬁed just by looking at a few\nimages [34]. The question of determining what summary'
 'ple, we study the problem of selecting the best pre-trained\nfeature extractor to solve a new task. This can be particu-\nlarly valuable when there is insufﬁcient data to train or ﬁne-\ntune a generic model, and transfer of knowledge is essen-\ntial. TASK 2VEC depends solely on the task, and ignores\ninteractions with the model which may however play an\nimportant role. To address this, we learn a joint task and\nmodel embedding, called MODEL 2VEC, in such a way that\nmodels whose embeddings are close to a task exhibit good\nperfmormance on the task. We use this to select an expert\nfrom a given collection, improving performance relative to\n1arXiv:1902.03545v1  [cs.LG]  10 Feb 2019'
 'TASK2VEC: Task Embedding for Meta-Learning\nAlessandro Achille\nUCLA and AWS\nachille@cs.ucla.eduMichael Lam\nAWS\nmichlam@amazon.comRahul Tewari\nAWS\ntewarir@amazon.comAvinash Ravichandran\nAWS\nravinash@amazon.com\nSubhransu Maji\nUMass and AWS\nsmmaji@amazon.comCharless Fowlkes\nUCI and AWS\nfowlkec@amazon.comStefano Soatto\nUCLA and AWS\nsoattos@amazon.comPietro Perona\nCaltech and AWS\nperonapp@amazon.com\nAbstract\nWe introduce a method to provide vectorial represen-\ntations of visual classiﬁcation tasks which can be used\nto reason about the nature of those tasks and their re-\nlations. Given a dataset with ground-truth labels and a\nloss function deﬁned over those labels, we process images\nthrough a “probe network” and compute an embedding\nbased on estimates of the Fisher information matrix asso-\nciated with the probe network parameters. This provides a\nﬁxed-dimensional embedding of the task that is independent\nof details such as the number of classes and does not require\nany understanding of the class label semantics. We demon-\nstrate that this embedding is capable of predicting task sim-\nilarities that match our intuition about semantic and tax-\nonomic relations between different visual tasks ( e.g., tasks\nbased on classifying different types of plants are similar).\nWe also demonstrate the practical value of this framework\nfor the meta-task of selecting a pre-trained feature extractor\nfor a new task. We present a simple meta-learning frame-\nwork for learning a metric on embeddings that is capable of\npredicting which feature extractors will perform well. Se-\nlecting a feature extractor with task embedding obtains a\nperformance close to the best available feature extractor,\nwhile costing substantially less than exhaustively training\nand evaluating on all available feature extractors.\n1. Introduction\nThe success of Deep Learning hinges in part on the fact\nthat models learned for one task can be used on other related\ntasks. Yet, no general framework exists to describe and\nlearn relations between tasks. We introduce the TASK 2VEC\nembedding, a technique to represent tasks as elements of a\nvector space based on the Fisher Information Matrix. The\nnorm of the embedding correlates with the complexity of\nthe task, while the distance between embeddings capturessemantic similarities between tasks (Fig. 1). When other\nnatural distances are available, such as the taxonomical dis-\ntance in biological classiﬁcation, we ﬁnd that the embed-\nding distance correlates positively with it (Fig. 2). More-\nover, we introduce an asymmetric distance on tasks which\ncorrelates with the transferability between tasks.\nComputation of the embedding leverages a duality be-\ntween network parameters (weights) and outputs (activa-\ntions) in a deep neural network (DNN): Just as the activa-\ntions of a DNN trained on a complex visual recognition task\nare a rich representation of the input images, we show that\nthe gradients of the weights relative to a task-speciﬁc loss\nare a rich representation of the task itself. Speciﬁcally, given\na task deﬁned by a dataset D={(xi,yi)}N\ni=1of labeled\nsamples, we feed the data through a pre-trained reference\nconvolutional neural network which we call “ probe net-\nwork ”, and compute the diagonal Fisher Information Ma-\ntrix (FIM) of the network ﬁlter parameters to capture the\nstructure of the task (Sect. 2). Since the architecture and\nweights of the probe network are ﬁxed, the FIM provides a\nﬁxed-dimensional representation of the task. We show this\nembedding encodes the “difﬁculty” of the task, character-\nistics of the input domain, and which features of the probe\nnetwork are useful to solve it (Sect. 2.1).\nOur task embedding can be used to reason about the\nspace of tasks and solve meta-tasks. As a motivating exam-\nple, we study the problem of selecting the best pre-trained\nfeature extractor to solve a new task. This can be particu-\nlarly valuable when there is insufﬁcient data to train or ﬁne-']","While effective, Task2Vec's embeddings may not fully capture the entire spectrum of task complexity and diversity found in broader real-world applications, indicating room for future enhancements.",0.9999999999666667,1.0,0.0,1.0,0.4645473862729017,0.9351126220146837
"How does GLM-130B manage to achieve INT4 weight quantization without post-training, and what are the benefits?","GLM-130B achieves INT4 weight quantization without post-training due to its unique architecture, which has narrower weight value distributions compared to GPT-style models. This allows for effective quantization with minimal performance degradation, resulting in nearly the same accuracy as higher precision models. The benefits include significantly reduced GPU memory requirements and the ability to run on more affordable hardware, making it accessible for large-scale language model applications.","['cause such a size supports inference on a single A100 (8 ×40G) server. Second, to further lower the\nGPU requirements, we quantize GLM-130B into INT4 precision without post training while OPT\nand BLOOM can only reach INT8. Due to a unique property of the GLM architecture, GLM-130B’s\nINT4 quantization introduces negligible performance degradation, e.g., -0.74% on LAMBADA and\neven +0.05% on MMLU, making it still better than the uncompressed GPT-3. This enables GLM-\n130B’s fast inference with performance guarantee on a server of 4 ×RTX 3090 (24G) or 8 ×RTX\n2080 Ti (11G), the most affordable GPU required for using 100B-scale LLMs to date.\n2'
 'Published as a conference paper at ICLR 2023\nTable 2: Left: Quantized GLM-130B’s performance on several benchmarks; Right: INT4 quantized\nGLM-130B’s inference speed (encode and decode) with FasterTransformer.\nModel PrecisionGLM-130B GPT-3\nFP16 INT8 INT4 FP16\nMMLU (acc, ↑) 44.75 44.71 44.80 43.9\nLAMBADA (acc, ↑) 80.21 80.21 79.47 76.2\nPile (a part, BPB, ↓) 0.634 0.638 0.641 0.74GPU Type 128 Enc./Dec. 512 Enc./Dec,\n8×A100 (40G) 0.15s 4.29s 0.18s 17.7s\n8×V100 (32G) 0.31s 6.97s 0.67s 28.1s\n4×RTX 3090 (24G) 0.37s 8.16s 1.30s 32.3s\n8×RTX 2080 Ti (11G) 0.39s 6.77s 1.04s 27.3s\nExcitingly, we manage to reach the INT4 weight quantization for GLM-130B while existing suc-\ncesses have thus far only come to the INT8. Memory-wise, by comparing to INT8, the INT4 version\nhelps additionally save half of the required GPU memory to 70GB, thus allowing GLM-130B infer-\nence on 4 ×RTX 3090 Ti (24G) or 8 ×RTX 2080 Ti (11G). Performance-wise, Table 2 left indicates\nthat without post-training at all, the INT4-version GLM-130B experiences almost no performance\ndegradation, thus maintaining the performance advantages over GPT-3 on common benchmarks.\nGLM’s INT4 Weight Quantization Scaling Law. We examine the underlying mechanism of this\nunique INT4 weight quantization scaling law exhibited in Figure 5 right. We plot the weight value\ndistributions in Figure 5 left, which turns out to directly impact the quantization quality. Specifically,\na wider-distributed linear layer needs to be quantized with larger bins, leading to more precision loss.\nThus the wide-distributed attn-dense andw2matrices explain the INT4 quantization failure for\nGPT-style BLOOM. Conversely, GLMs tend to have much narrower distributions than those of\nsimilar-sized GPTs, and the gap between INT4 and FP16 versions keeps further decreasing as the\nGLM model size scales up (Cf. Figure 15 in Appendix for details).\n5 T HERESULTS\nWe follow the common settings in LLMs such as GPT-3 and PaLM to evaluate GLM-130B for\nEnglish1. As a bilingual LLM with Chinese, GLM-130B is also evaluated on Chinese benchmarks.\nDiscussion on the Scope of Zero-Shot Learning in GLM-130B. Since GLM-130B has been\ntrained with MIP, here we clarify its scope of zero-shot evaluation. In fact, “zero-shot” seems to\nhave controversial interpretations without a consensus in the community. We follow one of the in-\nfluential related surveys (Xian et al., 2018), which says “At test time, in zero-shot learning setting,\nthe aim is to assign a test image to an unseen class label” where involving unseen class labels is a\nkey. Therefore, we derive our criterion to pick GLM-130B’s zero-shot (and few-shot) datasets as:\n•English : 1) For tasks with fixed labels (e.g., natural language inference ): no datasets in such tasks\nshould be evaluated on; 2) For tasks without fixed labels (e.g., (multiple-choice) QA, topic classi-\nfication ): only datasets with an obvious domain transfer from those in MIP should be considered.\n•Chinese : All datasets can be evaluated as there exists a zero-shot cross-lingual transfer.\nFiltering Test Datasets. Following prior practices (Brown et al., 2020; Rae et al., 2021) and our\ncriterion mentioned above, we filter and refrain to report potentially contaminated datasets’ evalua-\ntion results. For LAMBADA and CLUE, we find minimal overlap under the 13-gram setting. Pile,\nMMLU, and BIG-bench are either held-out or released later than the crawling of corpora.\n5.1 L ANGUAGE MODELING\nLAMBADA. LAMBADA (Paperno et al., 2016) is a dataset to test the last word language model-\ning capability. The results previously shown in Figure 2 suggest GLM-130B achieves a zero-shot\naccuracy of 80.2 with its bidirectional attention, setting up a new record on LAMBADA.\nTable 3: GLM-130B’s average BPB on\nPile evaluation (18 sub-datasets).\nJurassic-1 GPT-3 GLM-130B\nAvg. BPB 0.650 0.742 0.634Pile. The Pile test-set (Gao et al., 2020) includes a series\nof benchmarks for language modeling. On average, GLM-'
 'aims to teach machines to think like humans. After another week of testing, we finally kicked off\nthe training of the GLM-130B model on its 96 A100 (40G * 8) servers on May 6th. Additionally,\nZhipu.AI also sent a team to help evaluate the pre-trained model and build a demonstration website.\nThe training period spanned two months, during which we began developing a toolkit to allow GLM-\n130B’s inference in low-resource setting with swapping technique and quantization. Though it is\nalready the most accessible model of its scale, together with our partner from Tsinghua NLP, we have\nbeen exploring the limit of popularized hardware platforms, which would truly make the 100B-scale\nmodel accessible to as many people as possible. To date, we managed to reach the INT4 weight\nquantization for GLM-130B. Importantly, the INT4 version of GLM-130B without post training\n16This section is largely extracted and updated from the blog introduction of GLM-130B at http://keg.\ncs.tsinghua.edu.cn/glm-130b/ (Posted date: August 4, 2022).\n53'
 'Published as a conference paper at ICLR 2023\ncation for higher-precision computation in outlying dimensions proposed in (Dettmers et al., 2022)\nis not applicable to GLM-130B.\nFigure 13: GLM-130B’s\nactivation outliers’ absolute\nvalue scale.We study whether these outliers can be ignored in LLM quantiza-\ntion, and the answer is interestingly “no”. These values can be sev-\neral orders of magnitude larger than ordinary activation values (Cf.\nFigure 13). While most values (accounts for 99.98% dimensions in\na hidden state) stay less them 6, those two outlying dimensions can\nreach 50 or even over 100. They are speculated to be some important\nclues for GLM-130B and potentially other LLMs to memorize some\nfixed world or language knowledge, and thus removing or omitting\nthem in quantization can lead to significant performance degradation.\nB.7 W EIGHT QUANTIZATION\nB.7.1 P RELIMINARIES\nAbsmax Quantization is a symmetric quantization that a range of [−absmax( x),absmax( x)]is\nmapped to [−(2b−1),2b−1]forx.\nsx=absmax( x)\n2b−1−1(7)\nxq= round( x/sx) (8)\nwhere sxis the scaling factor, xqis the quantization result and bis the bit width.\nZeropoint Quantization is an asymmetric quantization that a range of [min( x),max( x)]is\nmapped to [−(2b−1),2b−1].\nsx=max( x)−min(x)\n2b−2(9)\nzx= round(min( x)/sx) + 2b−1−1 (10)\nxq= round( x/sx)−zx (11)\nwhere zxis the zero point.\nCol/Row-wise Quantization Using a single scaling factor for the weight matrix often leads to\nmore quantization errors because one single outlier leads to a decrease in the quantization precision\nof all other elements. A common workaround is to group the weight matrix by rows or by columns,\nwith each group being quantized separately and having independent scaling factors.\nB.8 Q UANTIZATION SETTINGS\nOur goal is to save GPU memory as much as possible without hurting model performance. In prac-\ntice, we only quantize linear layers, which take up most of the transformer parameters, and leave\ninput/output embedding, layer normalization, and bias terms unchanged. At the quantization pre-\ncision of INT4, two INT4 weights are compressed into one INT8 weight for saving GPU memory\nusage. Absmax quantization is adopted since we found it enough to maintain model performance,\nand it is more computationally efficient than zeropoint quantization. During inference, only quan-\ntized weights are stored in GPU memory, the FP16 weights for linear layers will be dequantized at\nruntime.\nB.8.1 Q UANTIZATION RESULTS AT SCALES\nGLM models at 110M to 10B scale are from GLM’s original paper(Du et al., 2022). Although the\narchitecture of smaller scale GLMs are not the same as GLM-130B, we believe that the training\nobjective is the key factor for quantization. Table 10 shows the performance of GLM and BLOOM\nfamily models at different scales on the LAMBADA dataset with different quantization methods.\nAlmost all models maintain performance at INT8 precision. In general, GLM maintains better\nperformance than BLOOM at INT4 precision as it scales.\n28']","By leveraging the GLM architecture's unique scaling law, GLM-130B achieves INT4 weight quantization, reducing memory requirements for inference with minimal performance degradation.",0.999999999975,1.0,0.9471886732076561,1.0,0.5353803618447589,0.9415214473790353
What contributions does GLM-130B offer to the open-source community and AI research field?,"GLM-130B offers open access to its weights and code, allowing researchers to analyze model parameters and internal states for in-depth studies, which is not possible with most limited inference APIs. It significantly lowers the hardware requirements for inference through speed-up implementations and INT4 quantization, making it accessible to individual developers and small companies. This open bilingual pre-trained language model promotes innovation and reduces costs in integrating large language models into various applications.","['Published as a conference paper at ICLR 2023\nE C ONTRIBUTIONS\nThe GLM-130B project was conceived in Dec. 2021 with its pre-training part completed in July 3rd,\n2022 and its evaluation and applications still ongoing. Over the course, we have experienced various\ntechnical and engineering challenges (Cf. Appendix F and Figure 21 for details). It would not be\npossible to reach its current status if without the collaboration of multiple teams—the Knowledge\nEngineering Group (KEG), Parallel Architecture & Compiler technology of Mobile, Accelerated,\nand Networked systems Group (PACMAN), and Natural Language Processing Group (THUNLP) at\nTsinghua University, as well as Zhipu.AI. The detailed contributions are listed below.\nE.1 P REPARATION\n•Model Implementation: Aohan Zeng, Zhengxiao Du\n•Self-Supervised Data Processing: Ming Ding, Wendi Zheng\n•Multitask Data Processing: Xiao Liu, Xiao Xia\n•Model Architecture: Aohan Zeng, Xiao Liu, Zhengxiao Du, Hanyu Lai\n•Training Stability: Aohan Zeng, Xiao Liu, Ming Ding\n•3D-Parallelism and Training Efficiency: Aohan Zeng, Zixuan Ma, Jiaao He, Zhenbo Sun\nE.2 M ODEL TRAINING\n•Large-Scale Training & Monitoring: Aohan Zeng, Xiao Liu\n•Model Performance Validation: Aohan Zeng\nE.3 P OST TRAINING\n•Evaluation Framework: Aohan Zeng, Zhengxiao Du\n•Language Modeling Evaluation: Aohan Zeng\n•MMLU & BIG-Bench Evaluation: Aohan Zeng\n•CLUE & FewCLUE Evaluation: Xiao Liu, Aohan Zeng\n•Ethical Evaluation: Yifan Xu, Aohan Zeng, Xiao Liu, Zihan Wang\n•Baseline Evaluation: Xiao Liu, Jifan Yu, Weng Lam Tam\n•INT4 Quantization: Aohan Zeng, Zihan Wang, Xiao Liu, Hanyu Lai\n•Inference Acceleration: Zihan Wang, Aohan Zeng\n•Low-Resource Inference: Gouyang Zeng, Xu Han, Weilin Zhao, Zhiyuan Liu\n•Demo and API: Hanyu Lai, Jifan Yu, Xiaohan Zhang, Yufei Xue, Shan Wang, Jiecai Shan, Hao-\nhan Jiang, Zhengang Guo\n•Manuscript Writing: Xiao Liu, Yuxiao Dong, and Jie Tang wrote the main paper, and Xiao Liu,\nAohan Zeng, and Zhengxiao Du wrote the Appendix.\nE.4 P ROJECT MANAGEMENT\n•Student Leaders: Aohan Zeng, Xiao Liu\n•Technical Advisors: Yuxiao Dong, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Jie\nTang\n•Project Leader: Jie Tang\nE.5 C OMPUTATION SPONSOR\n•GPU Sponsor: Zhipu.AI\n52'
 'ence. Altogether, it contributes to the high quality of GLM-130B in terms of both language perfor-\nmance on 112 tasks and ethical results on bias and toxicity benchmarks. Our experiences of both\nsuccess and failure are condensed into the lessons for training 100B-scale LLMs, attached in the\nAppendix B.10.\n9'
 'Published as a conference paper at ICLR 2023\nfaces negligible performance degradation compared to its uncompressed original, while it consumes\nonly 25% of the GPU memory required by the uncompressed version, thus supporting its effective\ninference on 4 ×RTX 3090 Ti (24G) or 8 ×RTX 2080 Ti (11G). We will attempt to further reduce\nthe resource requirements and keep the community updated on this important working item.\nG B ROADER IMPACT\nThis paper introduces an open bilingual pre-trained language model with 130 billion parameters.\nCurrently most pre-trained language models with over 100 billion parameters are privately owned\nby governments and large corporations (Brown et al., 2020; Thoppilan et al., 2022; Rae et al., 2021;\nChowdhery et al., 2022; Wang et al., 2021). A few of them (Brown et al., 2020; Lieber et al., 2021)\nprovide limited inference APIs with fees. In contrast, the weights and code of GLM-130B are open\nto anyone who is interested in LLMs. Moreover, we significantly lower the hardware requirements\nfor inference by speed-up implementation and INT4 quantization. The paper can have a broader\nimpact on the research community, individual developers and small companies, and society.\nG.1 I MPACT ON AI R ESEARCH\nMost research institutions cannot afford the substantial cost of pretraining large language models.\nAs a result, most researchers, except employees of governments and large corporations, only have\naccess to the limited inference APIs with fees. With the inference APIs, researchers can only analyze\nthe outputs of models as black boxes, which limits the scope of potential work. With GLM-130B,\nresearchers can analyze the model parameters and internal states corresponding to specific inputs,\nleading to in-depth studies of LLMs’ theory, capacity, and flaws. Researchers can also modify the\nmodel architecture and weights, to validate the proposed algorithms to improve LLMs Zhu et al.\n(2020); Cao et al. (2021); Hase et al. (2021); Mitchell et al. (2022).\nWith INT4 quantization, GLM-130B can perform inference on popularized GPUs such as 4 ×RTX\n3090 or 8 ×RTX 2080 Ti, which can be easily accessed from cloud service. As a result, researchers\nwho cannot afford powerful data-center GPU servers like DGX-A100 can also utilize GLM-130B.\nG.2 I MPACT ON INDIVIDUAL DEVELOPERS AND SMALL COMPANIES\nCurrently, individual developers and small companies who want to integrate LLMs into their busi-\nness can only choose paid inference APIs. The increased cost can hinder their attempts. Instead,\nGLM-130B can be deployed on popularized hardware that they own or can access via cloud service\nto reduce the cost. Furthermore, they can utilize distillation techniques Sanh et al. (2019); Jiao et al.\n(2020) to obtain smaller models that preserve comparable performance on their specific tasks. While\nsome developers may lack the ability to complete deployment and distillation on their own, we be-\nlieve with GLM-130B and more open LLMs in the future, the corresponding toolkits and service\nproviders will become more available.\nWe also note that currently most applications of LLMs are based on prompt engineering, partly\ndue to the limitation of inference APIs. In downstream scenarios such as online customer service,\nthe companies accumulate huge amounts of human-generated data that contain domain knowledge.\nWith the open-source weights and code, developers can finetune GLM-130B on their own data to\nmitigate the gap of domain knowledge.\nG.3 S OCIAL IMPACT\nLarge language models, together with other machine learning models in different modalities (e.g.,\nImage (Ramesh et al., 2021; Ding et al., 2021; Saharia et al.) and Video (Hong et al., 2022)), could\nbe used to generate synthetic text for harmful applications, such as telemarketing fraud, political\npropaganda, and personal harassment as is discussed in (Weidinger et al., 2021; Sheng et al., 2021;\nDev et al., 2021). We do not anticipate any hazardous outputs, especially towards vulnerable and'
 'Published as a conference paper at ICLR 2023\nF A B RIEF HISTORY OF GLM-130B\nThe GLM-130B project16was conceived in Dec. 2021 in a brainstorming meeting at Tsinghua KEG.\nWe firmly believe that it is of value to pre-train a highly accurate language model, in particular for\nboth Chinese and English. Though GPT-3 (Brown et al., 2020) is the pioneer for this effort, it is not\navailable to most people in the world. In addition, it supports English only. We therefore decide to\ninitialize the project GLM-130B. Please note that the WuDao 1.75T model we built last year is a\nsparse model with 480 mixture-of-experts (MoE), rather than a dense one as GPT-3. Our goal then\nis to train a bilingual pre-trained dense model with high accuracy on downstream tasks, and to make\nit open to everyone in the world-anyone, anywhere can download it and use it on a single server with\nappropriate GPUs.\nThe ambitious project soon faced several important challenges:\n•Lack of computational resources : No organization is willing to sponsor such a big project and\nfreely make it public.\n•Lack of a robust pre-training algorithm : Despite GPT-3’s success on English corpus, it is\nunclear how to train a high-accurate bilingual model for both English and Chinese.\n•Lack of fast inference solutions : Since the goal is to have the model public to everyone, we need\nto design fast inference solutions with low resource requirements to run the model.\nFor the pre-training algorithm, we finally chose GLM (Du et al., 2022) due to its high performance\nin practice. We eventually decided to train a GLM model of 130 billion parameters after several\nrounds of discussions and exploration, because such a size makes it possible to run the inference on\na single A100 (40G * 8) server.\nOur first attempt at training the model was in January 2022, shortly after we received a small sponsor\nof GPUs for test running. However, we soon realized that we had significantly underestimated the\ntechnical difficulties of pre-training a model at such a scale (>100B). It seems that pre-training a\nhighly accurate 100B-scale model is quite different from training a 10B-scale one. Due to frequent\nrandom hardware failures, model gradients exploding, unexpected excessive memory usage in the\nalgorithm, debug for the 3D pipeline in the new Megatron and DeepSpeed frameworks, inability to\nrecover from optimizer states, blocked TCP responses between processes, and many many unex-\npected “bugs”, the project was delayed for many times. The Tsinghua PACMAN team gave us a\nhand at this difficult time and together we successfully fixed most of the “bugs”.\nBy March, we were still short on computational resources, but fortunately got a chance to try test\nruns on several other platforms, including Ascend 910, Hygon DCU, NVIDIA, and Sunway. The\nimmediate challenge was for us to adapt our training code to these different platforms, as the under-\nlying operators are quite different. Also, it introduced many new issues: the element-wise operators\nnot supporting fast computation for large-dimension vectors, various issues that hindered conver-\ngence—the large gradient norms of input embeddings, native Post-LN, Pre-LN, and Sandwich-LN,\ndataloader state seeds, and computation precision choices in Softmax and Attention — as well as\nnumerous mistakes we ourselves made. With tremendous help from all of our generous partners, we\nfinally succeeded in making our pre-training algorithms runnable across all the platforms—frankly,\na surprising achievement for this project. The timeline of GLM-130B in Figure 21 covers most of\nthe issues we have encountered and addressed as of this writing.\nOn April 26th, we received a generous computing sponsorship from Zhipu.AI — an AI startup that\naims to teach machines to think like humans. After another week of testing, we finally kicked off\nthe training of the GLM-130B model on its 96 A100 (40G * 8) servers on May 6th. Additionally,']"," GLM-130B is fully open-sourced, including model weights, code, and training logs, promoting transparency, reproducibility, and inclusivity in large-scale language model research and applications.",0.8055555555287036,0.875,0.8802585384106102,1.0,0.3552567882453702,0.9210271529814809
What advancements does Megatron-LM contribute to the handling of layer normalization in BERT-like models to increase performance?,"Megatron-LM improves layer normalization in BERT-like models by using GeLU nonlinearities and applying layer normalization to the inputs of multi-head attention and feedforward layers, instead of the outputs as done in the original transformer architecture. This modification enables stable training and lowers training loss, which can lead to better overall performance. Additionally, these changes support the scaling of larger models effectively.","['Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nContext : The 36th International Conference on Machine\nLearning (ICML 2019) will be held in Long Beach, CA,\nUSA from June 10th to June 15th, 2019. The conference\nwill consist of one day of tutorials (June 10), followed\nby three days of main conference sessions (June 11-13),\nfollowed by two days of workshops (June 14-15).\nMegatron-LM : With a broad scope, the conference ad-\ndresses the challenges and opportunities in machine learning\nfor practitioners and researchers. The conference program\nwill feature sessions that cover different machine learning\ntopics, from reinforcement learning to computational biol-\nogy and from AI to deep learning. It is the world’s premier\nforum for exploring and presenting advances in machine\nlearning.\nICML 2019 will offer an impressive range of sessions,\nwhich will be of interest to practitioners, researchers, and\nstudents. Some of the topics we will cover include:\nReinforcement learning\nSoft Computing\nComputational biology\nConvolutional networks\nDeep learning\nStatistical learning theory\nStatistical learning machine\nNatural language processing\nBiology\nAwards\nSince its ﬁrst edition, the conference has grown and evolved.\nToday, it is the premier forum for showcasing state-of-the-\nart research in machine learning. The conference is widely\nrecognized as the preeminent venue for showcasing the\nlatest research in machine learning, and a place where prac-\ntitioners, researchers, and students can exchange ideas and\ninsights.\nThe conference is a peer-reviewed venue that draws its pres-\ntige and inﬂuence from the excellence of the presentations\nand the quality of the invited speakers. With over 50% of all\ntalks selected based on their content by an expert committee,\nit is clear that the conference’s high standards of research\nand quality keep a very high bar.Context : Rescue workers on Tuesday found a ﬂight\nrecorder from the Russian plane that crashed into the Black\nSea over the weekend, the defense ministry said. All 92\npeople aboard the Russian military’s Tu-154 plane are be-\nlieved to have died Sunday morning when it crashed two\nminutes after taking off from the southern Russian city of\nSochi.\nMegatron-LM : A child holds up a portrait of a dead relative\nat a makeshift morgue in a building belonging to a local\ngovernment in the town of Petropavlovsk-Kamchatsky. A\nRussian Defense Ministry Tu-154 plane with 92 people\non board crashed into the Black Sea on Sunday morning\nshortly after taking off from the southern Russian city of\nSochi. (Yuri Kochetkov/EPA)\nAccording to the ministry, the plane was found at a depth\nof more than 1,000 feet and the recorder was damaged.\n”The defense ministry has decided to hand over the recorder\nto the Interstate Aviation Committee, which is the main\ninvestigative body in the air crash,” the statement said.\nThe search for the plane’s black boxes is being hampered by\nstrong winds and rough seas. Investigators have launched a\ncriminal probe into the crash, which ofﬁcials said was likely\ncaused by a technical fault.\nRussian President Vladimir Putin declared Wednesday a\nday of mourning for the victims.\nD. Further Scaling Analysis\nIn this section we study the effect of number of attention\nheads on the scaling results. We also present strong scaling\nresults for our 1.2 billion parameter model.\nD.1. Attention Heads and Scaling\nThis section studies the effect of attention heads on model\nparallel scaling. To this end, we consider the 8.3 billion\nparameter conﬁguration with 8-way model parallelism and\nvary the number of heads from 16 to 32. The results are\npresented in Table 7. As the number of attention heads\nincreases, some of the GEMMS inside the self-attention\nlayer become smaller and also the number of elements in\nthe self attention softmax increases. This results in a slight\ndecrease in scaling efﬁciency. Future research should be\nwary of this hyperparameter to design large transformer'
 'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nTable 5. Development set results for MNLI, QQP, SQuAD 1.1 and SQuAD 2.0 and test set results for RACE. The trained tokens represents\nconsumed tokens during model pretraining (proportional to batch size times number of iterations) normalized by consumed tokens during\nmodel pretraining for our 336M model.\nModeltrained tokens MNLI m/mm QQP SQuAD 1.1 SQuAD 2.0 RACE m/h\nratio accuracy accuracy F1 / EM F1 / EM accuracy\n(dev set) (dev set) (dev set) (dev set) (test set)\nRoBERTa (Liu et al., 2019b) 2 90.2 / 90.2 92.2 94.6 / 88.9 89.4 / 86.5 83.2 (86.5 / 81.8)\nALBERT (Lan et al., 2019) 3 90.8 92.2 94.8 / 89.3 90.2 / 87.4 86.5 (89.0 / 85.5)\nXLNet (Yang et al., 2019) 2 90.8 / 90.8 92.3 95.1 / 89.7 90.6 / 87.9 85.4 (88.6 / 84.0)\nMegatron-336M 1 89.7 / 90.0 92.3 94.2 / 88.0 88.1 / 84.8 83.0 (86.9 / 81.5)\nMegatron-1.3B 1 90.9 / 91.0 92.6 94.9 / 89.1 90.2 / 87.1 87.3 (90.4 / 86.1)\nMegatron-3.9B 1 91.4 / 91.4 92.7 95.5 / 90.0 91.2 / 88.5 89.5 (91.8 / 88.6)\nALBERT ensemble (Lan et al., 2019) 95.5 / 90.1 91.4 / 88.9 89.4 (91.2 / 88.6)\nMegatron-3.9B ensemble 95.8 / 90.5 91.7 / 89.0 90.9 (93.1 / 90.0)\nFigure 7. Training loss for BERT model using the original architec-\nture (a) and the rearranged architecture (b). Left ﬁgure shows the\ntraining loss for 336M and 752M BERT model. While the original\narchitecture performs well on the 336M model, the modiﬁcations\nin (b) enable stable training with lower training loss.\nUsing the architecture change in Figure 7(b), we consider\nthree different cases as detailed in Table 4. The 336M model\nhas the same size as BERT-large. The 1.3B is the same as\nthe BERT-xlarge conﬁguration that was previously shown\nto get worse results than the 336M BERT-large model (Lan\net al., 2019). We further scale the BERT model using both\nlarger hidden size as well as more layers to arrive at the 3.9B\nparameter case. In all cases, the hidden size per attention\nhead is kept constant at 64. 336M and 1.3B models are\ntrained for 2 million iterations while the 3.9B model is\ntrained for 1.5 million iterations and is still training.\nOn a 3% held-out set, 336M, 1.3B, and 3.9B models achieve\nvalidation set perplexity of 1.58, 1.30, and 1.16, respectively,\na monotonic decrease with the model size. We ﬁnetune\nthe trained models on several downstream tasks including\nMNLI and QQP from the GLUE benchmark (Wang et al.,\n2019), SQuAD 1.1 and SQuAD 2.0 from the Stanford Ques-\ntion answering dataset (Rajpurkar et al., 2016; 2018), and\nthe reading comprehension RACE dataset (Lai et al., 2017).\nFor ﬁnetuning, we follow the same procedure as (Liu et al.,\n2019b). We ﬁrst perform hyperparameter tuning on batchsize and learning rate. Once we obtain the best values, we\nreport the median development set results over 5 different\nrandom seeds for initialization. The hyperparameters used\nfor each model and task are provided in the Appendix A.\nTable 5 shows the development set results for MNLI, QQP,\nSQuAD 1.1, and SQuAD 2.0 and test set results for RACE.\nFor the test set results of RACE, we ﬁrst use the develop-\nment set to ﬁnd the checkpoint that gives us the median\nscore on the 5 random seeds and we report the results from\nthat checkpoint on the test set. We also report 5-way ensem-\nble results for the development set of SQuAD and test set\nof RACE. From Table 5 we observe that (a) as the model\nsize increases, the downstream task performance improves\nin all cases, (b) our 3.9B model establishes state of the art\nresults on the development set compared to other BERT\nbased models, and (c) our 3.9B model achieves both single\nmodel as well as ensembled SOTA results on RACE test set.\n6. Conclusion and Future Work\nIn this work, we successfully surpassed the limitations posed\nby traditional single-GPU-per-model training by implement-\ning model parallelism with only a few modiﬁcations to\nthe existing PyTorch transformer implementations. We ef-'
 'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nFigure 2. Transformer Architecture. Purple blocks correspond to\nfully connected layers. Each blue block represents a single trans-\nformer layer that is replicated N times.\nand compute efﬁciency. The original transformer formula-\ntion was designed as a machine translation architecture that\ntransforms an input sequence into another output sequence\nusing two parts, an Encoder andDecoder . However, recent\nwork leveraging transformers for language modeling such as\nBERT (Devlin et al., 2018) and GPT-2 (Radford et al., 2019)\nuse only the Encoder orDecoder depending on their needs.\nThis work explores both a decoder architecture, GPT-2, and\nan encoder architecture, BERT.\nFigure 2 shows a schematic diagram of the model we used.\nWe refer the reader to prior work for a detailed descrip-\ntion of the model architecture (Vaswani et al., 2017; Devlin\net al., 2018; Radford et al., 2019). It is worthwhile to men-\ntion that both GPT-2 and BERT use GeLU (Hendrycks &\nGimpel, 2016) nonlinearities and layer normalization (Ba\net al., 2016) to the input of the multi-head attention and feed\nforward layers, whereas the original transformer (Vaswani\net al., 2017) uses ReLU nonlinearities and applies layer\nnormalization to outputs.\n2.3. Data and Model Parallelism in Deep Learning\nThere are two central paradigms for scaling out deep neu-\nral network training to numerous hardware accelerators:\ndata parallelism (Valiant, 1990) where a training minibatch\nis split across multiple workers, and model parallelism in\nwhich the memory usage and computation of a model is\ndistributed across multiple workers. By increasing the mini-\nbatch size proportionally to the number of available work-\ners (i.e. weak scaling ), one observes near linear scaling\nin training data throughput. However, large batch train-\ning introduces complications into the optimization process\nthat can result in reduced accuracy or longer time to conver-\ngence, offsetting the beneﬁt of increased training throughput\n(Keskar et al., 2017). Further research (Goyal et al., 2017;\nYou et al., 2017; 2019) has developed techniques to miti-gate these effects and drive down the training time of large\nneural networks. To scale out training even further, parallel\nwork (Chen et al., 2016) has combined data parallelism with\nactivation checkpointing: recomputing activations in the\nbackward pass without storing them in the forward pass to\nreduce memory requirements.\nHowever, these techniques have one fundamental limitation\nin the problem size they can tackle: the model must ﬁt\nentirely on one worker. With language models of increasing\nsize and complexity like BERT and GPT-2, neural networks\nhave approached the memory capacity of modern hardware\naccelerators. One solution to this problem is to employ\nparameter sharing to reduce the memory footprint of the\nmodel (Lan et al., 2019), but this limits the overall capacity\nof the model. Our approach is to utilize model parallelism\nto split the model across multiple accelerators. This not\nonly alleviates the memory pressure, but also increases the\namount of parallelism independently of the microbatch size.\nWithin model parallelism, there are two further paradigms:\nlayer-wise pipeline parallelism, and more general distributed\ntensor computation. In pipeline model parallelism, groups\nof operations are performed on one device before the outputs\nare passed to the next device in the pipeline where a differ-\nent group of operations are performed. Some approaches\n(Harlap et al., 2018; Chen et al., 2018) use a parameter\nserver (Li et al., 2014) in conjunction with pipeline par-\nallelism. However these suffer from inconsistency issues.\nThe GPipe framework for TensorFlow (Huang et al., 2018)\novercomes this inconsistency issue by using synchronous\ngradient decent. This approach requires additional logic to\nhandle the efﬁcient pipelining of these communication and'
 'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\ninvestigation that will further test existing deep learning\nhardware and software. To realize this, improvements in\nthe efﬁciency and memory footprint of optimizers will be\nneeded. In addition, training a model with more than 16\nbillion parameters will demand more memory than is avail-\nable within 16 GPUs of a DGX-2H box. For such models, a\nhybrid intra-layer and inter-layer model parallelism along\nwith inter-node model parallelism would be more suitable.\nThree other directions of investigation include (a) pretrain-\ning different model families (XLNet, T5), (b) evaluating per-\nformance of large models across more difﬁcult and diverse\ndownstream tasks (e.g. Generative Question Answering,\nSummarization, and Conversation), and (c) using knowl-\nedge distillation to train small student models from these\nlarge pretrained teacher models.\nReferences\nAbadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z.,\nCitro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M.,\nGhemawat, S., Goodfellow, I., Harp, A., Irving, G., Is-\nard, M., Jia, Y ., Jozefowicz, R., Kaiser, L., Kudlur, M.,\nLevenberg, J., Man ´e, D., Monga, R., Moore, S., Mur-\nray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B.,\nSutskever, I., Talwar, K., Tucker, P., Vanhoucke, V ., Va-\nsudevan, V ., Vi ´egas, F., Vinyals, O., Warden, P., Watten-\nberg, M., Wicke, M., Yu, Y ., and Zheng, X. TensorFlow:\nLarge-scale machine learning on heterogeneous systems,\n2015. URL http://tensorflow .org/ . Software\navailable from tensorﬂow.org.\nBa, J. L., Kiros, J. R., and Hinton, G. E. Layernorm. CoRR ,\nabs/1607.06450, 2016. URL http://arxiv .org/\nabs/1607.06450 .\nChen, C.-C., Yang, C.-L., and Cheng, H.-Y . Efﬁcient and\nrobust parallel dnn training through model parallelism on\nmulti-gpu platform. arXiv:1809.02839 , 2018.\nChen, T., Xu, B., Zhang, C., and Guestrin, C. Train-\ning deep nets with sublinear memory cost. CoRR ,\nabs/1604.06174, 2016. URL http://arxiv .org/\nabs/1604.06174 .\nDai, Z., Yang, Z., Yang, Y ., Carbonell, J. G., Le, Q. V .,\nand Salakhutdinov, R. Transformer-xl: Attentive lan-\nguage models beyond a ﬁxed-length context. CoRR ,\nabs/1901.02860, 2019. URL http://arxiv .org/\nabs/1901.02860 .\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding, 2018.\nGoyal, P., Doll ´ar, P., Girshick, R. B., Noordhuis, P.,\nWesolowski, L., Kyrola, A., Tulloch, A., Jia, Y ., andHe, K. Accurate, large minibatch SGD: training imagenet\nin 1 hour. CoRR , abs/1706.02677, 2017.\nHarlap, A., Narayanan, D., Phanishayee, A., Se-\nshadri, V ., Devanur, N., Ganger, G., and Gibbons, P.\nPipedream: Fast and efﬁcient pipeline parallel dnn train-\ning.arXiv:1806.03377 , 2018.\nHendrycks, D. and Gimpel, K. Bridging nonlinearities\nand stochastic regularizers with gaussian error linear\nunits. CoRR , abs/1606.08415, 2016. URL http:\n//arxiv.org/abs/1606 .08415 .\nHoward, J. and Ruder, S. Fine-tuned language models for\ntext classiﬁcation. CoRR , abs/1801.06146, 2018.\nHuang, Y ., Cheng, Y ., Chen, D., Lee, H., Ngiam, J., Le,\nQ. V ., and Chen, Z. Gpipe: Efﬁcient training of gi-\nant neural networks using pipeline parallelism. CoRR ,\nabs/1811.06965, 2018. URL http://arxiv .org/\nabs/1811.06965 .\nJia, Z., Zaharia, M., and Aiken, A. Beyond data and model\nparallelism for deep neural networks. arXiv:1807.05358 ,\n2018.\nJoshi, M., Chen, D., Liu, Y ., Weld, D. S., Zettlemoyer,\nL., and Levy, O. Spanbert: Improving pre-training by\nrepresenting and predicting spans. arXiv:1907.10529 ,\n2019.\nKeskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy,\nM., and Tang, P. T. P. On large- batch training for deep\nlearning: Generalization gap and sharp minima. ICLR ,\n2017.\nKhandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and\nLewis, M. Generalization through memorization: Nearest\nneighbor language models. arXiv:1911.00172 , 2019.']"," It rearranges the order of layer normalization and residual connections, allowing for stable training and enhanced accuracies as model size increases.",0.5833333333041666,0.6666666666666666,0.9646333283613009,0.0,0.43368965420897776,0.8116816937589879
What specific properties of Task2Vec embeddings allow for effective reasoning about task space?,"Task2Vec embeddings possess several key properties that facilitate effective reasoning about task space. The norm of the embeddings correlates with the complexity of the tasks, while the cosine distance between embeddings reflects semantic similarities and natural distances between tasks, such as taxonomic relationships in classification. Additionally, the embeddings can predict task similarities accurately, which aids in selecting appropriate feature extractors for new tasks.","['lem of meta-learning has a long history with much re-\ncent work dedicated to problems such as neural architecture\nsearch and hyper-parameter estimation. Closely related to\nour problem is work on selecting from a library of classi-\nﬁers to solve a new task [33, 2, 20]. Unlike our approach,\nthese usually address the question via land-marking or ac-\ntive testing, in which a few different models are evaluated\nand performance of the remainder estimated by extension.\nThis can be viewed as a problem of completing a matrix\ndeﬁned by performance of each model on each task.\nA similar approach has been taken in computer vision for\nselecting a detector for a new category out of a large library\nof detectors [26, 40, 38].\n7. Discussion\nTASK 2VEC is an efﬁcient way to represent a task, or the\ncorresponding dataset, as a ﬁxed dimensional vector. It has\nseveral appealing properties, in particular its norm corre-\nlates with the test error obtained on the task, and the co-\nsine distance between embeddings correlates with natural\ndistances between tasks, when available, such as the taxo-\nnomic distance for species classiﬁcation, and the ﬁne-tuning\ndistance for transfer learning. Having a representation of\ntasks paves the way for a wide variety of meta-learning\ntasks. In this work, we focused on selection of an expert\nfeature extractor in order to solve a new task, especially\nwhen little training data is present, and showed that using\nTASK 2VEC to select an expert from a collection can sen-\nsibly improve test performance while adding only a small\noverhead to the training process.'
 'TASK2VEC: Task Embedding for Meta-Learning\nAlessandro Achille\nUCLA and AWS\nachille@cs.ucla.eduMichael Lam\nAWS\nmichlam@amazon.comRahul Tewari\nAWS\ntewarir@amazon.comAvinash Ravichandran\nAWS\nravinash@amazon.com\nSubhransu Maji\nUMass and AWS\nsmmaji@amazon.comCharless Fowlkes\nUCI and AWS\nfowlkec@amazon.comStefano Soatto\nUCLA and AWS\nsoattos@amazon.comPietro Perona\nCaltech and AWS\nperonapp@amazon.com\nAbstract\nWe introduce a method to provide vectorial represen-\ntations of visual classiﬁcation tasks which can be used\nto reason about the nature of those tasks and their re-\nlations. Given a dataset with ground-truth labels and a\nloss function deﬁned over those labels, we process images\nthrough a “probe network” and compute an embedding\nbased on estimates of the Fisher information matrix asso-\nciated with the probe network parameters. This provides a\nﬁxed-dimensional embedding of the task that is independent\nof details such as the number of classes and does not require\nany understanding of the class label semantics. We demon-\nstrate that this embedding is capable of predicting task sim-\nilarities that match our intuition about semantic and tax-\nonomic relations between different visual tasks ( e.g., tasks\nbased on classifying different types of plants are similar).\nWe also demonstrate the practical value of this framework\nfor the meta-task of selecting a pre-trained feature extractor\nfor a new task. We present a simple meta-learning frame-\nwork for learning a metric on embeddings that is capable of\npredicting which feature extractors will perform well. Se-\nlecting a feature extractor with task embedding obtains a\nperformance close to the best available feature extractor,\nwhile costing substantially less than exhaustively training\nand evaluating on all available feature extractors.\n1. Introduction\nThe success of Deep Learning hinges in part on the fact\nthat models learned for one task can be used on other related\ntasks. Yet, no general framework exists to describe and\nlearn relations between tasks. We introduce the TASK 2VEC\nembedding, a technique to represent tasks as elements of a\nvector space based on the Fisher Information Matrix. The\nnorm of the embedding correlates with the complexity of\nthe task, while the distance between embeddings capturessemantic similarities between tasks (Fig. 1). When other\nnatural distances are available, such as the taxonomical dis-\ntance in biological classiﬁcation, we ﬁnd that the embed-\nding distance correlates positively with it (Fig. 2). More-\nover, we introduce an asymmetric distance on tasks which\ncorrelates with the transferability between tasks.\nComputation of the embedding leverages a duality be-\ntween network parameters (weights) and outputs (activa-\ntions) in a deep neural network (DNN): Just as the activa-\ntions of a DNN trained on a complex visual recognition task\nare a rich representation of the input images, we show that\nthe gradients of the weights relative to a task-speciﬁc loss\nare a rich representation of the task itself. Speciﬁcally, given\na task deﬁned by a dataset D={(xi,yi)}N\ni=1of labeled\nsamples, we feed the data through a pre-trained reference\nconvolutional neural network which we call “ probe net-\nwork ”, and compute the diagonal Fisher Information Ma-\ntrix (FIM) of the network ﬁlter parameters to capture the\nstructure of the task (Sect. 2). Since the architecture and\nweights of the probe network are ﬁxed, the FIM provides a\nﬁxed-dimensional representation of the task. We show this\nembedding encodes the “difﬁculty” of the task, character-\nistics of the input domain, and which features of the probe\nnetwork are useful to solve it (Sect. 2.1).\nOur task embedding can be used to reason about the\nspace of tasks and solve meta-tasks. As a motivating exam-\nple, we study the problem of selecting the best pre-trained\nfeature extractor to solve a new task. This can be particu-\nlarly valuable when there is insufﬁcient data to train or ﬁne-'
 'Task embedding for iMaterialist In Fig. 1 we show a\nt-SNE visualization of the embedding for iMaterialist and\niNaturalist tasks. Task embedding yields interpretable re-\nsults: Tasks that are correlated in the dataset, such as binary\nclasses corresponding to the same categorical attribute, may\nend up far away from each other and close to other tasks that\nare semantically more similar ( e.g., the jeans category task\nis close to the ripped attribute and the denim material). This\nis reﬂected in the mixture of colors of semantically related\nnearby tasks, showing non-trivial grouping.\nWe also compare the TASK 2VEC embedding with a do-\nmain embedding baseline, which only exploits the input\ndistribution p(x)rather than the task distribution p(x,y).\nWhile some tasks are highly correlated with their domain\n(e.g., tasks from iNaturalist), other tasks differ only on the\nlabels ( e.g., all the attribute tasks of iMaterialist, which\nshare the same clothes domain). Accordingly, the domain'
 'relatively few samples. In Fig. 4 we show how the perfor-\nmance of TASK 2VEC varies on a model selection task as the\nnumber of samples varies. At all sample sizes TASK 2VEC is\nclose to the optimum, and improves over selecting a generic\nexpert (ImageNet), both when ﬁne-tuning and when train-\ning only a classiﬁer. We observe that the best choice of ex-\nperts is not affected by the dataset size, and that even with\nfew examples TASK 2VEC is able to ﬁnd the optimal experts.\nChoice of probe network In Table 1 we show that\nDenseNet [15] and ResNet architectures [11] perform sig-\nniﬁcantly better when used as probe networks to compute\ntheTASK 2VEC embedding than a VGG [32] architecture.\n6. Related Work\nTask and Domain embedding. Tasks distinguished by\ntheir domain can be understood simply in terms of image\nstatistics. Due to the bias of different datasets, sometimes a\nbenchmark task may be identiﬁed just by looking at a few\nimages [34]. The question of determining what summary']","Task2Vec embeddings are invariant to label space, encode task difficulty, represent task-weighted domain characteristics, and highlight features important for the task.",0.9999999999666667,1.0,0.9871328509334968,1.0,0.5307906126523423,0.923162450609369
What distinctive strategy does GLM-130B employ to ensure training stability for a 130-billion-parameter model?,"GLM-130B employs several strategies to ensure training stability, including the use of a bidirectional-attention architecture and improved Post-LN techniques such as DeepNorm. It also categorizes training stability issues systematically and uses techniques like shrinking the embedding layer’s gradient to address numerical instability. Additionally, the model's design is platform-aware to maximize hardware potential.","['Published as a conference paper at ICLR 2023\nF A B RIEF HISTORY OF GLM-130B\nThe GLM-130B project16was conceived in Dec. 2021 in a brainstorming meeting at Tsinghua KEG.\nWe firmly believe that it is of value to pre-train a highly accurate language model, in particular for\nboth Chinese and English. Though GPT-3 (Brown et al., 2020) is the pioneer for this effort, it is not\navailable to most people in the world. In addition, it supports English only. We therefore decide to\ninitialize the project GLM-130B. Please note that the WuDao 1.75T model we built last year is a\nsparse model with 480 mixture-of-experts (MoE), rather than a dense one as GPT-3. Our goal then\nis to train a bilingual pre-trained dense model with high accuracy on downstream tasks, and to make\nit open to everyone in the world-anyone, anywhere can download it and use it on a single server with\nappropriate GPUs.\nThe ambitious project soon faced several important challenges:\n•Lack of computational resources : No organization is willing to sponsor such a big project and\nfreely make it public.\n•Lack of a robust pre-training algorithm : Despite GPT-3’s success on English corpus, it is\nunclear how to train a high-accurate bilingual model for both English and Chinese.\n•Lack of fast inference solutions : Since the goal is to have the model public to everyone, we need\nto design fast inference solutions with low resource requirements to run the model.\nFor the pre-training algorithm, we finally chose GLM (Du et al., 2022) due to its high performance\nin practice. We eventually decided to train a GLM model of 130 billion parameters after several\nrounds of discussions and exploration, because such a size makes it possible to run the inference on\na single A100 (40G * 8) server.\nOur first attempt at training the model was in January 2022, shortly after we received a small sponsor\nof GPUs for test running. However, we soon realized that we had significantly underestimated the\ntechnical difficulties of pre-training a model at such a scale (>100B). It seems that pre-training a\nhighly accurate 100B-scale model is quite different from training a 10B-scale one. Due to frequent\nrandom hardware failures, model gradients exploding, unexpected excessive memory usage in the\nalgorithm, debug for the 3D pipeline in the new Megatron and DeepSpeed frameworks, inability to\nrecover from optimizer states, blocked TCP responses between processes, and many many unex-\npected “bugs”, the project was delayed for many times. The Tsinghua PACMAN team gave us a\nhand at this difficult time and together we successfully fixed most of the “bugs”.\nBy March, we were still short on computational resources, but fortunately got a chance to try test\nruns on several other platforms, including Ascend 910, Hygon DCU, NVIDIA, and Sunway. The\nimmediate challenge was for us to adapt our training code to these different platforms, as the under-\nlying operators are quite different. Also, it introduced many new issues: the element-wise operators\nnot supporting fast computation for large-dimension vectors, various issues that hindered conver-\ngence—the large gradient norms of input embeddings, native Post-LN, Pre-LN, and Sandwich-LN,\ndataloader state seeds, and computation precision choices in Softmax and Attention — as well as\nnumerous mistakes we ourselves made. With tremendous help from all of our generous partners, we\nfinally succeeded in making our pre-training algorithms runnable across all the platforms—frankly,\na surprising achievement for this project. The timeline of GLM-130B in Figure 21 covers most of\nthe issues we have encountered and addressed as of this writing.\nOn April 26th, we received a generous computing sponsorship from Zhipu.AI — an AI startup that\naims to teach machines to think like humans. After another week of testing, we finally kicked off\nthe training of the GLM-130B model on its 96 A100 (40G * 8) servers on May 6th. Additionally,'
 'ence. Altogether, it contributes to the high quality of GLM-130B in terms of both language perfor-\nmance on 112 tasks and ethical results on bias and toxicity benchmarks. Our experiences of both\nsuccess and failure are condensed into the lessons for training 100B-scale LLMs, attached in the\nAppendix B.10.\n9'
 'Published as a conference paper at ICLR 2023\nexact dataset setting in T0 (Sanh et al., 2022) and the information extraction datasets in GLM-130B\nto allow the correct evaluation on some types of tasks (e.g., NLI).\nFigure 14 shows the ablation results. On the 8 datasets we test, we find that the GLM objective\nis a major contributor to the improvement (from GLM (uni) to GLM + MIP (bi)). For exam-\nple, it accounts for 73% improvement in LAMBADA and 90% improvement in MMLU, which\nare very widely adopted challenging benchmarks for LLMs. As for MIP, on some datasets (e.g.,\nWiC, ReCoRD, Hellaswag), MIP may even harm the performance. While for datasets related to text\nsimilarity and coreference (e.g., WSC, BoolQ, ANLI R1), MIP is the main contributor. It is likely\nbecause the text similarity and coreference challenges, which people usually construct intentionally\nto test language models’ ability, are seldom seen in the self-supervised corpus that makes up peo-\nple’s daily written texts. Thus, MIP training mainly helps to bridge the gap between self-supervised\npre-training and these tasks.\nB.10 L ESSONS LEARNED\nLesson 1 (Bidirectional Architecture). The bidirectional-attention GLM is a strong architec-\nture alternative, in addition to GPTs.\nLesson 2 (Platform-aware Configuration). Configure LLMs based on the cluster and parallel\nstrategy used to squeeze hardware potential.\nLesson 3 (Improved Post-LN). Counter-stereotypically, DeepNorm, a type of Post-LN, is the\noption to stabilize GLM-130B.\nLesson 4 (Training Stability Categorization). Unexpected training instability that LLMs\nsuffer from arouses systematically and numerically.\nLesson 5 (Systematical Instability: FP16). Though FP16 induces more instability, it enables\ntraining and inference on diverse platforms.\nLesson 6 (Numerical Instability: Embedding Gradient Shrink). Shrinking embedding\nlayer’s gradient to its 0.1 can solve most numerical instability problems.\nLesson 7 (GLM’s INT4 Quantization Scaling Law). GLM has a unique INT4 weight quan-\ntization scaling law unobserved in GPT-style BLOOM.\nLesson 8 (Future Direction). To create powerful LLMs, the main focus can be on 1) more and\nbetter data, 2) better architectures and pre-training objectives, and 3) more sufficient training.\n30'
 'Published as a conference paper at ICLR 2023\nEPRSTMT OCNLI-FC BUSTM CHID-FC CLUEWSC-FC C3 WSC1.1 CMNLI DRCD OCNLI_50K AFQMC CMRC2018020406080Acc. or EM92.5\n73.877.590.1\n77.4 77.583.977.0 77.1 74.771.2\n55.788.8\n53.864.487.1\n53.5 54.981.1\n51.7\n29.544.669.0\n16.6GLM-130B ERNIE 3.0 Titan-260B\nFigure 8: GLM-130B and ERNIE Titan 3.0 260B evaluated on zero-shot CLUE and FewCLUE.\n7 CLUE and 5 FewCLUE datasets (Cf. Appendix C.7 for details). We compare GLM-130B to the\nlargest existing Chinese monolingual language model—the 260B ERNIE Titan 3.0 (Wang et al.,\n2021). We follow its setting to report zero-shot results on dev datasets. GLM-130B consistently\noutperforms ERNIE Titan 3.0 across 12 tasks (Cf. Figure 8). Interestingly, GLM-130B performs at\nleast 260% better than ERNIE on two abstractive MRC datasets (DRCD and CMRC2018), possibly\ndue to GLM-130B’s pre-training objective that naturally resonates to abstractive MRC’s form.\n6 R ELATED WORK\nIn this section, we review related work to GLM-130B on topics of pre-training, transferring, and\ninference of pre-trained LLMs (Qiu et al., 2020; Bommasani et al., 2021).\nPre-Training. Vanilla language modeling refers to decoder-only autoregressive models (e.g.,\nGPT (Radford et al., 2018)), but it also recognizes any forms of self-supervised objectives on texts.\nRecently, transformer-based (Vaswani et al., 2017) language models present a fascinating scaling\nlaw: new abilities (Wei et al., 2022b) arise as models scale up, from 1.5B (Radford et al., 2019),\n10B-scale language models (Raffel et al., 2020; Shoeybi et al., 2019; Black et al., 2022), to 100B-\nscale GPT-3 (Brown et al., 2020). Later, despite many 100B-scale LLMs (Lieber et al., 2021; Thop-\npilan et al., 2022; Rae et al., 2021; Smith et al., 2022; Chowdhery et al., 2022; Wu et al., 2021; Zeng\net al., 2021; Wang et al., 2021) in both English and Chinese, they are not available to public or only\naccessible via limited APIs. The closeness of LLMs severely stymies its development. GLM-130B’s\nefforts, along with recent ElutherAI, OPT-175B (Zhang et al., 2022), and BLOOM-176B (Scao et al.,\n2022), aim to offer high-quality open-sourced LLMs to our community.\nTransferring. Though fine-tuning has been a de facto way for transfer learning, the evaluation for\nLLMs has been focused on prompting and in-context learning due to their tremendous sizes (Brown\net al., 2020; Liu et al., 2021a). Nevertheless, some recent attempts has been on parameter-efficient\nlearning on language models (Houlsby et al., 2019) and prompt tuning (i.e., P-tuning, Li & Liang\n(2021); Liu et al. (2021b); Lester et al. (2021); Liu et al. (2022)). For now we do not focus on them\nand will leave the comprehensive testing of them on GLM-130B in future study.\nInference. Most public-accessible LLMs nowadays are providing their services via limited APIs.In\nthis work, an important part of our endeavor has been on LLMs’ efficient and fast inference. Related\nwork may include distillation (Sanh et al., 2019; Jiao et al., 2020; Wang et al., 2020), quantiza-\ntion (Zafrir et al., 2019; Shen et al., 2020; Tao et al., 2022), and pruning (Michel et al., 2019; Fan\net al., 2019). Very recent work (Dettmers et al., 2022) shows that LLMs such as OPT-175B and\nBLOOM-176B can be quantized to 8 bit due to special distribution of outlier dimensions. In this\nwork, we demonstrate GLM’s scaling law for INT4 weight quantization, which allows GLM-130B\nto inference on as few as 4 ×RTX 3090 (24G) GPUs or 8 ×RTX 2080 Ti (11G) GPUs.\n7 C ONCLUSION AND LESSONS\nWe introduce GLM-130B, a bilingual pre-trained language model that aims to facilitate open and\ninclusive LLM research. GLM-130B’s technical and engineering undertakings generate insight into\nLLMs’ architectures, pre-training objectives, training stability and efficiency, and affordable infer-\nence. Altogether, it contributes to the high quality of GLM-130B in terms of both language perfor-']","GLM-130B utilizes embedding gradient shrink (EGS) and DeepNorm layer normalization to ensure training stability, significantly reducing the likelihood of loss spikes and divergence.",0.8055555555287036,1.0,0.9583690118351135,0.5,0.44631465957947497,0.9281157811750428
What parallel strategies and configurations are utilized to train GLM-130B efficiently on a GPU cluster?,"To train GLM-130B efficiently, a combination of data parallelism, tensor model parallelism, and pipeline model parallelism is utilized, forming a 3D parallel strategy. The configurations include 4-way tensor parallelism and 8-way pipeline parallelism, along with a big global batch size of 4,224. The model is designed to run on a single DGX-A100 (40G) node in FP16 precision, ensuring optimal GPU utilization while balancing memory requirements.","['suggests that most existing LLMs are largely under-trained.\nThe 3D Parallel Strategy. The data parallelism (Valiant, 1990) and tensor model paral-\nlelism (Shoeybi et al., 2019) are the de facto practices for training billion-scale models (Wang &\nKomatsuzaki, 2021; Du et al., 2022). To further handle the huge GPU memory requirement and the\ndecrease in overall GPU utilization resulted from applying tensor parallel between nodes—as 40G\nrather than 80G A100s are used for training GLM-130B, we combine the pipeline model parallelism\nwith the other two strategies to form a 3D parallel strategy.\nThe pipeline parallelism divides the model into sequential stages for each parallel group, and to fur-\nther minimize bubbles introduced by pipeline, we leverage the PipeDream-Flush (Narayanan et al.,\n2021) implementation from DeepSpeed (Rasley et al., 2020) to train GLM-130B with a relative\n4'
 'close the code, details, and the whole process of GLM-130B’s pre-training. Our endeavor to allow\nGLM-130B inference on few popularized GPUs such as 3090/2080 Ti also aligns with the repro-\nducibility undertaking, as it allows most academic researchers to reproduce GLM-130B’s results on\ntheir offline machines. We also provide free APIs for individual users to test GLM-130B’s ability.\n10'
 'Published as a conference paper at ICLR 2023\nbig global batch size (4,224) to reduce time and GPU memory wasting. Through both numeri-\ncal and empirical examinations, we adopt 4-way tensor parallelism and 8-way pipeline parallelism\n(Cf. Appendix B.4 for details). Following the calculation in (Chowdhery et al., 2022), we report\nhardware FLOPs utilization (HFU) of 43.3% and model FLOPs utilization (MFU) of 32.5% due to\nre-materialization.\nGLM-130B Configurations. We aim to enable our 100B-scale LLM to run a single DGX-A100\n(40G) node in FP16 precision. Based on the hidden state dimension of 12,288 we adopt from\nGPT-3, the resultant model size has to be no more than 130B parameters, thus GLM-130B. To\nmaximize GPU utilization, we configure the model based on the platform and its corresponding\nparallel strategy. To avoid insufficient memory utilization in the middle stages due to the additional\nword embedding at both ends, we balance the pipeline partition by removing one layer from them,\nmaking 9 ×8-2=70 transformer layers in GLM-130B.\nDuring the 60-day access to the cluster, we manage to train GLM-130B for 400 billion tokens\n(roughly 200 billion each for Chinese and English) with a fixed sequence length of 2,048 per sample.\nFor the [gMASK] training objective, we use a context window of 2,048 tokens. For the [MASK]\nand multi-task objectives, we use a context window of 512 and concatenate four samples together to\ncater the 2,048-sequence-length. We warm-up the batch size from 192 to 4224 over the first 2.5%\nsamples. We use AdamW (Loshchilov & Hutter, 2019) as our optimizer with β1andβ2set to 0.9\nand 0.95, and a weight decay value of 0.1. We warm up the learning rate from 10−7to8×10−5\nover the first 0.5% samples, then decay it by a 10×cosine schedule. We use a dropout rate of 0.1\nand clip gradients using a clipping value of 1.0 (Cf. Table 11 for the full configurations).\n3 T HETRAINING STABILITY OF GLM-130B\nThe training stability is the decisive factor in GLM-130B’s quality, which is also largely impacted\nby the number of tokens it passes through (Hoffmann et al., 2022). Thus, given the computing\nusage constraint, there has to be a trade-off between efficiency and stability with regard to floating-\npoint (FP) formats: low-precision FP formats (e.g., 16-bit precision—FP16) improve computing\nefficiency but are prone to overflow and underflow errors, resulting in training collapses.\n(a) Gradient norm with EGS α = 0.1\n(b) EGS in 40B-scale testing\nFigure 4: EGS reduces gradi-\nent scale and variance to stabi-\nlize LLMs’ pre-training.Mixed-Precision. We follow the common practice of a mixed-\nprecision (Micikevicius et al., 2018) strategy (Apex O2), i.e., FP16\nfor forwards and backwards and FP32 for optimizer states and mas-\nter weights, to reduce the GPU memory usage and improve train-\ning efficiency. Similar to OPT-175B and BLOOM-176B (C.f. Fig-\nure 10 in Appendix), the training of GLM-130B faces frequent loss\nspikes resulted from this choice, which tends to become increas-\ningly frequent as the training goes on. The precision related spikes\nare often without clear reasons: some recover on their own; others\ncome with a portent of suddenly soaring gradient norm and even-\ntually a spike or even NaN in loss. OPT-175B attempted to fix by\nmanually skipping data and adjusting hyper-parameters; BLOOM-\n176B did so via the embedding norm technique (Dettmers et al.,\n2021). We spent months to empirically investigate the spikes and\nrealize that a few issues emerge when transformers scale up:\nFirst, the transformer main branch’s value scale can be extremely\nlarge in deeper layers if using Pre-LN. This is addressed in GLM-\n130B by using DeepNorm based Post-LN (Cf. Section 2.1), which\nmakes the value scale always bounded.\nSecond, the attention scores grow so large that they exceed FP16’s\nrange, as the model scales up. There are a few options to overcome this issue in LLMs. In'
 'Published as a conference paper at ICLR 2023\nGPU 0F0B0GPU 1GPU 2GPU 3F0F0F0B0U0B0B0U0U0U0F1B1F1F1F1B1B1B1U1U1U1U1ForwardBackwardOptimizer Step\nTimeBubble time\n(a) Naive pipeline implementation, which can be extremely inefficient.\nGPU 0F0GPU 1GPU 2GPU 3ForwardBackwardOptimizer Step\nTimeF1F2F3F4F5F6F7F0F1F2F3F4F5F6F7F0F1F2F3F4F5F6F7F1F2F3F4F5F6F7B7B0B1B2B3B4B5B6U0U0U0U0B7B0B1B2B3B4B5B6B7B0B1B2B3B4B5B6B7B0B1B2B3B4B5B6F0\n(b) GPipe (Huang et al., 2019) implementation.\nGPU 0F0GPU 1GPU 2GPU 3B0ForwardBackwardOptimizer Step\nTimeF1F2F3F0F1F2F3F0F1F2F3F0U0U0U0U0B0B2B1F1B2F2B3F3B4F4B5F5B6F6B7F7B1B3F4B4F5B5F6B6F7B7B0B1B2F4B3F5B4F6B5F7B7B6B0B1F4B2F5B3F6B4F7B7B6B6\n(c) Pipedream (Narayanan et al., 2021) implementation (used in GLM-130B).\nFigure 11: Different pipeline strategies and their conceptual comparison.\nof pipeline bubble time is reduced to a negligible level due to the forward recomputation technique in\nbackpropagation that allows some overlap in computational communication, thus showing that the\nbubbles introduced in parallel by the pipeline model do not seriously deplete the training efficiency.\nIn general, in order to make full use of the hardware, it is common to place models into model\nparallel groups consisting of multiple nodes and try to use the full memory of each node. In this\ncase, we can freely adjust the ratio of pipeline model parallelism and tensor model parallelism. Since\ndata parallelism hardly affects the computation time, we assume that the scale of data parallelism is\nd= 1, the total number of nodes is n, the scale of tensor model parallelism is t, and the scale of\npipeline model parallelism is p, and satisfies n=t×p, the bubble share in this case is\nbubble-ratio =n/t−1\nm+n/t−1(6)\nFrom the above equation, we can see that increasing the size of tensor parallelism will further reduce\nthe bubble ratio. However, the tensor parallelism scale cannot be increased indefinitely, which would\nlead to a reduction in computational granularity and greatly increase the communication cost across\na certain threshold. Therefore, we can conclude that the size of tensor model parallelism should\nincrease slowly as the model size increases, but not more than the number of graphics cards in\na single machine. In the training of GLM-130B, the experiments show that the optimal tensor\nparallelism scale is t= 4and does not scale up to the scale of t= 8in the DGX-A100 system. The\nother parameters are m= 176 , p= 8, and the bubble share is calculated to be only 3.8%, which is\nsufficient to demonstrate the efficiency of pipeline model parallelism.\n26']","GLM-130B employs a 3D parallel strategy combining data, tensor, and pipeline model parallelism, optimized for 96 DGX-A100 GPU nodes, maximizing token throughput and hardware utilization.",0.8055555555287036,1.0,0.9103588769176812,0.6666666666666666,0.4693669141156339,0.9543907333856124
How does Megatron-LM's model parallel approach optimize memory and computation distribution across GPUs?,"Megatron-LM optimizes memory and computation distribution across GPUs by implementing model parallelism in both the multi-layer perceptron (MLP) and self-attention blocks of transformer layers. It partitions the weight matrices and input data across GPUs, allowing for independent computations that minimize synchronization points, thereby increasing efficiency. This approach requires only a few all-reduce operations during the forward and backward passes, facilitating better scaling and resource utilization.","['Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nrequire any new compiler or code re-writing, and can be\nfully implemented by inserting a few simple primitives, as\ndescribed in the next section.\n3. Model Parallel Transformers\nWe take advantage of the structure of transformer networks\nto create a simple model parallel implementation by adding a\nfew synchronization primitives. A transformer layer consists\nof a self attention block followed by a two-layer, multi-layer\nperceptron (MLP) as shown in Figure 2. We introduce\nmodel parallelism in both of these blocks separately.\nWe start by detailing the MLP block. The ﬁrst part of the\nblock is a GEMM followed by a GeLU nonlinearity:\nY=GeLU (XA) (1)\nOne option to parallelize the GEMM is to split the weight\nmatrixAalong its rows and input Xalong its columns as:\nX= [X1,X2], A=[A1\nA2]\n. (2)\nThis partitioning will result in Y=GeLU (X1A1+\nX2A2). Since GeLU is a nonlinear function, GeLU (X1A1+\nX2A2)̸=GeLU (X1A1)+GeLU (X2A2)and this approach\nwill require a synchronization point before the GeLU func-\ntion.\nAnother option is to split Aalong its columns A= [A1,A2].\nThis partitioning allows the GeLU nonlinearity to be inde-\npendently applied to the output of each partitioned GEMM:\n[Y1,Y2] = [ GeLU (XA 1),GeLU (XA 2)] (3)\nThis is advantageous as it removes a synchronization point.\nHence, we partition the ﬁrst GEMM in this column parallel\nfashion and split the second GEMM along its rows so it takes\nthe output of the GeLU layer directly without requiring any\ncommunication as shown in Figure 3a. The output of the\nsecond GEMM is then reduced across the GPUs before\npassing the output to the dropout layer. This approach splits\nboth GEMMs in the MLP block across GPUs and requires\nonly a single all-reduce operation in the forward pass ( g\noperator) and a single all-reduce in the backward pass ( f\noperator). These two operators are conjugates of each other\nand can be implemented in PyTorch with only a few lines of\ncode. As an example, the implementation of the foperator\nis provided below:\nclass f(torch.autograd.Function):\ndef forward(ctx, x):\nreturn x\ndef backward(ctx, gradient):\nall_reduce(gradient)\nreturn gradient\nCode 1. Implementation of foperator. gis similar to fwith\nidentity in the backward and all-reduce in the forward\nfunctions.\n(a) MLP\n(b) Self-Attention\nFigure 3. Blocks of Transformer with Model Parallelism. fandg\nare conjugate. fis an identity operator in the forward pass and all\nreduce in the backward pass while gis an all reduce in the forward\npass and identity in the backward pass.\nAs shown in Figure 3b, for the self attention block we exploit\ninherent parallelism in the multihead attention operation,\npartitioning the GEMMs associated with key ( K), query\n(Q), and value ( V) in a column parallel fashion such that\nthe matrix multiply corresponding to each attention head is\ndone locally on one GPU. This allows us to split per atten-\ntion head parameters and workload across the GPUs, and\ndoesnt require any immediate communication to complete\nthe self-attention. The subsequent GEMM from the output\nlinear layer (after self attention) is parallelized along its\nrows and takes the output of the parallel attention layer di-\nrectly, without requiring communication between the GPUs.\nThis approach for both the MLP and self attention layer\nfuses groups of two GEMMs, eliminates a synchronization\npoint in between, and results in better scaling. This enables\nus to perform all GEMMs in a simple transformer layer\nusing only two all-reduces in the forward path and two in\nthe backward path (see Figure 4).\nThe transformer language model has an output embedding\nwith the dimension of hidden-size ( H) times vocabulary-\nsize (v). Since the vocabulary size is on the order of tens\nof thousands of tokens for modern language models (for\nexample, GPT-2 used a vocabulary size of 50,257), it is ben-\neﬁcial to parallelize the output embedding GEMM. How-'
 'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nFigure 1. Model (blue) and model+data (green) parallel FLOPS\nas a function of number of GPUs. Model parallel (blue): up to\n8-way model parallel weak scaling with approximately 1 billion\nparameters per GPU (e.g. 2 billion for 2 GPUs and 4 billion for\n4 GPUs). Model+data parallel (green): similar conﬁguration as\nmodel parallel combined with 64-way data parallel.\na baseline by training a model of 1.2 billion parameters\non a single NVIDIA V100 32GB GPU, that sustains 39\nTeraFLOPs. This is 30% of the theoretical peak FLOPS\nfor a single GPU as conﬁgured in a DGX-2H server, and\nis thus a strong baseline. Scaling the model to 8.3 billion\nparameters on 512 GPUs with 8-way model parallelism,\nwe achieve up to 15.1 PetaFLOPs per second sustained\nover the entire application. This is 76% scaling efﬁciency\ncompared to the single GPU case. Figure 1 shows more\ndetailed scaling results.\nTo analyze the effect of model size scaling on accuracy,\nwe train both left-to-right GPT-2 (Radford et al., 2019) lan-\nguage models as well as BERT (Devlin et al., 2018) bidi-\nrectional transformers and evaluate them on several down-\nstream tasks. We show that the existing BERT architecture\nresults in model degradation as the size increases. We over-\ncome this challenge by rearranging the layer normalization\nand residual connection in the transformer layers and show\nthat with this change, results for the downstream tasks on\ndevelopment sets improve monotonically as the model size\nincreases. In addition, we show that our models achieve\ntest set state of the art (SOTA) results on WikiText103,\ncloze-style prediction accuracy on LAMBADA, and reading\ncomprehension RACE datasets.\nIn summary, our contributions are as follows:\n•We implement a simple and efﬁcient model parallel\napproach by making only a few targeted modiﬁcations\nto an existing PyTorch transformer implementation.\n•We perform an in-depth empirical analysis of our\nmodel and data parallel technique and demonstrate\nup to 76% scaling efﬁciency using 512 GPUs.•We show that careful attention to the placement of\nlayer normalization in BERT-like models is critical to\nachieving increased accuracies as the model grows.\n•We demonstrate that scaling the model size results in\nimproved accuracies for both GPT-2 (studied up to\n8.3 billion parameters) and BERT (studied up to 3.9B\nparameters) models.\n•We showcase that our models achieve state of the art\nresults on test sets: perplexity on WikiText103 (10.8\nppl), accuracy on LAMBADA (66.5%), and accuracy\non RACE (90.9%).\n•We open source our code along with the training\nand evaluation pipelines at https://github .com/\nNVIDIA/Megatron-LM\n2. Background and Challenges\n2.1. Neural Language Model Pretraining\nPretrained language models have become an indispensable\npart of NLP researchers’ toolkits. Leveraging large corpus\npretraining to learn robust neural representations of lan-\nguage is an active area of research that has spanned the\npast decade. Early examples of pretraining and transferring\nneural representations of language demonstrated that pre-\ntrained word embedding tables improve downstream task\nresults compared to word embedding tables learned from\nscratch (Mikolov et al., 2013; Pennington et al., 2014; Turian\net al., 2010). Later work advanced research in this area by\nlearning and transferring neural models that capture contex-\ntual representations of words (Melamud et al., 2016; Mc-\nCann et al., 2017; Peters et al., 2018; Radford et al., 2017;\n2019). Recent parallel work (Ramachandran et al., 2016;\nHoward & Ruder, 2018; Radford et al., 2018; Devlin et al.,\n2018; Liu et al., 2019b; Dai et al., 2019; Yang et al., 2019;\nLiu et al., 2019a; Lan et al., 2019) further builds upon these\nideas by not just transferring the language model to extract\ncontextual word representations, but by also ﬁnetuning the\nlanguage model in an end to end fashion on downstream'
 'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\ninvestigation that will further test existing deep learning\nhardware and software. To realize this, improvements in\nthe efﬁciency and memory footprint of optimizers will be\nneeded. In addition, training a model with more than 16\nbillion parameters will demand more memory than is avail-\nable within 16 GPUs of a DGX-2H box. For such models, a\nhybrid intra-layer and inter-layer model parallelism along\nwith inter-node model parallelism would be more suitable.\nThree other directions of investigation include (a) pretrain-\ning different model families (XLNet, T5), (b) evaluating per-\nformance of large models across more difﬁcult and diverse\ndownstream tasks (e.g. Generative Question Answering,\nSummarization, and Conversation), and (c) using knowl-\nedge distillation to train small student models from these\nlarge pretrained teacher models.\nReferences\nAbadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z.,\nCitro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M.,\nGhemawat, S., Goodfellow, I., Harp, A., Irving, G., Is-\nard, M., Jia, Y ., Jozefowicz, R., Kaiser, L., Kudlur, M.,\nLevenberg, J., Man ´e, D., Monga, R., Moore, S., Mur-\nray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B.,\nSutskever, I., Talwar, K., Tucker, P., Vanhoucke, V ., Va-\nsudevan, V ., Vi ´egas, F., Vinyals, O., Warden, P., Watten-\nberg, M., Wicke, M., Yu, Y ., and Zheng, X. TensorFlow:\nLarge-scale machine learning on heterogeneous systems,\n2015. URL http://tensorflow .org/ . Software\navailable from tensorﬂow.org.\nBa, J. L., Kiros, J. R., and Hinton, G. E. Layernorm. CoRR ,\nabs/1607.06450, 2016. URL http://arxiv .org/\nabs/1607.06450 .\nChen, C.-C., Yang, C.-L., and Cheng, H.-Y . Efﬁcient and\nrobust parallel dnn training through model parallelism on\nmulti-gpu platform. arXiv:1809.02839 , 2018.\nChen, T., Xu, B., Zhang, C., and Guestrin, C. Train-\ning deep nets with sublinear memory cost. CoRR ,\nabs/1604.06174, 2016. URL http://arxiv .org/\nabs/1604.06174 .\nDai, Z., Yang, Z., Yang, Y ., Carbonell, J. G., Le, Q. V .,\nand Salakhutdinov, R. Transformer-xl: Attentive lan-\nguage models beyond a ﬁxed-length context. CoRR ,\nabs/1901.02860, 2019. URL http://arxiv .org/\nabs/1901.02860 .\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding, 2018.\nGoyal, P., Doll ´ar, P., Girshick, R. B., Noordhuis, P.,\nWesolowski, L., Kyrola, A., Tulloch, A., Jia, Y ., andHe, K. Accurate, large minibatch SGD: training imagenet\nin 1 hour. CoRR , abs/1706.02677, 2017.\nHarlap, A., Narayanan, D., Phanishayee, A., Se-\nshadri, V ., Devanur, N., Ganger, G., and Gibbons, P.\nPipedream: Fast and efﬁcient pipeline parallel dnn train-\ning.arXiv:1806.03377 , 2018.\nHendrycks, D. and Gimpel, K. Bridging nonlinearities\nand stochastic regularizers with gaussian error linear\nunits. CoRR , abs/1606.08415, 2016. URL http:\n//arxiv.org/abs/1606 .08415 .\nHoward, J. and Ruder, S. Fine-tuned language models for\ntext classiﬁcation. CoRR , abs/1801.06146, 2018.\nHuang, Y ., Cheng, Y ., Chen, D., Lee, H., Ngiam, J., Le,\nQ. V ., and Chen, Z. Gpipe: Efﬁcient training of gi-\nant neural networks using pipeline parallelism. CoRR ,\nabs/1811.06965, 2018. URL http://arxiv .org/\nabs/1811.06965 .\nJia, Z., Zaharia, M., and Aiken, A. Beyond data and model\nparallelism for deep neural networks. arXiv:1807.05358 ,\n2018.\nJoshi, M., Chen, D., Liu, Y ., Weld, D. S., Zettlemoyer,\nL., and Levy, O. Spanbert: Improving pre-training by\nrepresenting and predicting spans. arXiv:1907.10529 ,\n2019.\nKeskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy,\nM., and Tang, P. T. P. On large- batch training for deep\nlearning: Generalization gap and sharp minima. ICLR ,\n2017.\nKhandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and\nLewis, M. Generalization through memorization: Nearest\nneighbor language models. arXiv:1911.00172 , 2019.'
 'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nContext : The 36th International Conference on Machine\nLearning (ICML 2019) will be held in Long Beach, CA,\nUSA from June 10th to June 15th, 2019. The conference\nwill consist of one day of tutorials (June 10), followed\nby three days of main conference sessions (June 11-13),\nfollowed by two days of workshops (June 14-15).\nMegatron-LM : With a broad scope, the conference ad-\ndresses the challenges and opportunities in machine learning\nfor practitioners and researchers. The conference program\nwill feature sessions that cover different machine learning\ntopics, from reinforcement learning to computational biol-\nogy and from AI to deep learning. It is the world’s premier\nforum for exploring and presenting advances in machine\nlearning.\nICML 2019 will offer an impressive range of sessions,\nwhich will be of interest to practitioners, researchers, and\nstudents. Some of the topics we will cover include:\nReinforcement learning\nSoft Computing\nComputational biology\nConvolutional networks\nDeep learning\nStatistical learning theory\nStatistical learning machine\nNatural language processing\nBiology\nAwards\nSince its ﬁrst edition, the conference has grown and evolved.\nToday, it is the premier forum for showcasing state-of-the-\nart research in machine learning. The conference is widely\nrecognized as the preeminent venue for showcasing the\nlatest research in machine learning, and a place where prac-\ntitioners, researchers, and students can exchange ideas and\ninsights.\nThe conference is a peer-reviewed venue that draws its pres-\ntige and inﬂuence from the excellence of the presentations\nand the quality of the invited speakers. With over 50% of all\ntalks selected based on their content by an expert committee,\nit is clear that the conference’s high standards of research\nand quality keep a very high bar.Context : Rescue workers on Tuesday found a ﬂight\nrecorder from the Russian plane that crashed into the Black\nSea over the weekend, the defense ministry said. All 92\npeople aboard the Russian military’s Tu-154 plane are be-\nlieved to have died Sunday morning when it crashed two\nminutes after taking off from the southern Russian city of\nSochi.\nMegatron-LM : A child holds up a portrait of a dead relative\nat a makeshift morgue in a building belonging to a local\ngovernment in the town of Petropavlovsk-Kamchatsky. A\nRussian Defense Ministry Tu-154 plane with 92 people\non board crashed into the Black Sea on Sunday morning\nshortly after taking off from the southern Russian city of\nSochi. (Yuri Kochetkov/EPA)\nAccording to the ministry, the plane was found at a depth\nof more than 1,000 feet and the recorder was damaged.\n”The defense ministry has decided to hand over the recorder\nto the Interstate Aviation Committee, which is the main\ninvestigative body in the air crash,” the statement said.\nThe search for the plane’s black boxes is being hampered by\nstrong winds and rough seas. Investigators have launched a\ncriminal probe into the crash, which ofﬁcials said was likely\ncaused by a technical fault.\nRussian President Vladimir Putin declared Wednesday a\nday of mourning for the victims.\nD. Further Scaling Analysis\nIn this section we study the effect of number of attention\nheads on the scaling results. We also present strong scaling\nresults for our 1.2 billion parameter model.\nD.1. Attention Heads and Scaling\nThis section studies the effect of attention heads on model\nparallel scaling. To this end, we consider the 8.3 billion\nparameter conﬁguration with 8-way model parallelism and\nvary the number of heads from 16 to 32. The results are\npresented in Table 7. As the number of attention heads\nincreases, some of the GEMMS inside the self-attention\nlayer become smaller and also the number of elements in\nthe self attention softmax increases. This results in a slight\ndecrease in scaling efﬁciency. Future research should be\nwary of this hyperparameter to design large transformer']", By partitioning key operations such as GEMMs within transformer layers across GPUs and minimizing communication overhead with strategic all-reduce operations.,0.9999999999666667,0.5714285714285714,0.9735873637796155,1.0,0.8837973499549842,0.8351893998199365
How does Megatron-LM address the challenges of large batch training and optimization in transformer models?,"Megatron-LM addresses challenges of large batch training and optimization by utilizing model parallelism, which allows the memory and computation of a model to be distributed across multiple workers. This approach alleviates memory pressure and increases parallelism independently of the microbatch size, enabling the training of huge transformer models without requiring the entire model to fit on a single worker. Additionally, it implements efficient intra-layer model parallelism, which does not require custom compilers or libraries, facilitating smoother scaling and optimization across multiple GPUs.","['Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nContext : The 36th International Conference on Machine\nLearning (ICML 2019) will be held in Long Beach, CA,\nUSA from June 10th to June 15th, 2019. The conference\nwill consist of one day of tutorials (June 10), followed\nby three days of main conference sessions (June 11-13),\nfollowed by two days of workshops (June 14-15).\nMegatron-LM : With a broad scope, the conference ad-\ndresses the challenges and opportunities in machine learning\nfor practitioners and researchers. The conference program\nwill feature sessions that cover different machine learning\ntopics, from reinforcement learning to computational biol-\nogy and from AI to deep learning. It is the world’s premier\nforum for exploring and presenting advances in machine\nlearning.\nICML 2019 will offer an impressive range of sessions,\nwhich will be of interest to practitioners, researchers, and\nstudents. Some of the topics we will cover include:\nReinforcement learning\nSoft Computing\nComputational biology\nConvolutional networks\nDeep learning\nStatistical learning theory\nStatistical learning machine\nNatural language processing\nBiology\nAwards\nSince its ﬁrst edition, the conference has grown and evolved.\nToday, it is the premier forum for showcasing state-of-the-\nart research in machine learning. The conference is widely\nrecognized as the preeminent venue for showcasing the\nlatest research in machine learning, and a place where prac-\ntitioners, researchers, and students can exchange ideas and\ninsights.\nThe conference is a peer-reviewed venue that draws its pres-\ntige and inﬂuence from the excellence of the presentations\nand the quality of the invited speakers. With over 50% of all\ntalks selected based on their content by an expert committee,\nit is clear that the conference’s high standards of research\nand quality keep a very high bar.Context : Rescue workers on Tuesday found a ﬂight\nrecorder from the Russian plane that crashed into the Black\nSea over the weekend, the defense ministry said. All 92\npeople aboard the Russian military’s Tu-154 plane are be-\nlieved to have died Sunday morning when it crashed two\nminutes after taking off from the southern Russian city of\nSochi.\nMegatron-LM : A child holds up a portrait of a dead relative\nat a makeshift morgue in a building belonging to a local\ngovernment in the town of Petropavlovsk-Kamchatsky. A\nRussian Defense Ministry Tu-154 plane with 92 people\non board crashed into the Black Sea on Sunday morning\nshortly after taking off from the southern Russian city of\nSochi. (Yuri Kochetkov/EPA)\nAccording to the ministry, the plane was found at a depth\nof more than 1,000 feet and the recorder was damaged.\n”The defense ministry has decided to hand over the recorder\nto the Interstate Aviation Committee, which is the main\ninvestigative body in the air crash,” the statement said.\nThe search for the plane’s black boxes is being hampered by\nstrong winds and rough seas. Investigators have launched a\ncriminal probe into the crash, which ofﬁcials said was likely\ncaused by a technical fault.\nRussian President Vladimir Putin declared Wednesday a\nday of mourning for the victims.\nD. Further Scaling Analysis\nIn this section we study the effect of number of attention\nheads on the scaling results. We also present strong scaling\nresults for our 1.2 billion parameter model.\nD.1. Attention Heads and Scaling\nThis section studies the effect of attention heads on model\nparallel scaling. To this end, we consider the 8.3 billion\nparameter conﬁguration with 8-way model parallelism and\nvary the number of heads from 16 to 32. The results are\npresented in Table 7. As the number of attention heads\nincreases, some of the GEMMS inside the self-attention\nlayer become smaller and also the number of elements in\nthe self attention softmax increases. This results in a slight\ndecrease in scaling efﬁciency. Future research should be\nwary of this hyperparameter to design large transformer'
 'Megatron-LM: Training Multi-Billion Parameter Language Models Using\nModel Parallelism\nMohammad Shoeybi1 2Mostofa Patwary1 2Raul Puri1 2Patrick LeGresley2Jared Casper2\nBryan Catanzaro2\nAbstract\nRecent work in language modeling demonstrates\nthat training large transformer models advances\nthe state of the art in Natural Language Processing\napplications. However, very large models can be\nquite difﬁcult to train due to memory constraints.\nIn this work, we present our techniques for train-\ning very large transformer models and implement\na simple, efﬁcient intra-layer model parallel ap-\nproach that enables training transformer models\nwith billions of parameters. Our approach does\nnot require a new compiler or library changes, is\northogonal and complimentary to pipeline model\nparallelism, and can be fully implemented with\nthe insertion of a few communication operations\nin native PyTorch. We illustrate this approach\nby converging transformer based models up to\n8.3 billion parameters using 512 GPUs. We sus-\ntain 15.1 PetaFLOPs across the entire applica-\ntion with 76% scaling efﬁciency when compared\nto a strong single GPU baseline that sustains 39\nTeraFLOPs, which is 30% of peak FLOPs. To\ndemonstrate that large language models can fur-\nther advance the state of the art (SOTA), we train\nan 8.3 billion parameter transformer language\nmodel similar to GPT-2 and a 3.9 billion parame-\nter model similar to BERT. We show that careful\nattention to the placement of layer normalization\nin BERT-like models is critical to achieving in-\ncreased performance as the model size grows. Us-\ning the GPT-2 model we achieve SOTA results\non the WikiText103 (10.8 compared to SOTA per-\nplexity of 15.8) and LAMBADA (66.5% com-\npared to SOTA accuracy of 63.2%) datasets. Our\nBERT model achieves SOTA results on the RACE\ndataset (90.9% compared to SOTA accuracy of\n89.4%).\n1Equal contribution2NVIDIA. Correspondence to: Mohammad\nShoeybi <mshoeybi@nvidia.com >.1. Introduction\nNatural Language Processing (NLP) is advancing quickly in\npart due to an increase in available compute and dataset size.\nThe abundance of compute and data enables training increas-\ningly larger language models via unsupervised pretraining\n(Devlin et al., 2018; Radford et al., 2019). Empirical evi-\ndence indicates that larger language models are dramatically\nmore useful for NLP tasks such as article completion, ques-\ntion answering, and natural language inference (Lan et al.,\n2019; Raffel et al., 2019). By ﬁnetuning these pretrained\nlanguage models on downstream natural language tasks,\none can achieve state of the art results as shown in recent\nwork (Devlin et al., 2018; Peters et al., 2018; Howard &\nRuder, 2018; Radford et al., 2018; 2017; Ramachandran\net al., 2016; Liu et al., 2019b; Dai et al., 2019; Yang et al.,\n2019; Liu et al., 2019a; Lan et al., 2019).\nAs these models become larger, they exceed the memory\nlimit of modern processors, and require additional memory\nmanagement techniques such as activation checkpointing\n(Chen et al., 2016). Widely used optimization algorithms\nsuch as ADAM require additional memory per parameter to\nstore momentum and other optimizer state, which reduces\nthe size of models that can be effectively trained. Several\napproaches to model parallelism overcome this limit by\npartitioning the model such that the weights and their asso-\nciated optimizer state do not need to reside concurrently on\nthe processor. For example, GPipe (Huang et al., 2018) and\nMesh-Tensorﬂow (Shazeer et al., 2018) provide frameworks\nfor model parallelism of different kinds. However, they\nrequire rewriting the model, and rely on custom compilers\nand frameworks that are still under development.\nIn this work, we implement a simple and efﬁcient model\nparallel approach using intra-layer model-parallelism. We\nexploit the inherent structure in transformer based language\nmodels to make a simple model-parallel implementation that\ntrains efﬁciently in PyTorch, with no custom C++ code or'
 'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\ninvestigation that will further test existing deep learning\nhardware and software. To realize this, improvements in\nthe efﬁciency and memory footprint of optimizers will be\nneeded. In addition, training a model with more than 16\nbillion parameters will demand more memory than is avail-\nable within 16 GPUs of a DGX-2H box. For such models, a\nhybrid intra-layer and inter-layer model parallelism along\nwith inter-node model parallelism would be more suitable.\nThree other directions of investigation include (a) pretrain-\ning different model families (XLNet, T5), (b) evaluating per-\nformance of large models across more difﬁcult and diverse\ndownstream tasks (e.g. Generative Question Answering,\nSummarization, and Conversation), and (c) using knowl-\nedge distillation to train small student models from these\nlarge pretrained teacher models.\nReferences\nAbadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z.,\nCitro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M.,\nGhemawat, S., Goodfellow, I., Harp, A., Irving, G., Is-\nard, M., Jia, Y ., Jozefowicz, R., Kaiser, L., Kudlur, M.,\nLevenberg, J., Man ´e, D., Monga, R., Moore, S., Mur-\nray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B.,\nSutskever, I., Talwar, K., Tucker, P., Vanhoucke, V ., Va-\nsudevan, V ., Vi ´egas, F., Vinyals, O., Warden, P., Watten-\nberg, M., Wicke, M., Yu, Y ., and Zheng, X. TensorFlow:\nLarge-scale machine learning on heterogeneous systems,\n2015. URL http://tensorflow .org/ . Software\navailable from tensorﬂow.org.\nBa, J. L., Kiros, J. R., and Hinton, G. E. Layernorm. CoRR ,\nabs/1607.06450, 2016. URL http://arxiv .org/\nabs/1607.06450 .\nChen, C.-C., Yang, C.-L., and Cheng, H.-Y . Efﬁcient and\nrobust parallel dnn training through model parallelism on\nmulti-gpu platform. arXiv:1809.02839 , 2018.\nChen, T., Xu, B., Zhang, C., and Guestrin, C. Train-\ning deep nets with sublinear memory cost. CoRR ,\nabs/1604.06174, 2016. URL http://arxiv .org/\nabs/1604.06174 .\nDai, Z., Yang, Z., Yang, Y ., Carbonell, J. G., Le, Q. V .,\nand Salakhutdinov, R. Transformer-xl: Attentive lan-\nguage models beyond a ﬁxed-length context. CoRR ,\nabs/1901.02860, 2019. URL http://arxiv .org/\nabs/1901.02860 .\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding, 2018.\nGoyal, P., Doll ´ar, P., Girshick, R. B., Noordhuis, P.,\nWesolowski, L., Kyrola, A., Tulloch, A., Jia, Y ., andHe, K. Accurate, large minibatch SGD: training imagenet\nin 1 hour. CoRR , abs/1706.02677, 2017.\nHarlap, A., Narayanan, D., Phanishayee, A., Se-\nshadri, V ., Devanur, N., Ganger, G., and Gibbons, P.\nPipedream: Fast and efﬁcient pipeline parallel dnn train-\ning.arXiv:1806.03377 , 2018.\nHendrycks, D. and Gimpel, K. Bridging nonlinearities\nand stochastic regularizers with gaussian error linear\nunits. CoRR , abs/1606.08415, 2016. URL http:\n//arxiv.org/abs/1606 .08415 .\nHoward, J. and Ruder, S. Fine-tuned language models for\ntext classiﬁcation. CoRR , abs/1801.06146, 2018.\nHuang, Y ., Cheng, Y ., Chen, D., Lee, H., Ngiam, J., Le,\nQ. V ., and Chen, Z. Gpipe: Efﬁcient training of gi-\nant neural networks using pipeline parallelism. CoRR ,\nabs/1811.06965, 2018. URL http://arxiv .org/\nabs/1811.06965 .\nJia, Z., Zaharia, M., and Aiken, A. Beyond data and model\nparallelism for deep neural networks. arXiv:1807.05358 ,\n2018.\nJoshi, M., Chen, D., Liu, Y ., Weld, D. S., Zettlemoyer,\nL., and Levy, O. Spanbert: Improving pre-training by\nrepresenting and predicting spans. arXiv:1907.10529 ,\n2019.\nKeskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy,\nM., and Tang, P. T. P. On large- batch training for deep\nlearning: Generalization gap and sharp minima. ICLR ,\n2017.\nKhandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and\nLewis, M. Generalization through memorization: Nearest\nneighbor language models. arXiv:1911.00172 , 2019.'
 'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nFigure 2. Transformer Architecture. Purple blocks correspond to\nfully connected layers. Each blue block represents a single trans-\nformer layer that is replicated N times.\nand compute efﬁciency. The original transformer formula-\ntion was designed as a machine translation architecture that\ntransforms an input sequence into another output sequence\nusing two parts, an Encoder andDecoder . However, recent\nwork leveraging transformers for language modeling such as\nBERT (Devlin et al., 2018) and GPT-2 (Radford et al., 2019)\nuse only the Encoder orDecoder depending on their needs.\nThis work explores both a decoder architecture, GPT-2, and\nan encoder architecture, BERT.\nFigure 2 shows a schematic diagram of the model we used.\nWe refer the reader to prior work for a detailed descrip-\ntion of the model architecture (Vaswani et al., 2017; Devlin\net al., 2018; Radford et al., 2019). It is worthwhile to men-\ntion that both GPT-2 and BERT use GeLU (Hendrycks &\nGimpel, 2016) nonlinearities and layer normalization (Ba\net al., 2016) to the input of the multi-head attention and feed\nforward layers, whereas the original transformer (Vaswani\net al., 2017) uses ReLU nonlinearities and applies layer\nnormalization to outputs.\n2.3. Data and Model Parallelism in Deep Learning\nThere are two central paradigms for scaling out deep neu-\nral network training to numerous hardware accelerators:\ndata parallelism (Valiant, 1990) where a training minibatch\nis split across multiple workers, and model parallelism in\nwhich the memory usage and computation of a model is\ndistributed across multiple workers. By increasing the mini-\nbatch size proportionally to the number of available work-\ners (i.e. weak scaling ), one observes near linear scaling\nin training data throughput. However, large batch train-\ning introduces complications into the optimization process\nthat can result in reduced accuracy or longer time to conver-\ngence, offsetting the beneﬁt of increased training throughput\n(Keskar et al., 2017). Further research (Goyal et al., 2017;\nYou et al., 2017; 2019) has developed techniques to miti-gate these effects and drive down the training time of large\nneural networks. To scale out training even further, parallel\nwork (Chen et al., 2016) has combined data parallelism with\nactivation checkpointing: recomputing activations in the\nbackward pass without storing them in the forward pass to\nreduce memory requirements.\nHowever, these techniques have one fundamental limitation\nin the problem size they can tackle: the model must ﬁt\nentirely on one worker. With language models of increasing\nsize and complexity like BERT and GPT-2, neural networks\nhave approached the memory capacity of modern hardware\naccelerators. One solution to this problem is to employ\nparameter sharing to reduce the memory footprint of the\nmodel (Lan et al., 2019), but this limits the overall capacity\nof the model. Our approach is to utilize model parallelism\nto split the model across multiple accelerators. This not\nonly alleviates the memory pressure, but also increases the\namount of parallelism independently of the microbatch size.\nWithin model parallelism, there are two further paradigms:\nlayer-wise pipeline parallelism, and more general distributed\ntensor computation. In pipeline model parallelism, groups\nof operations are performed on one device before the outputs\nare passed to the next device in the pipeline where a differ-\nent group of operations are performed. Some approaches\n(Harlap et al., 2018; Chen et al., 2018) use a parameter\nserver (Li et al., 2014) in conjunction with pipeline par-\nallelism. However these suffer from inconsistency issues.\nThe GPipe framework for TensorFlow (Huang et al., 2018)\novercomes this inconsistency issue by using synchronous\ngradient decent. This approach requires additional logic to\nhandle the efﬁcient pipelining of these communication and']","Through mixed precision training with dynamic loss scaling, weight initialization strategies, and careful optimizer configurations, including Adam with weight decay.",0.6388888888675925,1.0,0.9810204265398029,1.0,0.8154702007540533,0.8073353484707587
"What specific tasks and benchmarks were used to evaluate PAL's performance, and what were the results?","PAL's performance was evaluated using several math reasoning datasets, including GSM 8K, SVAMP, ASDIV, and MAWPS. The results showed that PAL achieved high solve rates, such as 79.6% for ASDIV and 96.1% for MAWPS-SingleEq, outperforming the CoT approach across these benchmarks. Additionally, PAL demonstrated minimal standard deviations across multiple orders of prompts, indicating consistent performance.","['PAL: Program-aided Language Models 16\n1 8 15 405060708085\nNumber of sampled generations for each questionSolve Rate (%)\nPAL\nCOT\nMinerva\nPaLM\nFigure 12: Comparison of solve rates between PALand baselines as the number of samples increases from 1 to 40. Note that\nthe solve rates for the baselines ( PaLM,COT, Minerva) are obtained through logistic interpolation of solve rates at 1 and 40'
 'PAL: Program-aided Language Models 12\nPart I\nAppendix\nTable of Contents\nA Alternative Prompts without Meaningful Variable Names 13\nB Additional analysis on Arithmetic Reasoning 13\nC Effect of Using Language Models of Code 14\nD Analyzing the Effect of Increasing Number of Samples on P AL 14\nE Standard Deviations Across Multiple Order of Prompts 17\nF P AL Beyond Benchmarks 17\nG Closer Look into Token-level Behaviors of Different Mechanisms 20\nH Datasets 20\nH.1 Creating GSM -HARD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nH.2 GSM -HARD Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nI Generalization of PAL to Least-to-Most Prompting 24\nJ Prompts 26\nJ.1 Reasoning about Colored Objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\nJ.2 Penguins in a Table . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\nJ.3 Date Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\nJ.4 Math . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\nJ.5 Object Counting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\nJ.6 Repeat Copy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\nK Success and Failure Modes in Symbolic Tasks 33\nK.1 Colored Objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\nK.2 Penguins in a Table . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\nK.3 Date Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34'
 'PAL: Program-aided Language Models 17\nE. Standard Deviations Across Multiple Order of Prompts\nFor each math reasoning task, we run inference using three random orderings of the prompts. As shown in Table 7, the\nstandard deviation between the results obtained from the three different seeds is minimal.\nCOT P AL\nAverage Standard Deviation Average Standard Deviation\nGSM 8K 65.6 1.10 72.0 0.16\nSVAMP 74.8 0.19 79.4 0.20\nASDIV 76.9 0.65 79.6 0.14\nGSM -HARD 23.3 0.49 61.2 0.91\nMAWPS -SingleEq 89.1 0.54 96.1 0.30\nMAWPS -SingleOp 91.9 0.55 94.6 0.36\nMAWPS -AddSub 86.0 0.62 92.5 0.34\nMAWPS -MultiArith 95.9 0.51 99.2 0.48\nTable 7: Standard deviations for three runs for the math reasoning datasets.\nF. PAL Beyond Benchmarks\nWe argue that symbolic reasoning is a crucial component in solving a wide range of tasks. In this section, we demonstrate\nexamples of tasks that may not initially appear to require using programs as intermediate reasoning steps, but can be\nimproved through the use of PAL-style reasoning. We demonstrate these examples using the ChatGPT tool.1In contrast to\nthe in-context-learning methods we used in the main paper, here we instruct ChatGPT to perform program-aided reasoning\nthrough one of the user utterances.\nIn Figure 13, in COT-style reasoning, while the reasoning chain is correct, the ﬁnal answer is wrong. In contrast, PAL-style\nreasoning could not only accurately extract the color of objects from the question but also produce the correct lines of code\nto branch to different situations that yield their corresponding correct answers.\nA more intriguing example is letting an LLM count the number of letters in the word “intriguing”. In Figure 14a, while the\nstep-by-step explanation appears reasonable by splitting the letters by spaces, ChatGPT does not change the answer after\nthis explicit reasoning and insists on the wrong answer. Explicitly instructing the model to perform step-by-step reasoning\nbefore answering the question still yields the wrong answer. In contrast, PAL-style reasoning only takes a few lines of code,\nand the execution does produce the correct answer, in this case. These examples indicate that PALcan beneﬁt even an\nostensibly powerful model like ChatGPT.\n1chat.openai.com'
 'PAL: Program-aided Language Models 14\nthat even with a prompt that matches the numbers, there are only modest gains in performance. These results show that the\ngains achieved by using code-based reasoning chains may not be achieved simply by using better few-shot examples for\nCOT.\nRegular Prompt Prompt with Larger Numbers\nCOT 23.3 23.8\nTable 5: GSM-hard results, when the prompts also had examples of larger numbers.\nSuccinct Code The programs used in few-shot examples by PALare multi-step, and show a step-by-step breakdown of\nthe reasoning process. Is this breakdown necessary? Alternatively, can we return a single line expression (see Figure 11b) to\ncalculate the result? Results in Table 6 ( 4throw) shows that is not the case. With single-line expressions, the performance of\nPAL falls to the level of direct prompting.\nGenerating the answer directly PALﬁrst generates a reasoning chain in the form of a Python program, and passes the\ngenerated program to a runtime to obtain an answer. Is PALbetter only because of the program-style intermediate reasoning\nchains, or are the improvements derived from ofﬂoading execution to the Python runtime? To investigate this, we experiment\nwith a variant that forces the LLM to generate the answer after generating the reasoning chain (Figure 11e). This setting\ncompels the LLM to condition on the generated code-based reasoning to generate an answer, simulating the runtime. The\nresults in Table 6 ( 5throw) show that the solve rate drops to near DIRECT levels. This reinforces our hypothesis that while\ncurrent LLMs can be excellent at specifying a high-level plan to solve a task—they are still incapable of executing them.\nAblation Solve Rate\nDIRECT (no intermediate reasoning) 19.7\nCOT 65.6\nPAL 72.0\nSuccinct Code 47.8\nLLM Simulating Runtime 23.2\nTable 6: Solve Rates for Ablations\nC. Effect of Using Language Models of Code\nIn our experiments, we focused on evaluating the performance of a language model for code. We aimed to investigate\nwhether the additional performance boost observed in our results was due to the use of models like Codex, or whether our\nformulation was useful even for text-based models. To this end, we conducted additional experiments using text-based\nlanguage models. Our ﬁndings indicate that the PAL approach is not restricted to working solely with Codex, but can also\nbe applied to natural language (NL) models, as long as the model is sufﬁciently strong. Speciﬁcally, our results showed that\nin the text-davinci-001 model, the use of the CoT approach resulted in better performance.\nModel CoT PaL\ntext-davinci-001 26.5 8.6\ntext-davinci-002 46.9 65.8\ntext-davinci-003 65.3 69.8\nD. Analyzing the Effect of Increasing Number of Samples on P AL\nIn Section 5.1, we show that PALoutperforms strong baselines both for a single sample and by drawing 40 samples and\nusing majority voting. Figure 12 illustrates the trends for cases when the number of samples drawn are between 1 and 40,\nand the interpolation estimates demonstrate that P AL remains competitive throughout the number of samples.']"," PAL was evaluated across 13 tasks, including mathematical, symbolic, and algorithmic reasoning from BIG-Bench Hard and other benchmarks, consistently outperforming larger models and setting new accuracy standards across all evaluated tasks.",0.999999999975,1.0,0.9079317139115984,0.5,0.49847994984757893,0.8689197993903158
How does the inclusion of specific metadata in CodeNet facilitate a wide range of code analysis tasks?,"The specific metadata in CodeNet allows for organized data queries and selections among various problems, languages, and source files, enhancing the efficiency of code analysis tasks. It provides detailed information about each problem, such as CPU time, memory limits, and submission outcomes, which can be utilized for performance evaluations and comparisons. This rich metadata structure enables researchers to benchmark a variety of coding tasks, including code similarity, classification, and translation, thus facilitating comprehensive analysis.","['9 Further Uses of CodeNet\nThe rich metadata and language diversity open CodeNet to a plethora of use cases. The problem-\nsubmission relationship in CodeNet corresponds to type-4 similarity [ 43] and can be used for code\nsearch and clone detection. The code samples in CodeNet are labeled with their acceptance status\nso we can readily extract pairs of buggy and ﬁxed code for code repair [ 49,50]. A large number\nof code samples come with inputs so that we can execute the code to extract the CPU run time and\nmemory footprint, which can be used for regression studies and prediction.\nCodeNet may also be used for program translation, given its wealth of programs written in a multitude\nof languages. Translation between two programming languages is born out of a practical need to port\nlegacy codebases to modern languages in order to increase accessibility and lower maintenance costs.\nWith the help of neural networks, machine translation models developed for natural languages [ 51]\nwere adapted to programming languages, producing pivotal success [ 4]. One considerable challenge of\nneural machine translation is that model training depends on large, parallel corpora that are expensive\nto curate [ 52], especially for low-resource languages (e.g., legacy code). Recently, monolingual\napproaches [ 53,4] were developed to mitigate the reliance on parallel data, paving ways to build\nmodels for languages with little translation. Compared with current popular data sets (e.g., [ 4,54]),\nCodeNet covers a much richer set of languages with ample training instances.\n10 Conclusion\nArtiﬁcial intelligence has made great strides in understanding human language. Computer scientists\nhave been fascinated by the possibility and tantalized by the vision of computers (AI) programming\ncomputers. In this paper, we presented ""CodeNet"", a ﬁrst-of-its-kind very large-scale, diverse and\nhigh-quality dataset to accelerate the algorithmic advances in AI for Code. This dataset is not\nonly unique in its scale, but also in the diversity of coding tasks it can help benchmark: from code\nsimilarity and classiﬁcation for advances in code recommendation algorithms, and code translation\nbetween a large variety of programming languages, to advances in code performance improvement\ntechniques. We hope that the scale, diversity and rich, high-quality annotations of CodeNet will offer\nunprecedented research opportunities at the intersection of AI and Software Engineering.\n11 Acknowledgements\nWe would like to acknowledge AIZU and AtCoder for making the code submissions publicly available.\nWe would like to thank the IBM Data Asset eXchange team for providing a platform to host the\nCodeNet dataset. We would like to thank the Women in Data Science team at Stanford University\nand the IBM Call for Code team for their collaboration in launching the CodeNet challenge.\n12 Bibliography\n[1]Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. A survey of\nmachine learning for big code and naturalness. ACM Computing Surveys (CSUR) , 51(4):1–37,\n2018.\n[2]Yanming Yang, Xin Xia, David Lo, and John Grundy. A survey on deep learning for software\nengineering. arXiv preprint arXiv:2011.14597 , 2020.\n[3]Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul\nPuri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke\nChan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad\nBavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias\nPlappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-V oss, William Hebgen Guss, Alex\nNichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra,'
 'Given the success of non-AI tools for code, why should we consider AI to augment or possibly\nreplace them? Firstly, AI can help reﬁne and re-tune the heuristics used by traditional coding tools.\nSecondly, based on the training data from past experience, AI can help prioritize when there is more\nthan one sound answer [ 5]. Thirdly, an AI-based tool may handle incomplete or invalid code more\nrobustly, thus expanding its scope. Finally, AI can incorporate signals usually ignored by traditional\ntools for code, such as the natural language in identiﬁers or comments.\nIn the enterprise environment, developers often face code written by large teams over many years\nand geographies. Developers must manipulate such code to modernize it, ﬁx bugs, improve its\nperformance, evolve it when requirements change, make it more secure, and/or comply with regu-\nlations. These tasks are challenging, and it is crucial to provide tool support for developers to be\nmore productive at performing them. It is well known that the latest advancements in deep learning\nalgorithms rely on best-of-breed datasets, such as ImageNet, to create increasingly complex and\npowerful models. In this paper, we present ""CodeNet"", a ﬁrst-of-its-kind dataset in scale, diversity,\nand quality, to accelerate the algorithmic advances in AI for Code.\nTo promote widespread adoption of CodeNet, we will be launching contests involving use cases based\non the dataset. The ﬁrst contest [ 6] will focus on diversity, inclusion and spurring interest among\naspiring data scientists. We are partnering with the Global Women in Data Science organization (with\npresence in over 50 countries) founded by Stanford University [ 7] and targeting teams with at least\nﬁfty percent women. We are planning follow-up contests that target experienced AI practitioners.\nThe rest of the paper is organized as follows. Section 2 introduces the CodeNet dataset. Related\ndatasets are discussed in Section 3, and the differentiation of CodeNet with respect to these related\ndatasets is elaborated in Section 4. Section 5 describes how CodeNet was curated and Section 6\nenumerates the usability features of CodeNet with several pre-processing tools to transform source\ncodes into representations that can be readily used as inputs into machine learning models. Section 7\ndiscusses the upcoming CodeNet contest and Section 8 describes important baseline experiments\nwith the CodeNet dataset. Section 9 presents further uses of the CodeNet dataset and Section 10\nconcludes the paper.\n2 The CodeNet Dataset\nThe CodeNet dataset consists of a large collection of code samples with extensive metadata. It\nalso contains documented tools to transform code samples into intermediate representations and to\naccess the dataset and make tailored selections. Our goal is to provide the community with a large,\nhigh-quality curated dataset that can be used to advance AI techniques for source code.\nCodeNet is derived from the data available on two online judge websites: AIZU [ 8] and AtCoder [ 9].\nOnline judge websites pose programming problems in the form of courses and contests. The dataset\nconsists of submissions to these problems, which are judged by an automated review process for\ncorrectness. Problem descriptions, submission outcomes, and associated metadata are available via\nvarious REST APIs.\nScale and Statistics. CodeNet contains a total of 13,916,868 submissions, divided into 4053\nproblems. Among the submissions, 53.6% (7,460,588) are accepted (compilable and pass the\nprescribed tests), 29.5% are marked with wrong answer, and the remaining rejected due to their\nfailure to meet run time or memory requirements. To our knowledge, this is the largest dataset so\nfar among similar kinds. Submissions are in 55 different languages; 95% of them are coded in C++,\nPython, Java, C, Ruby, and C#. C++ is the most common language, with 8,008,527 submissions (57%'
 '(a) Languages\n (b) Status\nFigure 1: Percentage of submissions per language (left) and per status (right).\nMetadata. The metadata enables data queries and selections among the large collection of problems,\nlanguages, and source ﬁles. The metadata is organized in a two level hierarchy. The ﬁrst is the\ndataset level, which describes all problems. The second is the problem level, which details all the\nsubmissions to a single problem. Metadata and data are separated in the dataset structure.\nAt the dataset level, a single CSV ﬁle lists all problems and their origins, along with the CPU time\nand memory limits set for them. Additionally, every problem has an HTML ﬁle with a detailed\ndescription of the problem, the requirements and constraints, and the IO examples.\nAt the problem level, every problem has a CSV ﬁle. The metadata for each submission is summarized\nin Table 2 below, which lists the ﬁelds contained in each CSV ﬁle as well as the corresponding\ndescriptions.\n2.1 How to read the CodeNet dataset\nThe data and metadata are organized in a rigorous directory structure. The top level Project_CodeNet\ndirectory contains several sub-directories: data ,metadata ,problem_descriptions , and\nderived . The code samples or submissions reside under the data directory. The data directory\nis organized as (problem_id)/(language)/(submission) , so the ﬁle path data/p00023/C++/\ns006384060.cpp denotes a submission to problem p00023 in C++ with id s006384060. Detailed\nstatement of the problems can be found in problem_descriptions/(problem_id).html . The\nmeta data for the dataset is contained in the metadata directory. metadata/problem_list.csv\ncontains metadata for all the problems in the dataset, which is summarized in Table 1. metadata/\n(problem_id).csv contains the metadata for all the submissions to problem problem_id , which is\ndescribed in Table 2. Each submission comes with cpu time, memory usage and status with possible\nvalues described in Table 3. The derived directory contains information derived from the dataset,\nsuch as near-duplicate information for submissions to speciﬁc languages, token sequences for code\nsamples, and information on identical problems.\nTable 1: Metadata at the dataset level\nname of column data type unit description\nid string none unique anonymized id of the problem\nname string none short name of the problem\ndataset string none original dataset, AIZU or AtCoder\ntime_limit int millisecond maximum time allowed for a submission\nmemory_limit int KB maximum memory allowed for a submission\nrating int none rating, i.e., difﬁculty of the problem\ntags string none list of tags separated by ""|""; not used\ncomplexity string none degree of difﬁculty of the problem; not used\n3'
 'CodeNet: A Large-Scale AI for Code Dataset for\nLearning a Diversity of Coding Tasks\nRuchir Puri1, David S. Kung1, Geert Janssen1, Wei Zhang1,\nGiacomo Domeniconi1,Vladimir Zolotov1,Julian Dolby1,Jie Chen2,1,\nMihir Choudhury1,Lindsey Decker1,Veronika Thost2,1,Luca Buratti1,\nSaurabh Pujar1,Shyam Ramji1,Ulrich Finkler1,Susan Malaika3,Frederick Reiss1\n1IBM Research\n2MIT-IBM Watson AI Lab\n3IBM Worldwide Ecosystems\nAbstract\nOver the last several decades, software has been woven into the fabric of every\naspect of our society. As software development surges and code infrastructure of\nenterprise applications ages, it is now more critical than ever to increase software\ndevelopment productivity and modernize legacy applications. Advances in deep\nlearning and machine learning algorithms have enabled breakthroughs in computer\nvision, speech recognition, natural language processing and beyond, motivating\nresearchers to leverage AI techniques to improve software development efﬁciency.\nThus, the fast-emerging research area of “AI for Code” has garnered new interest\nand gathered momentum. In this paper, we present a large-scale dataset CodeNet ,\nconsisting of over 14 million code samples and about 500 million lines of code\nin 55 different programming languages, which is aimed at teaching AI to code.\nIn addition to its large scale, CodeNet has a rich set of high-quality annotations\nto benchmark and help accelerate research in AI techniques for a variety of crit-\nical coding tasks, including code similarity and classiﬁcation, code translation\nbetween a large variety of programming languages, and code performance (runtime\nand memory) improvement techniques. Additionally, CodeNet provides sample\ninput and output test sets for 98.5% of the code samples, which can be used as\nan oracle for determining code correctness and potentially guide reinforcement\nlearning for code quality improvements. As a usability feature, we provide several\npre-processing tools in CodeNet to transform source code into representations that\ncan be readily used as inputs into machine learning models. Results of code classi-\nﬁcation and code similarity experiments using the CodeNet dataset are provided as\na reference. We hope that the scale, diversity and rich, high-quality annotations of\nCodeNet will offer unprecedented research opportunities at the intersection of AI\nand Software Engineering.\n1 Introduction\nThere is a growing trend towards leveraging AI for building tools that support software engineering\nand development [ 1,2]. AI can manipulate and generate computer code, but can it do so with\nhigh quality? Many researchers are fascinated by this possibility, encouraged by AI successes in\nother domains and tantalized by the vision of computers programming computers. Some recent\ndeep-learning models [ 3,4] for code have received a lot of publicity: trained on vast amounts of\ndata and using novel architectures with billions of parameters, they sometimes generate surprisingly\nplausible code.\nPreprint. Under review.arXiv:2105.12655v2  [cs.SE]  29 Aug 2021']","Rich metadata including problem descriptions, submission outcomes, and programming languages enable tasks like bug fixing, performance improvement, and understanding coding patterns.",0.999999999975,0.75,0.9366074244779558,1.0,0.5863491807031219,0.897120860743522
"What types of tasks are included in SuperGLUE, and how do they enhance the benchmark's complexity?","SuperGLUE includes eight tasks that test various aspects of language understanding, such as Boolean Questions (BoolQ), CommitmentBank (CB), causal reasoning (COPA), and Multi-Sentence Reading Comprehension (MultiRC). These tasks enhance the benchmark's complexity by requiring models to understand and reason about context, handle multiple correct answers, and engage with more diverse input formats beyond simple sentence-pair classifications. The tasks are designed to be challenging for current NLP systems while still solvable by college-educated English speakers, thus pushing the state of the art further.","['BiLSTM+ELMo+Attn\nOpenAI GPT\nBERT + Single-task Adapters\nBERT (Large)\nBERT on STILTs\nBERT + BAM\nSemBERT\nSnorkel MeTaL\nALICE (Large)\nMT-DNN (ensemble)\nXLNet-Large (ensemble)0.50.60.70.80.91.01.11.2\nGLUE Score\nHuman Performance\nCoLA\nSST-2MRPC\nSTS-B\nQQP\nMNLIQNLI\nRTE\nWNLIFigure 1: GLUE benchmark performance for submitted systems, rescaled to set human performance\nto 1.0, shown as a single number score, and broken down into the nine constituent task performances.\nFor tasks with multiple metrics, we use an average of the metrics. More information on the tasks\nincluded in GLUE can be found in Wang et al. (2019a) and in Warstadt et al. (2019, CoLA), Socher\net al. (2013, SST-2), Dolan and Brockett (2005, MRPC), Cer et al. (2017, STS-B), and Williams et al.\n(2018, MNLI), and Rajpurkar et al. (2016, the original data source for QNLI).\nremains substantial scope for improvement towards GLUE’s high-level goals, the original version of\nthe benchmark is no longer a suitable metric for quantifying such progress.\nIn response, we introduce SuperGLUE, a new benchmark designed to pose a more rigorous test of\nlanguage understanding. SuperGLUE has the same high-level motivation as GLUE: to provide a\nsimple, hard-to-game measure of progress toward general-purpose language understanding technolo-\ngies for English. We anticipate that signiﬁcant progress on SuperGLUE should require substantive\ninnovations in a number of core areas of machine learning, including sample-efﬁcient, transfer,\nmultitask, and unsupervised or self-supervised learning.\nSuperGLUE follows the basic design of GLUE: It consists of a public leaderboard built around\neight language understanding tasks, drawing on existing data, accompanied by a single-number\nperformance metric, and an analysis toolkit. However, it improves upon GLUE in several ways:\nMore challenging tasks: SuperGLUE retains the two hardest tasks in GLUE. The remaining tasks\nwere identiﬁed from those submitted to an open call for task proposals and were selected based on\ndifﬁculty for current NLP approaches.\nMore diverse task formats: The task formats in GLUE are limited to sentence- and sentence-pair\nclassiﬁcation. We expand the set of task formats in SuperGLUE to include coreference resolution\nand question answering (QA).\nComprehensive human baselines: We include human performance estimates for all benchmark\ntasks, which verify that substantial headroom exists between a strong BERT-based baseline and\nhuman performance.\nImproved code support: SuperGLUE is distributed with a new, modular toolkit for work on\npretraining, multi-task learning, and transfer learning in NLP, built around standard tools including\nPyTorch (Paszke et al., 2017) and AllenNLP (Gardner et al., 2017).\nReﬁned usage rules: The conditions for inclusion on the SuperGLUE leaderboard have been\nrevamped to ensure fair competition, an informative leaderboard, and full credit assignment to data\nand task creators.\nThe SuperGLUE leaderboard, data, and software tools are available at super.gluebenchmark.com .\n2 Related Work\nMuch work prior to GLUE demonstrated that training neural models with large amounts of available\nsupervision can produce representations that effectively transfer to a broad range of NLP tasks\n2'
 'SuperGLUE: A Stickier Benchmark for\nGeneral-Purpose Language Understanding Systems\nAlex Wang∗\nNew York UniversityYada Pruksachatkun∗\nNew York UniversityNikita Nangia∗\nNew York University\nAmanpreet Singh∗\nFacebook AI ResearchJulian Michael\nUniversity of WashingtonFelix Hill\nDeepMindOmer Levy\nFacebook AI Research\nSamuel R. Bowman\nNew York University\nAbstract\nIn the last year, new models and methods for pretraining and transfer learning have\ndriven striking performance improvements across a range of language understand-\ning tasks. The GLUE benchmark, introduced a little over one year ago, offers\na single-number metric that summarizes progress on a diverse set of such tasks,\nbut performance on the benchmark has recently surpassed the level of non-expert\nhumans, suggesting limited headroom for further research. In this paper we present\nSuperGLUE, a new benchmark styled after GLUE with a new set of more difﬁ-\ncult language understanding tasks, a software toolkit, and a public leaderboard.\nSuperGLUE is available at super.gluebenchmark.com .\n1 Introduction\nRecently there has been notable progress across many natural language processing (NLP) tasks, led\nby methods such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT\n(Devlin et al., 2019). The unifying theme of these methods is that they couple self-supervised learning\nfrom massive unlabelled text corpora with effective adapting of the resulting model to target tasks.\nThe tasks that have proven amenable to this general approach include question answering, textual\nentailment, and parsing, among many others (Devlin et al., 2019; Kitaev et al., 2019, i.a.).\nIn this context, the GLUE benchmark (Wang et al., 2019a) has become a prominent evaluation\nframework for research towards general-purpose language understanding technologies. GLUE is\na collection of nine language understanding tasks built on existing public datasets, together with\nprivate test data, an evaluation server, a single-number target metric, and an accompanying expert-\nconstructed diagnostic set. GLUE was designed to provide a general-purpose evaluation of language\nunderstanding that covers a range of training data volumes, task genres, and task formulations. We\nbelieve it was these aspects that made GLUE particularly appropriate for exhibiting the transfer-\nlearning potential of approaches like OpenAI GPT and BERT.\nThe progress of the last twelve months has eroded headroom on the GLUE benchmark dramatically.\nWhile some tasks (Figure 1) and some linguistic phenomena (Figure 2 in Appendix B) measured\nin GLUE remain difﬁcult, the current state of the art GLUE Score as of early July 2019 (88.4 from\nYang et al., 2019) surpasses human performance (87.1 from Nangia and Bowman, 2019) by 1.3\npoints, and in fact exceeds this human performance estimate on four tasks. Consequently, while there\n∗Equal contribution. Correspondence: glue-benchmark-admin@googlegroups.com\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.arXiv:1905.00537v3  [cs.CL]  13 Feb 2020'
 'Table 2: Development set examples from the tasks in SuperGLUE. Bold text represents part of the\nexample format for each task. Text in italics is part of the model input. Underlined text is specially\nmarked in the input. Text in a monospaced font represents the expected model output.BoolQPassage: Barq’s – Barq’s is an American soft drink. Its brand of root beer is notable for having caffeine.\nBarq’s, created by Edward Barq and bottled since the turn of the 20th century, is owned by the Barq\nfamily but bottled by the Coca-Cola Company. It was known as Barq’s Famous Olde Tyme Root Beer\nuntil 2012.\nQuestion: is barq’s root beer a pepsi product Answer: NoCBText: B: And yet, uh, I we-, I hope to see employer based, you know, helping out. You know, child, uh,\ncare centers at the place of employment and things like that, that will help out. A: Uh-huh. B: What do\nyou think, do you think we are, setting a trend?\nHypothesis: they are setting a trend Entailment: UnknownCOPAPremise: My body cast a shadow over the grass. Question: What’s the CAUSE for this?\nAlternative 1: The sun was rising. Alternative 2: The grass was cut.\nCorrect Alternative: 1MultiRCParagraph: Susan wanted to have a birthday party. She called all of her friends. She has ﬁve friends.\nHer mom said that Susan can invite them all to the party. Her ﬁrst friend could not go to the party\nbecause she was sick. Her second friend was going out of town. Her third friend was not so sure if her\nparents would let her. The fourth friend said maybe. The ﬁfth friend could go to the party for sure. Susan\nwas a little sad. On the day of the party, all ﬁve friends showed up. Each friend had a present for Susan.\nSusan was happy and sent each friend a thank you card the next week\nQuestion: Did Susan’s sick friend recover? Candidate answers: Yes, she recovered (T),No(F),Yes\n(T),No, she didn’t recover (F),Yes, she was at Susan’s party (T)ReCoRDParagraph: (CNN )Puerto Rico on Sunday overwhelmingly voted for statehood. But Congress, the only\nbody that can approve new states, will ultimately decide whether the status of the UScommonwealth\nchanges. Ninety-seven percent of the votes in the nonbinding referendum favored statehood, an increase\nover the results of a 2012 referendum, ofﬁcial results from the State Electorcal Commission show. It\nwas the ﬁfth such vote on statehood. ""Today, we the people of Puerto Rico are sending a strong and\nclear message to the US Congress ... and to the world ... claiming our equal rights as American citizens,\nPuerto Rico Gov. Ricardo Rossello said in a news release. @highlight Puerto Rico voted Sunday in\nfavor of US statehood\nQuery For one, they can truthfully say, “Don’t blame me, I didn’t vote for them, ” when discussing the\n<placeholder> presidency Correct Entities: USRTEText: Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44,\naccording to the Christopher Reeve Foundation.\nHypothesis: Christopher Reeve had an accident. Entailment: FalseWiCContext 1: Room and board .Context 2: He nailed boards across the windows.\nSense match: FalseWSCText: Mark told Pete many lies about himself, which Pete included in his book. Heshould have been\nmore truthful. Coreference: False\n3 SuperGLUE Overview\n3.1 Design Process\nThe goal of SuperGLUE is to provide a simple, robust evaluation metric of any method capable of\nbeing applied to a broad range of language understanding tasks. To that end, in designing SuperGLUE,\nwe identify the following desiderata of tasks in the benchmark:\nTask substance: Tasks should test a system’s ability to understand and reason about texts in English.\nTask difﬁculty: Tasks should be beyond the scope of current state-of-the-art systems, but solvable by\nmost college-educated English speakers. We exclude tasks that require domain-speciﬁc knowledge,\ne.g. medical notes or scientiﬁc papers.\nEvaluability: Tasks must have an automatic performance metric that corresponds well to human'
 'Public data: We require that tasks have existing public training data in order to minimize the risks\ninvolved in newly-created datasets. We also prefer tasks for which we have access to (or could create)\na test set with private labels.\nTask format: We prefer tasks that had relatively simple input and output formats, to avoid incentiviz-\ning the users of the benchmark to create complex task-speciﬁc model architectures. Still, while GLUE\nis restricted to tasks involving single sentence or sentence pair inputs, for SuperGLUE we expand\nthe scope to consider tasks with longer inputs. This yields a set of tasks that requires understanding\nindividual tokens in context, complete sentences, inter-sentence relations, and entire paragraphs.\nLicense: Task data must be available under licences that allow use and redistribution for research\npurposes.\nTo identify possible tasks for SuperGLUE, we disseminated a public call for task proposals to the\nNLP community, and received approximately 30 proposals. We ﬁltered these proposals according\nto our criteria. Many proposals were not suitable due to licensing issues, complex formats, and\ninsufﬁcient headroom; we provide examples of such tasks in Appendix D. For each of the remaining\ntasks, we ran a BERT-based baseline and a human baseline, and ﬁltered out tasks which were either\ntoo challenging for humans without extensive training or too easy for our machine baselines.\n3.2 Selected Tasks\nFollowing this process, we arrived at eight tasks to use in SuperGLUE. See Tables 1 and 2 for details\nand speciﬁc examples of each task.\nBoolQ (Boolean Questions, Clark et al., 2019a) is a QA task where each example consists of a short\npassage and a yes/no question about the passage. The questions are provided anonymously and\nunsolicited by users of the Google search engine, and afterwards paired with a paragraph from a\nWikipedia article containing the answer. Following the original work, we evaluate with accuracy.\nCB(CommitmentBank, de Marneffe et al., 2019) is a corpus of short texts in which at least one\nsentence contains an embedded clause. Each of these embedded clauses is annotated with the degree\nto which it appears the person who wrote the text is committed to the truth of the clause. The resulting\ntask framed as three-class textual entailment on examples that are drawn from the Wall Street Journal,\nﬁction from the British National Corpus, and Switchboard. Each example consists of a premise\ncontaining an embedded clause and the corresponding hypothesis is the extraction of that clause.\nWe use a subset of the data that had inter-annotator agreement above 80%. The data is imbalanced\n(relatively fewer neutral examples), so we evaluate using accuracy and F1, where for multi-class F1\nwe compute the unweighted average of the F1 per class.\nCOPA (Choice of Plausible Alternatives, Roemmele et al., 2011) is a causal reasoning task in which\na system is given a premise sentence and must determine either the cause or effect of the premise\nfrom two possible choices. All examples are handcrafted and focus on topics from blogs and a\nphotography-related encyclopedia. Following the original work, we evaluate using accuracy.\nMultiRC (Multi-Sentence Reading Comprehension, Khashabi et al., 2018) is a QA task where each\nexample consists of a context paragraph, a question about that paragraph, and a list of possible\nanswers. The system must predict which answers are true and which are false. While many QA\ntasks exist, we use MultiRC because of a number of desirable properties: (i) each question can have\nmultiple possible correct answers, so each question-answer pair must be evaluated independent of\nother pairs, (ii) the questions are designed such that answering each question requires drawing facts\nfrom multiple context sentences, and (iii) the question-answer pair format more closely matches\nthe API of other tasks in SuperGLUE than the more popular span-extractive QA format does. The']","SuperGLUE includes eight diverse tasks such as Boolean Questions (BoolQ), Choice of Plausible Alternatives (COPA), and Winograd Schema Challenge (WSC), enhancing complexity through a mix of question answering, natural language inference, coreference resolution, and word sense disambiguation tasks.",0.999999999975,1.0,0.8923126198921713,0.5,0.8512600849076045,0.9504948850849636
"What criteria were used to select tasks for inclusion in SuperGLUE, and how do these criteria benefit the benchmark?","Tasks for SuperGLUE were selected based on criteria such as the availability of public training data, task format simplicity, existing human performance baselines, and the ability to evaluate tasks with automatic performance metrics. These criteria ensure that the tasks are challenging yet solvable by college-educated individuals, promoting fair competition and providing a robust evaluation of language understanding capabilities. Additionally, the focus on diverse task formats and comprehensive baselines helps to better gauge progress in natural language processing technologies.","['Public data: We require that tasks have existing public training data in order to minimize the risks\ninvolved in newly-created datasets. We also prefer tasks for which we have access to (or could create)\na test set with private labels.\nTask format: We prefer tasks that had relatively simple input and output formats, to avoid incentiviz-\ning the users of the benchmark to create complex task-speciﬁc model architectures. Still, while GLUE\nis restricted to tasks involving single sentence or sentence pair inputs, for SuperGLUE we expand\nthe scope to consider tasks with longer inputs. This yields a set of tasks that requires understanding\nindividual tokens in context, complete sentences, inter-sentence relations, and entire paragraphs.\nLicense: Task data must be available under licences that allow use and redistribution for research\npurposes.\nTo identify possible tasks for SuperGLUE, we disseminated a public call for task proposals to the\nNLP community, and received approximately 30 proposals. We ﬁltered these proposals according\nto our criteria. Many proposals were not suitable due to licensing issues, complex formats, and\ninsufﬁcient headroom; we provide examples of such tasks in Appendix D. For each of the remaining\ntasks, we ran a BERT-based baseline and a human baseline, and ﬁltered out tasks which were either\ntoo challenging for humans without extensive training or too easy for our machine baselines.\n3.2 Selected Tasks\nFollowing this process, we arrived at eight tasks to use in SuperGLUE. See Tables 1 and 2 for details\nand speciﬁc examples of each task.\nBoolQ (Boolean Questions, Clark et al., 2019a) is a QA task where each example consists of a short\npassage and a yes/no question about the passage. The questions are provided anonymously and\nunsolicited by users of the Google search engine, and afterwards paired with a paragraph from a\nWikipedia article containing the answer. Following the original work, we evaluate with accuracy.\nCB(CommitmentBank, de Marneffe et al., 2019) is a corpus of short texts in which at least one\nsentence contains an embedded clause. Each of these embedded clauses is annotated with the degree\nto which it appears the person who wrote the text is committed to the truth of the clause. The resulting\ntask framed as three-class textual entailment on examples that are drawn from the Wall Street Journal,\nﬁction from the British National Corpus, and Switchboard. Each example consists of a premise\ncontaining an embedded clause and the corresponding hypothesis is the extraction of that clause.\nWe use a subset of the data that had inter-annotator agreement above 80%. The data is imbalanced\n(relatively fewer neutral examples), so we evaluate using accuracy and F1, where for multi-class F1\nwe compute the unweighted average of the F1 per class.\nCOPA (Choice of Plausible Alternatives, Roemmele et al., 2011) is a causal reasoning task in which\na system is given a premise sentence and must determine either the cause or effect of the premise\nfrom two possible choices. All examples are handcrafted and focus on topics from blogs and a\nphotography-related encyclopedia. Following the original work, we evaluate using accuracy.\nMultiRC (Multi-Sentence Reading Comprehension, Khashabi et al., 2018) is a QA task where each\nexample consists of a context paragraph, a question about that paragraph, and a list of possible\nanswers. The system must predict which answers are true and which are false. While many QA\ntasks exist, we use MultiRC because of a number of desirable properties: (i) each question can have\nmultiple possible correct answers, so each question-answer pair must be evaluated independent of\nother pairs, (ii) the questions are designed such that answering each question requires drawing facts\nfrom multiple context sentences, and (iii) the question-answer pair format more closely matches\nthe API of other tasks in SuperGLUE than the more popular span-extractive QA format does. The'
 'BiLSTM+ELMo+Attn\nOpenAI GPT\nBERT + Single-task Adapters\nBERT (Large)\nBERT on STILTs\nBERT + BAM\nSemBERT\nSnorkel MeTaL\nALICE (Large)\nMT-DNN (ensemble)\nXLNet-Large (ensemble)0.50.60.70.80.91.01.11.2\nGLUE Score\nHuman Performance\nCoLA\nSST-2MRPC\nSTS-B\nQQP\nMNLIQNLI\nRTE\nWNLIFigure 1: GLUE benchmark performance for submitted systems, rescaled to set human performance\nto 1.0, shown as a single number score, and broken down into the nine constituent task performances.\nFor tasks with multiple metrics, we use an average of the metrics. More information on the tasks\nincluded in GLUE can be found in Wang et al. (2019a) and in Warstadt et al. (2019, CoLA), Socher\net al. (2013, SST-2), Dolan and Brockett (2005, MRPC), Cer et al. (2017, STS-B), and Williams et al.\n(2018, MNLI), and Rajpurkar et al. (2016, the original data source for QNLI).\nremains substantial scope for improvement towards GLUE’s high-level goals, the original version of\nthe benchmark is no longer a suitable metric for quantifying such progress.\nIn response, we introduce SuperGLUE, a new benchmark designed to pose a more rigorous test of\nlanguage understanding. SuperGLUE has the same high-level motivation as GLUE: to provide a\nsimple, hard-to-game measure of progress toward general-purpose language understanding technolo-\ngies for English. We anticipate that signiﬁcant progress on SuperGLUE should require substantive\ninnovations in a number of core areas of machine learning, including sample-efﬁcient, transfer,\nmultitask, and unsupervised or self-supervised learning.\nSuperGLUE follows the basic design of GLUE: It consists of a public leaderboard built around\neight language understanding tasks, drawing on existing data, accompanied by a single-number\nperformance metric, and an analysis toolkit. However, it improves upon GLUE in several ways:\nMore challenging tasks: SuperGLUE retains the two hardest tasks in GLUE. The remaining tasks\nwere identiﬁed from those submitted to an open call for task proposals and were selected based on\ndifﬁculty for current NLP approaches.\nMore diverse task formats: The task formats in GLUE are limited to sentence- and sentence-pair\nclassiﬁcation. We expand the set of task formats in SuperGLUE to include coreference resolution\nand question answering (QA).\nComprehensive human baselines: We include human performance estimates for all benchmark\ntasks, which verify that substantial headroom exists between a strong BERT-based baseline and\nhuman performance.\nImproved code support: SuperGLUE is distributed with a new, modular toolkit for work on\npretraining, multi-task learning, and transfer learning in NLP, built around standard tools including\nPyTorch (Paszke et al., 2017) and AllenNLP (Gardner et al., 2017).\nReﬁned usage rules: The conditions for inclusion on the SuperGLUE leaderboard have been\nrevamped to ensure fair competition, an informative leaderboard, and full credit assignment to data\nand task creators.\nThe SuperGLUE leaderboard, data, and software tools are available at super.gluebenchmark.com .\n2 Related Work\nMuch work prior to GLUE demonstrated that training neural models with large amounts of available\nsupervision can produce representations that effectively transfer to a broad range of NLP tasks\n2'
 'Table 2: Development set examples from the tasks in SuperGLUE. Bold text represents part of the\nexample format for each task. Text in italics is part of the model input. Underlined text is specially\nmarked in the input. Text in a monospaced font represents the expected model output.BoolQPassage: Barq’s – Barq’s is an American soft drink. Its brand of root beer is notable for having caffeine.\nBarq’s, created by Edward Barq and bottled since the turn of the 20th century, is owned by the Barq\nfamily but bottled by the Coca-Cola Company. It was known as Barq’s Famous Olde Tyme Root Beer\nuntil 2012.\nQuestion: is barq’s root beer a pepsi product Answer: NoCBText: B: And yet, uh, I we-, I hope to see employer based, you know, helping out. You know, child, uh,\ncare centers at the place of employment and things like that, that will help out. A: Uh-huh. B: What do\nyou think, do you think we are, setting a trend?\nHypothesis: they are setting a trend Entailment: UnknownCOPAPremise: My body cast a shadow over the grass. Question: What’s the CAUSE for this?\nAlternative 1: The sun was rising. Alternative 2: The grass was cut.\nCorrect Alternative: 1MultiRCParagraph: Susan wanted to have a birthday party. She called all of her friends. She has ﬁve friends.\nHer mom said that Susan can invite them all to the party. Her ﬁrst friend could not go to the party\nbecause she was sick. Her second friend was going out of town. Her third friend was not so sure if her\nparents would let her. The fourth friend said maybe. The ﬁfth friend could go to the party for sure. Susan\nwas a little sad. On the day of the party, all ﬁve friends showed up. Each friend had a present for Susan.\nSusan was happy and sent each friend a thank you card the next week\nQuestion: Did Susan’s sick friend recover? Candidate answers: Yes, she recovered (T),No(F),Yes\n(T),No, she didn’t recover (F),Yes, she was at Susan’s party (T)ReCoRDParagraph: (CNN )Puerto Rico on Sunday overwhelmingly voted for statehood. But Congress, the only\nbody that can approve new states, will ultimately decide whether the status of the UScommonwealth\nchanges. Ninety-seven percent of the votes in the nonbinding referendum favored statehood, an increase\nover the results of a 2012 referendum, ofﬁcial results from the State Electorcal Commission show. It\nwas the ﬁfth such vote on statehood. ""Today, we the people of Puerto Rico are sending a strong and\nclear message to the US Congress ... and to the world ... claiming our equal rights as American citizens,\nPuerto Rico Gov. Ricardo Rossello said in a news release. @highlight Puerto Rico voted Sunday in\nfavor of US statehood\nQuery For one, they can truthfully say, “Don’t blame me, I didn’t vote for them, ” when discussing the\n<placeholder> presidency Correct Entities: USRTEText: Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44,\naccording to the Christopher Reeve Foundation.\nHypothesis: Christopher Reeve had an accident. Entailment: FalseWiCContext 1: Room and board .Context 2: He nailed boards across the windows.\nSense match: FalseWSCText: Mark told Pete many lies about himself, which Pete included in his book. Heshould have been\nmore truthful. Coreference: False\n3 SuperGLUE Overview\n3.1 Design Process\nThe goal of SuperGLUE is to provide a simple, robust evaluation metric of any method capable of\nbeing applied to a broad range of language understanding tasks. To that end, in designing SuperGLUE,\nwe identify the following desiderata of tasks in the benchmark:\nTask substance: Tasks should test a system’s ability to understand and reason about texts in English.\nTask difﬁculty: Tasks should be beyond the scope of current state-of-the-art systems, but solvable by\nmost college-educated English speakers. We exclude tasks that require domain-speciﬁc knowledge,\ne.g. medical notes or scientiﬁc papers.\nEvaluability: Tasks must have an automatic performance metric that corresponds well to human'
 'SuperGLUE: A Stickier Benchmark for\nGeneral-Purpose Language Understanding Systems\nAlex Wang∗\nNew York UniversityYada Pruksachatkun∗\nNew York UniversityNikita Nangia∗\nNew York University\nAmanpreet Singh∗\nFacebook AI ResearchJulian Michael\nUniversity of WashingtonFelix Hill\nDeepMindOmer Levy\nFacebook AI Research\nSamuel R. Bowman\nNew York University\nAbstract\nIn the last year, new models and methods for pretraining and transfer learning have\ndriven striking performance improvements across a range of language understand-\ning tasks. The GLUE benchmark, introduced a little over one year ago, offers\na single-number metric that summarizes progress on a diverse set of such tasks,\nbut performance on the benchmark has recently surpassed the level of non-expert\nhumans, suggesting limited headroom for further research. In this paper we present\nSuperGLUE, a new benchmark styled after GLUE with a new set of more difﬁ-\ncult language understanding tasks, a software toolkit, and a public leaderboard.\nSuperGLUE is available at super.gluebenchmark.com .\n1 Introduction\nRecently there has been notable progress across many natural language processing (NLP) tasks, led\nby methods such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT\n(Devlin et al., 2019). The unifying theme of these methods is that they couple self-supervised learning\nfrom massive unlabelled text corpora with effective adapting of the resulting model to target tasks.\nThe tasks that have proven amenable to this general approach include question answering, textual\nentailment, and parsing, among many others (Devlin et al., 2019; Kitaev et al., 2019, i.a.).\nIn this context, the GLUE benchmark (Wang et al., 2019a) has become a prominent evaluation\nframework for research towards general-purpose language understanding technologies. GLUE is\na collection of nine language understanding tasks built on existing public datasets, together with\nprivate test data, an evaluation server, a single-number target metric, and an accompanying expert-\nconstructed diagnostic set. GLUE was designed to provide a general-purpose evaluation of language\nunderstanding that covers a range of training data volumes, task genres, and task formulations. We\nbelieve it was these aspects that made GLUE particularly appropriate for exhibiting the transfer-\nlearning potential of approaches like OpenAI GPT and BERT.\nThe progress of the last twelve months has eroded headroom on the GLUE benchmark dramatically.\nWhile some tasks (Figure 1) and some linguistic phenomena (Figure 2 in Appendix B) measured\nin GLUE remain difﬁcult, the current state of the art GLUE Score as of early July 2019 (88.4 from\nYang et al., 2019) surpasses human performance (87.1 from Nangia and Bowman, 2019) by 1.3\npoints, and in fact exceeds this human performance estimate on four tasks. Consequently, while there\n∗Equal contribution. Correspondence: glue-benchmark-admin@googlegroups.com\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.arXiv:1905.00537v3  [cs.CL]  13 Feb 2020']","Tasks were selected based on difficulty, evaluability, substance, public data availability, simple formats, and licensing conditions, ensuring a challenging, diverse, and accessible benchmark for advancing general-purpose language understanding technologies.",0.999999999975,1.0,0.9520446504750725,1.0,0.8638938298226175,0.9171137808289318
"What are the main components of GLM-130B's pre-training objective, and how do they contribute to its performance?","The main components of GLM-130B's pre-training objective include the GLM objective and the MIP (Multi-Input Processing) technique. The GLM objective significantly enhances performance, contributing to a 73% improvement in LAMBADA and 90% in MMLU benchmarks, while MIP primarily aids tasks related to text similarity and coreference. However, MIP can also negatively impact performance on certain datasets, highlighting its selective effectiveness depending on the task type.","['ence. Altogether, it contributes to the high quality of GLM-130B in terms of both language perfor-\nmance on 112 tasks and ethical results on bias and toxicity benchmarks. Our experiences of both\nsuccess and failure are condensed into the lessons for training 100B-scale LLMs, attached in the\nAppendix B.10.\n9'
 'Published as a conference paper at ICLR 2023\nE C ONTRIBUTIONS\nThe GLM-130B project was conceived in Dec. 2021 with its pre-training part completed in July 3rd,\n2022 and its evaluation and applications still ongoing. Over the course, we have experienced various\ntechnical and engineering challenges (Cf. Appendix F and Figure 21 for details). It would not be\npossible to reach its current status if without the collaboration of multiple teams—the Knowledge\nEngineering Group (KEG), Parallel Architecture & Compiler technology of Mobile, Accelerated,\nand Networked systems Group (PACMAN), and Natural Language Processing Group (THUNLP) at\nTsinghua University, as well as Zhipu.AI. The detailed contributions are listed below.\nE.1 P REPARATION\n•Model Implementation: Aohan Zeng, Zhengxiao Du\n•Self-Supervised Data Processing: Ming Ding, Wendi Zheng\n•Multitask Data Processing: Xiao Liu, Xiao Xia\n•Model Architecture: Aohan Zeng, Xiao Liu, Zhengxiao Du, Hanyu Lai\n•Training Stability: Aohan Zeng, Xiao Liu, Ming Ding\n•3D-Parallelism and Training Efficiency: Aohan Zeng, Zixuan Ma, Jiaao He, Zhenbo Sun\nE.2 M ODEL TRAINING\n•Large-Scale Training & Monitoring: Aohan Zeng, Xiao Liu\n•Model Performance Validation: Aohan Zeng\nE.3 P OST TRAINING\n•Evaluation Framework: Aohan Zeng, Zhengxiao Du\n•Language Modeling Evaluation: Aohan Zeng\n•MMLU & BIG-Bench Evaluation: Aohan Zeng\n•CLUE & FewCLUE Evaluation: Xiao Liu, Aohan Zeng\n•Ethical Evaluation: Yifan Xu, Aohan Zeng, Xiao Liu, Zihan Wang\n•Baseline Evaluation: Xiao Liu, Jifan Yu, Weng Lam Tam\n•INT4 Quantization: Aohan Zeng, Zihan Wang, Xiao Liu, Hanyu Lai\n•Inference Acceleration: Zihan Wang, Aohan Zeng\n•Low-Resource Inference: Gouyang Zeng, Xu Han, Weilin Zhao, Zhiyuan Liu\n•Demo and API: Hanyu Lai, Jifan Yu, Xiaohan Zhang, Yufei Xue, Shan Wang, Jiecai Shan, Hao-\nhan Jiang, Zhengang Guo\n•Manuscript Writing: Xiao Liu, Yuxiao Dong, and Jie Tang wrote the main paper, and Xiao Liu,\nAohan Zeng, and Zhengxiao Du wrote the Appendix.\nE.4 P ROJECT MANAGEMENT\n•Student Leaders: Aohan Zeng, Xiao Liu\n•Technical Advisors: Yuxiao Dong, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Jie\nTang\n•Project Leader: Jie Tang\nE.5 C OMPUTATION SPONSOR\n•GPU Sponsor: Zhipu.AI\n52'
 'Published as a conference paper at ICLR 2023\nexact dataset setting in T0 (Sanh et al., 2022) and the information extraction datasets in GLM-130B\nto allow the correct evaluation on some types of tasks (e.g., NLI).\nFigure 14 shows the ablation results. On the 8 datasets we test, we find that the GLM objective\nis a major contributor to the improvement (from GLM (uni) to GLM + MIP (bi)). For exam-\nple, it accounts for 73% improvement in LAMBADA and 90% improvement in MMLU, which\nare very widely adopted challenging benchmarks for LLMs. As for MIP, on some datasets (e.g.,\nWiC, ReCoRD, Hellaswag), MIP may even harm the performance. While for datasets related to text\nsimilarity and coreference (e.g., WSC, BoolQ, ANLI R1), MIP is the main contributor. It is likely\nbecause the text similarity and coreference challenges, which people usually construct intentionally\nto test language models’ ability, are seldom seen in the self-supervised corpus that makes up peo-\nple’s daily written texts. Thus, MIP training mainly helps to bridge the gap between self-supervised\npre-training and these tasks.\nB.10 L ESSONS LEARNED\nLesson 1 (Bidirectional Architecture). The bidirectional-attention GLM is a strong architec-\nture alternative, in addition to GPTs.\nLesson 2 (Platform-aware Configuration). Configure LLMs based on the cluster and parallel\nstrategy used to squeeze hardware potential.\nLesson 3 (Improved Post-LN). Counter-stereotypically, DeepNorm, a type of Post-LN, is the\noption to stabilize GLM-130B.\nLesson 4 (Training Stability Categorization). Unexpected training instability that LLMs\nsuffer from arouses systematically and numerically.\nLesson 5 (Systematical Instability: FP16). Though FP16 induces more instability, it enables\ntraining and inference on diverse platforms.\nLesson 6 (Numerical Instability: Embedding Gradient Shrink). Shrinking embedding\nlayer’s gradient to its 0.1 can solve most numerical instability problems.\nLesson 7 (GLM’s INT4 Quantization Scaling Law). GLM has a unique INT4 weight quan-\ntization scaling law unobserved in GPT-style BLOOM.\nLesson 8 (Future Direction). To create powerful LLMs, the main focus can be on 1) more and\nbetter data, 2) better architectures and pre-training objectives, and 3) more sufficient training.\n30'
 'Published as a conference paper at ICLR 2023\nF A B RIEF HISTORY OF GLM-130B\nThe GLM-130B project16was conceived in Dec. 2021 in a brainstorming meeting at Tsinghua KEG.\nWe firmly believe that it is of value to pre-train a highly accurate language model, in particular for\nboth Chinese and English. Though GPT-3 (Brown et al., 2020) is the pioneer for this effort, it is not\navailable to most people in the world. In addition, it supports English only. We therefore decide to\ninitialize the project GLM-130B. Please note that the WuDao 1.75T model we built last year is a\nsparse model with 480 mixture-of-experts (MoE), rather than a dense one as GPT-3. Our goal then\nis to train a bilingual pre-trained dense model with high accuracy on downstream tasks, and to make\nit open to everyone in the world-anyone, anywhere can download it and use it on a single server with\nappropriate GPUs.\nThe ambitious project soon faced several important challenges:\n•Lack of computational resources : No organization is willing to sponsor such a big project and\nfreely make it public.\n•Lack of a robust pre-training algorithm : Despite GPT-3’s success on English corpus, it is\nunclear how to train a high-accurate bilingual model for both English and Chinese.\n•Lack of fast inference solutions : Since the goal is to have the model public to everyone, we need\nto design fast inference solutions with low resource requirements to run the model.\nFor the pre-training algorithm, we finally chose GLM (Du et al., 2022) due to its high performance\nin practice. We eventually decided to train a GLM model of 130 billion parameters after several\nrounds of discussions and exploration, because such a size makes it possible to run the inference on\na single A100 (40G * 8) server.\nOur first attempt at training the model was in January 2022, shortly after we received a small sponsor\nof GPUs for test running. However, we soon realized that we had significantly underestimated the\ntechnical difficulties of pre-training a model at such a scale (>100B). It seems that pre-training a\nhighly accurate 100B-scale model is quite different from training a 10B-scale one. Due to frequent\nrandom hardware failures, model gradients exploding, unexpected excessive memory usage in the\nalgorithm, debug for the 3D pipeline in the new Megatron and DeepSpeed frameworks, inability to\nrecover from optimizer states, blocked TCP responses between processes, and many many unex-\npected “bugs”, the project was delayed for many times. The Tsinghua PACMAN team gave us a\nhand at this difficult time and together we successfully fixed most of the “bugs”.\nBy March, we were still short on computational resources, but fortunately got a chance to try test\nruns on several other platforms, including Ascend 910, Hygon DCU, NVIDIA, and Sunway. The\nimmediate challenge was for us to adapt our training code to these different platforms, as the under-\nlying operators are quite different. Also, it introduced many new issues: the element-wise operators\nnot supporting fast computation for large-dimension vectors, various issues that hindered conver-\ngence—the large gradient norms of input embeddings, native Post-LN, Pre-LN, and Sandwich-LN,\ndataloader state seeds, and computation precision choices in Softmax and Attention — as well as\nnumerous mistakes we ourselves made. With tremendous help from all of our generous partners, we\nfinally succeeded in making our pre-training algorithms runnable across all the platforms—frankly,\na surprising achievement for this project. The timeline of GLM-130B in Figure 21 covers most of\nthe issues we have encountered and addressed as of this writing.\nOn April 26th, we received a generous computing sponsorship from Zhipu.AI — an AI startup that\naims to teach machines to think like humans. After another week of testing, we finally kicked off\nthe training of the GLM-130B model on its 96 A100 (40G * 8) servers on May 6th. Additionally,']","GLM-130B's pre-training includes self-supervised blank infilling for 95% of tokens and multi-task instruction pre-training (MIP) for 5% of tokens, enhancing downstream zero-shot performance.",0.4166666666458333,1.0,0.9721397767361708,0.0,0.43713659538730154,0.891403524406349
How does GLM-130B address ethical concerns and biases compared to its counterparts?,"GLM-130B demonstrates significantly less bias and generation toxicity compared to other 100B-scale models like GPT-3 and OPT-175B. It includes a model license that requires users to agree not to use it for harmful purposes and evaluates its performance on bias and toxicity benchmarks. Additionally, being a bilingual model, GLM-130B may reconcile social biases better due to its exposure to both English and Chinese corpora.","['ence. Altogether, it contributes to the high quality of GLM-130B in terms of both language perfor-\nmance on 112 tasks and ethical results on bias and toxicity benchmarks. Our experiences of both\nsuccess and failure are condensed into the lessons for training 100B-scale LLMs, attached in the\nAppendix B.10.\n9'
 'Published as a conference paper at ICLR 2023\nG.3 Social Impact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\nH Environmental Impact 56\nA E THICS : EVALUATION ON BIASES AND TOXICITY\nAlbeit LLMs’ strong abilities in language and beyond, which could bring substantial welfare to\nhuman beings, they can potentially produce toxic and illegal contents for evil use (Weidinger et al.,\n2021; Sheng et al., 2021; Dev et al., 2021; Bommasani et al., 2021). In GLM-130B, before granting\nmodel weight to applicants, in the model license we demand them to agree that they will not use it\nfor any deeds that may be harmful to society and human beings.\nAdditionally, from a technical perspective, we argue that we must also understand LLMs’ toxic\nand biased behaviors and ultimately eliminate them. This aligns with our commitment to “LLM\nInclusivity”, as it is necessary to include more people in the open-sourced LLM research to facilitate\nthe process. Moreover, if an LLM is shown to be good at identifying toxic and biased content,\ntechniques such as self-diagnoses (Schick et al., 2021) can help to reduce the harmful generation\nin a self-consistent post-processing procedure. Therefore, as an initial step, we evaluate GLM-\n130B over a variety of related benchmarks to shed light on the challenging topic. Despite their\nlimitations (Blodgett et al., 2021; Jacobs & Wallach, 2021) which should be addressed in future\nwork, they still serve as a good start to arouse the community’s awareness of the problem.\nA.1 B IASMEASUREMENT : CROW S-P AIRSTable 5: CrowS-Pairs (Nangia et al., 2020) Bias\nMeasurement. The lower scores the better.\nCategory GPT-3 OPT-175B GLM-130B\nGender 62.6 65.7 55.7\nReligion 73.3 68.6 73.3\nRace/Color 64.7 68.6 58.5\nSexual orientation 76.2 78.6 60.7\nAge 64.4 67.8 63.2\nNationality 61.6 62.9 64.1\nDisability 76.7 76.7 71.6\nPhysical appearance 74.6 76.2 74.6\nSocioeconomic status 73.8 76.2 70.9\nOverall 67.2 69.5 65.8CrowS-Pairs (Nangia et al., 2020), or namely\nCrowdsourced Stereotype Pairs benchmark, is\nwidely used for measuring biases for masked\nlanguage models. It collects 1508 examples with\nnine different conventional biases and adopts a\nprobing-based approach to compare the pseudo-\nlog-likelihood of a pair of stereotypical and anti-\nstereotypical sentences. Since GLM-130B is\npre-trained with autoregressive blanking infill-\ning, CrowS-Pairs evaluation is directly appli-\ncable. We compare the GPT-3 Davinci and\nOPT-175B’s results on CrowS-Pairs reported\nin (Zhang et al., 2022) with GLM-130B.\nOur results are presented in Table 5. GLM-130B shows fewer biases on almost all kinds of stereo-\ntypes except for religion and nationality. We speculate that it is because GLM-130B is a bilingual\npre-trained LLM that learns the semantics for certain content from both English and Chinese cor-\npora. Since CrowsS-Pairs’ stereotypes mainly draw from the US Equal Employment Opportunities\nCommission’s list2, the bias distributions in two different cultures and languages may be different\nand consequently reconcile social biases in GLM-130B on a benchmark originally designed for\nEnglish-language society. We think this is an interesting finding, as multi-lingual pre-training may\nhelp LLMs to present less harmful biases for better fairness. Finally, we also admit that GLM-\n130B may in turn presents some special Chinese biases which currently lack testing benchmarks\nand require considerable future efforts to detect and prevent.\nA.2 B IASMEASUREMENT : STEREO SET\nAnother widely used bias and stereotype evaluation benchmark is StereoSet (Nadeem et al., 2021),\nwhich is also adopted in (Lieber et al., 2021; Artetxe et al., 2021; Zhang et al., 2022). To balance\nthe evaluation between bias detecting and language modeling quality, StereoSet reports a series of\nmetrics including Language Modeling Scores (LMS), Stereotype Score (SS), and Idealized Context\nAssociation Test Score (ICAT) as an overall averaged metric. For example, given the premise “ She'
 'Published as a conference paper at ICLR 2023\nPart I\nAppendix\nTable of Contents\nA Ethics: Evaluation on Biases and Toxicity 21\nA.1 Bias Measurement: CrowS-Pairs . . . . . . . . . . . . . . . . . . . . . . . . . 21\nA.2 Bias Measurement: StereoSet . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nA.3 Hate Speech Detection: ETHOS . . . . . . . . . . . . . . . . . . . . . . . . . . 22\nA.4 Toxic Genearation: RealToxicPrompts . . . . . . . . . . . . . . . . . . . . . . 22\nB Technical Details 23\nB.1 Tokenization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nB.2 Layer Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\nB.3 Positional Encoding and Feed-forward Network . . . . . . . . . . . . . . . . . 24\nB.4 Pipeline Parallel Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\nB.5 Inference Acceleration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\nB.6 Activation Outlier Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\nB.7 Weight Quantization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\nB.8 Quantization settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\nB.9 Ablation on Contribution Attribution . . . . . . . . . . . . . . . . . . . . . . . 29\nB.10 Lessons Learned . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\nC Dataset and Evaluation Details 32\nC.1 Multi-task Instruction Pre-training (MIP) . . . . . . . . . . . . . . . . . . . . . 32\nC.2 Data and prompts in MIP for DeepStruct . . . . . . . . . . . . . . . . . . . . . 32\nC.3 Result Sources for GPT-3, BLOOM-176B, and OPT-175B . . . . . . . . . . . . 39\nC.4 Pile Test-set Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\nC.5 BIG-bench-lite Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\nC.6 MMLU Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\nC.7 Chinese Language Understanding Evaluation . . . . . . . . . . . . . . . . . . . 40\nC.8 Natural Language Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\nC.9 Winograd-Style Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\nC.10 Closed-book Question Answering . . . . . . . . . . . . . . . . . . . . . . . . . 43\nC.11 Commonsense Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\nC.12 Fixed Label Datasets: A Case Study in Natural Language Inference . . . . . . . 44\nC.13 SuperGLUE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\nC.14 Chain-of-Thought Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\nD Scaling and Emergent Abilities in GLM-130B 46\nE Contributions 52\nE.1 Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\nE.2 Model Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\nE.3 Post Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\nE.4 Project Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\nE.5 Computation Sponsor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\nF A Brief History of GLM-130B 53\nG Broader Impact 55\nG.1 Impact on AI Research . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\nG.2 Impact on Individual Developers and Small Companies . . . . . . . . . . . . . . 55\n20'
 'Published as a conference paper at ICLR 2023\nGPT-3 175BPaLM 540BBLOOM 176BOPT-175BGLM-130BLanguage Ability EvaluationBias & Toxicity Evaluation\nFigure 1: A summary of the performance evaluation and ethical studies.\nTable 1: A comparison between GLM-130B and other 100B-scale LLMs and PaLM 540B. (LN:\nlayer norm.; FPF: floating-point format; MIP: multi-task instruction pre-training; CN : Chinese)\nArchitecture & Data Training Inference\nModelOpen-\nsource Objective LN Major Lang. FPF Stabilization Quantization GPU Needed\nGPT-3 175B × English FP16 undisclosed undisclosed undisclosed\nOPT-175B ✓ English FP16 Manual Adjusting INT8 8 ×3090\nBLOOM-176B ✓GPT Pre-LN\nMulti-lingual BF16 Embedding Norm INT8 8 ×3090\nPaLM 540B × GPT Pre-LN English BF16 Manual Adjusting undisclosed undisclosed\nGLM-130B ✓GLM (Blank\nInfilling & MIP)Deep-\nNormBilingual\n(EN & CN)FP16Embedding\nGradient ShrinkINT44×3090 or\n8×1080 Ti\nIn this work, we introduce the pre-training of a 100B-scale model—GLM-130B, in terms of engi-\nneering efforts, model design choices, training strategies for efficiency and stability, and quantization\nfor affordable inference. As it has been widely realized that it is computationally unaffordable to\nempirically enumerate all possible designs for training 100B-scale LLMs, we present not only the\nsuccessful part for training GLM-130B but also many of the failed options and lessons learned.\nParticularly, the training stability is the decisive factor in the success of training models of such a\nscale. Different from practices such as manually adjusting learning rates in OPT-175B and using\nembedding norm in the sacrifice of performance in BLOOM-176B, we experiment with various op-\ntions and find the strategy of embedding gradient shrink can significantly stabilize the training of\nGLM-130B.\nSpecifically, GLM-130B is a bilingual (English and Chinese) bidirectional dense model with 130 bil-\nlion parameters, pre-trained over 400 billion tokens on a cluster of 96 NVIDIA DGX-A100 (8 ×40G)\nGPU nodes between May 6 and July 3, 2022. Instead of using the GPT-style architecture, we adopt\nthe General Language Model (GLM) algorithm (Du et al., 2022) to leverage its bidirectional at-\ntention advantage and autoregressive blank infilling objective. Table 1 summarizes the comparison\nbetween GLM-130B, GPT-3 and another two open-source efforts—OPT-175B and BLOOM-176B,\nas well as PaLM 540B (Chowdhery et al., 2022)—a 4 ×larger model—as a reference.\nAltogether, the conceptual uniqueness and engineering efforts enable GLM-130B to exhibit perfor-\nmance that surpasses the level of GPT-3 on a wide range of benchmarks (in total 112 tasks) and also\noutperforms PaLM 540B in many cases, while outperformance over GPT-3 has not been observed in\nOPT-175B and BLOOM-176B (Cf. Figure 1 left). For zero-shot performance, GLM-130B is better\nthan GPT-3 175B (+5.0%), OPT-175B (+6.5%), and BLOOM-176B (+13.0%) on LAMBADA (Pa-\nperno et al., 2016), and achieves 3 ×better performance than GPT-3 on Big-bench-lite (Srivastava\net al., 2022). For the 5-shot MMLU (Hendrycks et al., 2021) tasks, it is better than GPT-3 175B\n(+0.9%) and BLOOM-176B (+12.7%). As a bilingual LLM also in Chinese, it offers significantly\nbetter results than ERNIE TITAN 3.0 260B (Wang et al., 2021)—the largest Chinese LLM—on 7\nzero-shot CLUE (Xu et al., 2020) datasets (+24.26%) and 5 zero-shot FewCLUE (Xu et al., 2021)\nones (+12.75%). Importantly, as summarized in Figure 1 right, GLM-130B as an open model is\nassociated with significantly less bias and generation toxicity than its 100B-scale counterparts .\nFinally, we design GLM-130B to empower as many people as possible to conduct 100B-scale LLM\nstudies. First, instead of using 175B+ parameters as OPT and BLOOM, the 130B size is decided be-\ncause such a size supports inference on a single A100 (8 ×40G) server. Second, to further lower the\nGPU requirements, we quantize GLM-130B into INT4 precision without post training while OPT']","Through comprehensive ethical evaluations, GLM-130B exhibits significantly less bias and toxicity generation than similar-scale models, highlighting its consideration for ethical AI development.",0.999999999975,1.0,0.886179721334838,1.0,0.7311270078189771,0.9245080312759086
How does Megatron-LM's implementation ensure training stability for extremely large transformer models?,"Megatron-LM ensures training stability for extremely large transformer models by implementing a simple and efficient intra-layer model parallel approach that does not require extensive code modifications or new compilers. It employs careful placement of layer normalization and leverages model parallelism to split the model across multiple GPUs, which alleviates memory constraints. Additionally, it incorporates synchronization primitives to facilitate communication between GPUs while maintaining performance efficiency.","['Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nContext : The 36th International Conference on Machine\nLearning (ICML 2019) will be held in Long Beach, CA,\nUSA from June 10th to June 15th, 2019. The conference\nwill consist of one day of tutorials (June 10), followed\nby three days of main conference sessions (June 11-13),\nfollowed by two days of workshops (June 14-15).\nMegatron-LM : With a broad scope, the conference ad-\ndresses the challenges and opportunities in machine learning\nfor practitioners and researchers. The conference program\nwill feature sessions that cover different machine learning\ntopics, from reinforcement learning to computational biol-\nogy and from AI to deep learning. It is the world’s premier\nforum for exploring and presenting advances in machine\nlearning.\nICML 2019 will offer an impressive range of sessions,\nwhich will be of interest to practitioners, researchers, and\nstudents. Some of the topics we will cover include:\nReinforcement learning\nSoft Computing\nComputational biology\nConvolutional networks\nDeep learning\nStatistical learning theory\nStatistical learning machine\nNatural language processing\nBiology\nAwards\nSince its ﬁrst edition, the conference has grown and evolved.\nToday, it is the premier forum for showcasing state-of-the-\nart research in machine learning. The conference is widely\nrecognized as the preeminent venue for showcasing the\nlatest research in machine learning, and a place where prac-\ntitioners, researchers, and students can exchange ideas and\ninsights.\nThe conference is a peer-reviewed venue that draws its pres-\ntige and inﬂuence from the excellence of the presentations\nand the quality of the invited speakers. With over 50% of all\ntalks selected based on their content by an expert committee,\nit is clear that the conference’s high standards of research\nand quality keep a very high bar.Context : Rescue workers on Tuesday found a ﬂight\nrecorder from the Russian plane that crashed into the Black\nSea over the weekend, the defense ministry said. All 92\npeople aboard the Russian military’s Tu-154 plane are be-\nlieved to have died Sunday morning when it crashed two\nminutes after taking off from the southern Russian city of\nSochi.\nMegatron-LM : A child holds up a portrait of a dead relative\nat a makeshift morgue in a building belonging to a local\ngovernment in the town of Petropavlovsk-Kamchatsky. A\nRussian Defense Ministry Tu-154 plane with 92 people\non board crashed into the Black Sea on Sunday morning\nshortly after taking off from the southern Russian city of\nSochi. (Yuri Kochetkov/EPA)\nAccording to the ministry, the plane was found at a depth\nof more than 1,000 feet and the recorder was damaged.\n”The defense ministry has decided to hand over the recorder\nto the Interstate Aviation Committee, which is the main\ninvestigative body in the air crash,” the statement said.\nThe search for the plane’s black boxes is being hampered by\nstrong winds and rough seas. Investigators have launched a\ncriminal probe into the crash, which ofﬁcials said was likely\ncaused by a technical fault.\nRussian President Vladimir Putin declared Wednesday a\nday of mourning for the victims.\nD. Further Scaling Analysis\nIn this section we study the effect of number of attention\nheads on the scaling results. We also present strong scaling\nresults for our 1.2 billion parameter model.\nD.1. Attention Heads and Scaling\nThis section studies the effect of attention heads on model\nparallel scaling. To this end, we consider the 8.3 billion\nparameter conﬁguration with 8-way model parallelism and\nvary the number of heads from 16 to 32. The results are\npresented in Table 7. As the number of attention heads\nincreases, some of the GEMMS inside the self-attention\nlayer become smaller and also the number of elements in\nthe self attention softmax increases. This results in a slight\ndecrease in scaling efﬁciency. Future research should be\nwary of this hyperparameter to design large transformer'
 'Megatron-LM: Training Multi-Billion Parameter Language Models Using\nModel Parallelism\nMohammad Shoeybi1 2Mostofa Patwary1 2Raul Puri1 2Patrick LeGresley2Jared Casper2\nBryan Catanzaro2\nAbstract\nRecent work in language modeling demonstrates\nthat training large transformer models advances\nthe state of the art in Natural Language Processing\napplications. However, very large models can be\nquite difﬁcult to train due to memory constraints.\nIn this work, we present our techniques for train-\ning very large transformer models and implement\na simple, efﬁcient intra-layer model parallel ap-\nproach that enables training transformer models\nwith billions of parameters. Our approach does\nnot require a new compiler or library changes, is\northogonal and complimentary to pipeline model\nparallelism, and can be fully implemented with\nthe insertion of a few communication operations\nin native PyTorch. We illustrate this approach\nby converging transformer based models up to\n8.3 billion parameters using 512 GPUs. We sus-\ntain 15.1 PetaFLOPs across the entire applica-\ntion with 76% scaling efﬁciency when compared\nto a strong single GPU baseline that sustains 39\nTeraFLOPs, which is 30% of peak FLOPs. To\ndemonstrate that large language models can fur-\nther advance the state of the art (SOTA), we train\nan 8.3 billion parameter transformer language\nmodel similar to GPT-2 and a 3.9 billion parame-\nter model similar to BERT. We show that careful\nattention to the placement of layer normalization\nin BERT-like models is critical to achieving in-\ncreased performance as the model size grows. Us-\ning the GPT-2 model we achieve SOTA results\non the WikiText103 (10.8 compared to SOTA per-\nplexity of 15.8) and LAMBADA (66.5% com-\npared to SOTA accuracy of 63.2%) datasets. Our\nBERT model achieves SOTA results on the RACE\ndataset (90.9% compared to SOTA accuracy of\n89.4%).\n1Equal contribution2NVIDIA. Correspondence to: Mohammad\nShoeybi <mshoeybi@nvidia.com >.1. Introduction\nNatural Language Processing (NLP) is advancing quickly in\npart due to an increase in available compute and dataset size.\nThe abundance of compute and data enables training increas-\ningly larger language models via unsupervised pretraining\n(Devlin et al., 2018; Radford et al., 2019). Empirical evi-\ndence indicates that larger language models are dramatically\nmore useful for NLP tasks such as article completion, ques-\ntion answering, and natural language inference (Lan et al.,\n2019; Raffel et al., 2019). By ﬁnetuning these pretrained\nlanguage models on downstream natural language tasks,\none can achieve state of the art results as shown in recent\nwork (Devlin et al., 2018; Peters et al., 2018; Howard &\nRuder, 2018; Radford et al., 2018; 2017; Ramachandran\net al., 2016; Liu et al., 2019b; Dai et al., 2019; Yang et al.,\n2019; Liu et al., 2019a; Lan et al., 2019).\nAs these models become larger, they exceed the memory\nlimit of modern processors, and require additional memory\nmanagement techniques such as activation checkpointing\n(Chen et al., 2016). Widely used optimization algorithms\nsuch as ADAM require additional memory per parameter to\nstore momentum and other optimizer state, which reduces\nthe size of models that can be effectively trained. Several\napproaches to model parallelism overcome this limit by\npartitioning the model such that the weights and their asso-\nciated optimizer state do not need to reside concurrently on\nthe processor. For example, GPipe (Huang et al., 2018) and\nMesh-Tensorﬂow (Shazeer et al., 2018) provide frameworks\nfor model parallelism of different kinds. However, they\nrequire rewriting the model, and rely on custom compilers\nand frameworks that are still under development.\nIn this work, we implement a simple and efﬁcient model\nparallel approach using intra-layer model-parallelism. We\nexploit the inherent structure in transformer based language\nmodels to make a simple model-parallel implementation that\ntrains efﬁciently in PyTorch, with no custom C++ code or'
 'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nrequire any new compiler or code re-writing, and can be\nfully implemented by inserting a few simple primitives, as\ndescribed in the next section.\n3. Model Parallel Transformers\nWe take advantage of the structure of transformer networks\nto create a simple model parallel implementation by adding a\nfew synchronization primitives. A transformer layer consists\nof a self attention block followed by a two-layer, multi-layer\nperceptron (MLP) as shown in Figure 2. We introduce\nmodel parallelism in both of these blocks separately.\nWe start by detailing the MLP block. The ﬁrst part of the\nblock is a GEMM followed by a GeLU nonlinearity:\nY=GeLU (XA) (1)\nOne option to parallelize the GEMM is to split the weight\nmatrixAalong its rows and input Xalong its columns as:\nX= [X1,X2], A=[A1\nA2]\n. (2)\nThis partitioning will result in Y=GeLU (X1A1+\nX2A2). Since GeLU is a nonlinear function, GeLU (X1A1+\nX2A2)̸=GeLU (X1A1)+GeLU (X2A2)and this approach\nwill require a synchronization point before the GeLU func-\ntion.\nAnother option is to split Aalong its columns A= [A1,A2].\nThis partitioning allows the GeLU nonlinearity to be inde-\npendently applied to the output of each partitioned GEMM:\n[Y1,Y2] = [ GeLU (XA 1),GeLU (XA 2)] (3)\nThis is advantageous as it removes a synchronization point.\nHence, we partition the ﬁrst GEMM in this column parallel\nfashion and split the second GEMM along its rows so it takes\nthe output of the GeLU layer directly without requiring any\ncommunication as shown in Figure 3a. The output of the\nsecond GEMM is then reduced across the GPUs before\npassing the output to the dropout layer. This approach splits\nboth GEMMs in the MLP block across GPUs and requires\nonly a single all-reduce operation in the forward pass ( g\noperator) and a single all-reduce in the backward pass ( f\noperator). These two operators are conjugates of each other\nand can be implemented in PyTorch with only a few lines of\ncode. As an example, the implementation of the foperator\nis provided below:\nclass f(torch.autograd.Function):\ndef forward(ctx, x):\nreturn x\ndef backward(ctx, gradient):\nall_reduce(gradient)\nreturn gradient\nCode 1. Implementation of foperator. gis similar to fwith\nidentity in the backward and all-reduce in the forward\nfunctions.\n(a) MLP\n(b) Self-Attention\nFigure 3. Blocks of Transformer with Model Parallelism. fandg\nare conjugate. fis an identity operator in the forward pass and all\nreduce in the backward pass while gis an all reduce in the forward\npass and identity in the backward pass.\nAs shown in Figure 3b, for the self attention block we exploit\ninherent parallelism in the multihead attention operation,\npartitioning the GEMMs associated with key ( K), query\n(Q), and value ( V) in a column parallel fashion such that\nthe matrix multiply corresponding to each attention head is\ndone locally on one GPU. This allows us to split per atten-\ntion head parameters and workload across the GPUs, and\ndoesnt require any immediate communication to complete\nthe self-attention. The subsequent GEMM from the output\nlinear layer (after self attention) is parallelized along its\nrows and takes the output of the parallel attention layer di-\nrectly, without requiring communication between the GPUs.\nThis approach for both the MLP and self attention layer\nfuses groups of two GEMMs, eliminates a synchronization\npoint in between, and results in better scaling. This enables\nus to perform all GEMMs in a simple transformer layer\nusing only two all-reduces in the forward path and two in\nthe backward path (see Figure 4).\nThe transformer language model has an output embedding\nwith the dimension of hidden-size ( H) times vocabulary-\nsize (v). Since the vocabulary size is on the order of tens\nof thousands of tokens for modern language models (for\nexample, GPT-2 used a vocabulary size of 50,257), it is ben-\neﬁcial to parallelize the output embedding GEMM. How-'
 'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nFigure 2. Transformer Architecture. Purple blocks correspond to\nfully connected layers. Each blue block represents a single trans-\nformer layer that is replicated N times.\nand compute efﬁciency. The original transformer formula-\ntion was designed as a machine translation architecture that\ntransforms an input sequence into another output sequence\nusing two parts, an Encoder andDecoder . However, recent\nwork leveraging transformers for language modeling such as\nBERT (Devlin et al., 2018) and GPT-2 (Radford et al., 2019)\nuse only the Encoder orDecoder depending on their needs.\nThis work explores both a decoder architecture, GPT-2, and\nan encoder architecture, BERT.\nFigure 2 shows a schematic diagram of the model we used.\nWe refer the reader to prior work for a detailed descrip-\ntion of the model architecture (Vaswani et al., 2017; Devlin\net al., 2018; Radford et al., 2019). It is worthwhile to men-\ntion that both GPT-2 and BERT use GeLU (Hendrycks &\nGimpel, 2016) nonlinearities and layer normalization (Ba\net al., 2016) to the input of the multi-head attention and feed\nforward layers, whereas the original transformer (Vaswani\net al., 2017) uses ReLU nonlinearities and applies layer\nnormalization to outputs.\n2.3. Data and Model Parallelism in Deep Learning\nThere are two central paradigms for scaling out deep neu-\nral network training to numerous hardware accelerators:\ndata parallelism (Valiant, 1990) where a training minibatch\nis split across multiple workers, and model parallelism in\nwhich the memory usage and computation of a model is\ndistributed across multiple workers. By increasing the mini-\nbatch size proportionally to the number of available work-\ners (i.e. weak scaling ), one observes near linear scaling\nin training data throughput. However, large batch train-\ning introduces complications into the optimization process\nthat can result in reduced accuracy or longer time to conver-\ngence, offsetting the beneﬁt of increased training throughput\n(Keskar et al., 2017). Further research (Goyal et al., 2017;\nYou et al., 2017; 2019) has developed techniques to miti-gate these effects and drive down the training time of large\nneural networks. To scale out training even further, parallel\nwork (Chen et al., 2016) has combined data parallelism with\nactivation checkpointing: recomputing activations in the\nbackward pass without storing them in the forward pass to\nreduce memory requirements.\nHowever, these techniques have one fundamental limitation\nin the problem size they can tackle: the model must ﬁt\nentirely on one worker. With language models of increasing\nsize and complexity like BERT and GPT-2, neural networks\nhave approached the memory capacity of modern hardware\naccelerators. One solution to this problem is to employ\nparameter sharing to reduce the memory footprint of the\nmodel (Lan et al., 2019), but this limits the overall capacity\nof the model. Our approach is to utilize model parallelism\nto split the model across multiple accelerators. This not\nonly alleviates the memory pressure, but also increases the\namount of parallelism independently of the microbatch size.\nWithin model parallelism, there are two further paradigms:\nlayer-wise pipeline parallelism, and more general distributed\ntensor computation. In pipeline model parallelism, groups\nof operations are performed on one device before the outputs\nare passed to the next device in the pipeline where a differ-\nent group of operations are performed. Some approaches\n(Harlap et al., 2018; Chen et al., 2018) use a parameter\nserver (Li et al., 2014) in conjunction with pipeline par-\nallelism. However these suffer from inconsistency issues.\nThe GPipe framework for TensorFlow (Huang et al., 2018)\novercomes this inconsistency issue by using synchronous\ngradient decent. This approach requires additional logic to\nhandle the efﬁcient pipelining of these communication and']","Through techniques like embedding gradient shrink (EGS) and DeepNorm layer normalization, Megatron-LM stabilizes training of models with billions of parameters.",0.499999999975,0.7777777777777778,0.9881860531140032,1.0,0.8921252313561646,0.901834258757992
How does PAL's performance on the GSM8K benchmark compare to other advanced models?,"PAL outperforms strong baselines such as COT, Minerva, and others on the GSM8K benchmark, achieving a solve rate of 72.0%. In contrast, COT achieves a solve rate of 65.6%, while other models show lower performance rates. PAL's effectiveness is consistent across various language models, including both code-based and natural language models, particularly when the models are sufficiently strong.","['PAL: Program-aided Language Models 23\nH.1. Creating GSM-HARD\nWhile replacing numbers in the question is easy using pattern matching, a more challenging aspect is recalculating the correct\nanswer. GSM 8Kevaluation set contains 1319 samples, which is prohibitively expensive to perform manual re-calculation.\nInstead, we leverage PALto assist obtaining the correct answers. For 71% of the examples where PALis correct on\nGSM 8K, we utilize the generated program and replace the initial value with the larger values. For example, if we create\na harder version of the problem in Figure 3 by replacing $23 dollars with $15687 dollars, we correspondingly replace\nmoneyinitial=23 tomoneyinitial=15678 . Running the program could automatically produce the correct\nanswer of the harder question. Notably, this annotation process assumes that a program that produces a correct answer to\naGSM 8Kquestion indicates the correctness of the program itself. While this is not guaranteed due to possible spurious\ncorrelations, we manually checked 25 programs and found all of them are correct. For the incorrect 29% of the cases, we\nrunPALagain and perform nucleus sampling (Holtzman et al., 2019) with temperature 0.7, and repeat the above process if\nany correct solution is found. Finally, the authors manually annotate the correct answer for 50remaining cases that PALwas\nnot able to solve after 100 iterations.\nH.2. GSM-HARD Analysis\nTable 11 shows thoughts generated with COTonGSM 8Kand GSM -HARD . A manual analysis reveals that a majority of the\ngenerated thoughts (16/25) were identical for GSM 8Kand GSM -HARD , indicating that larger numbers primarily diminish\nperformance due to failure of LLM to do arithmetic..'
 'PAL: Program-aided Language Models 16\n1 8 15 405060708085\nNumber of sampled generations for each questionSolve Rate (%)\nPAL\nCOT\nMinerva\nPaLM\nFigure 12: Comparison of solve rates between PALand baselines as the number of samples increases from 1 to 40. Note that\nthe solve rates for the baselines ( PaLM,COT, Minerva) are obtained through logistic interpolation of solve rates at 1 and 40'
 'PAL: Program-aided Language Models 14\nthat even with a prompt that matches the numbers, there are only modest gains in performance. These results show that the\ngains achieved by using code-based reasoning chains may not be achieved simply by using better few-shot examples for\nCOT.\nRegular Prompt Prompt with Larger Numbers\nCOT 23.3 23.8\nTable 5: GSM-hard results, when the prompts also had examples of larger numbers.\nSuccinct Code The programs used in few-shot examples by PALare multi-step, and show a step-by-step breakdown of\nthe reasoning process. Is this breakdown necessary? Alternatively, can we return a single line expression (see Figure 11b) to\ncalculate the result? Results in Table 6 ( 4throw) shows that is not the case. With single-line expressions, the performance of\nPAL falls to the level of direct prompting.\nGenerating the answer directly PALﬁrst generates a reasoning chain in the form of a Python program, and passes the\ngenerated program to a runtime to obtain an answer. Is PALbetter only because of the program-style intermediate reasoning\nchains, or are the improvements derived from ofﬂoading execution to the Python runtime? To investigate this, we experiment\nwith a variant that forces the LLM to generate the answer after generating the reasoning chain (Figure 11e). This setting\ncompels the LLM to condition on the generated code-based reasoning to generate an answer, simulating the runtime. The\nresults in Table 6 ( 5throw) show that the solve rate drops to near DIRECT levels. This reinforces our hypothesis that while\ncurrent LLMs can be excellent at specifying a high-level plan to solve a task—they are still incapable of executing them.\nAblation Solve Rate\nDIRECT (no intermediate reasoning) 19.7\nCOT 65.6\nPAL 72.0\nSuccinct Code 47.8\nLLM Simulating Runtime 23.2\nTable 6: Solve Rates for Ablations\nC. Effect of Using Language Models of Code\nIn our experiments, we focused on evaluating the performance of a language model for code. We aimed to investigate\nwhether the additional performance boost observed in our results was due to the use of models like Codex, or whether our\nformulation was useful even for text-based models. To this end, we conducted additional experiments using text-based\nlanguage models. Our ﬁndings indicate that the PAL approach is not restricted to working solely with Codex, but can also\nbe applied to natural language (NL) models, as long as the model is sufﬁciently strong. Speciﬁcally, our results showed that\nin the text-davinci-001 model, the use of the CoT approach resulted in better performance.\nModel CoT PaL\ntext-davinci-001 26.5 8.6\ntext-davinci-002 46.9 65.8\ntext-davinci-003 65.3 69.8\nD. Analyzing the Effect of Increasing Number of Samples on P AL\nIn Section 5.1, we show that PALoutperforms strong baselines both for a single sample and by drawing 40 samples and\nusing majority voting. Figure 12 illustrates the trends for cases when the number of samples drawn are between 1 and 40,\nand the interpolation estimates demonstrate that P AL remains competitive throughout the number of samples.'
 'PAL: Program-aided Language Models 7\ncode-cushman-001 code-davinci-001 code-davinci-002020406080\n21.731.872.0\n19.126.060.1\n13.6%22.3%19.8%Solve ratePAL\nCOT\nRelative Improvement\nFigure 7: PALwith different models on GSM8K: though\nthe absolute accuracies with code-cushman-001\nandcode-davinci-001 are lower than\ncode-davinci-002 , the relative improvement of\nPAL over C OT is consistent across models.text-davinci-001 text-davinci-002 text-davinci-003020406080\n26.546.965.3\n8.665.869.8 COT PAL\nFigure 8: PALwith NL LMs on GSM 8K: though\nCOToutperforms PALwithtext-davinci-001 , once\nthe base LM is sufﬁciently strong, PALis beneﬁcial\nwithtext-davinci-002 andtext-davinci-003\nas well. That is, P AL is not limited to code-LMs only.\n6. Analysis\nDoes P AL work with weaker LMs? In all our experi-\nments in Section 5, PALused thecode-davinci-002\nmodel; but can PALwork with weaker models of code? We\ncompared PALwith COTwhen both prompting approaches\nuse the same weaker base LMs code-cushman-001\nandcode-davinci-001 . As shown in Figure 7, even\nthough the absolute accuracies of code-cushman-001\nandcode-davinci-001 are lower, the relative improve-\nment of PALover COTremains consistent across models.\nThis shows that PALcan work with weaker models, while\nits beneﬁt scales elegantly to stronger models as well.\nDoes P AL work with LMs of natural language? We\nalso experimented with PALusing thetext-davinci\nseries. Figure 8 shows the following interesting re-\nsults: when the base LM’s “code modeling ability” is\nweak (using text-davinci-001 ),COTperforms better\nthan PAL. However, once the LM’s code modeling abil-\nity is sufﬁciently high (using text-davinci-002 and\ntext-davinci-003 ),PALoutperforms COT, and PAL\ntext-davinci-003 performs almost as PALcode-davinci-002 .\nThis shows that PALis not limited to LMs of code, but it\ncan work with LMs that were mainly trained for natural\nlanguage, if they have a sufﬁciently high coding ability.\nIs P AL better because of the Python prompt or because\nof the interpreter? We experimented with generating\nPython code, while requiring the neural LM to “execute” it\nas well, without using an interpreter, following Nye et al.\n(2021); Madaan et al. (2022). We created prompts that are\nsimilar to PAL’s, except that they do include the ﬁnal answer.\nThis resulted in a 23.2 solve rate on GSM 8K, much lower\nthan PAL(72.0), and only 4.5 points higher than DIRECT .\nThese results reinforce our hypothesis that the main beneﬁt\nofPALcomes from the synergy with the interpreter, andnot only from having a better prompt. Additional details\nare provided in Appendix B. For additional discussion on\nthe advantages of code-prompts over textual-prompts, see\nAppendix G.\nDo variable names matter? In all our experiments, we\nused meaningful variable names in the PALprompts, to ease\nthe model’s grounding of variables to the entities they rep-\nresent. For the Python interpreter, however, variable names\nare meaningless. To measure the importance of meaningful\nvariable names, we experimented with two prompts variants:\n1. P AL−comment – the PALprompt without intermediate\nNL comments.\n2. P AL−var\n−comment – the PALprompt without intermediate\nNL comments and with variable names substituted\nwith random characters.\nThe results are shown in Figure 9. In COLORED OBJECTED\nandDATE, removing intermediate NL comments but keep-\ning meaningful variable names ( PAL−comment ) – slightly re-\nduces the results compared to the full PALprompt, but it still\nachieves higher accuracy than the baselines COT. Remov-\ning variable names as well ( PAL−var\n−comment ) further decreases\naccuracy, and performs worse than COT. Since variable\nnames have an important part in code quality (Gellenbeck\n& Cook, 1991; Takang et al., 1996), meaningful variable\nnames are only expected to ease reasoning for Codex, which\nwas trained on mostly meaningful names, as was also found\nby Madaan et al. (2022).\n7. Related Work\nPrompting Few-shot prompting (Brown et al., 2020) has']","PAL achieves state-of-the-art few-shot accuracy on the GSM8K benchmark, surpassing PaLM-540B by a significant 15% top-1 accuracy improvement, demonstrating its effectiveness in handling complex arithmetic and symbolic reasoning tasks.",0.999999999975,0.8571428571428571,0.9824701031108803,0.0,0.6249019746646763,0.8996078986587054
Can PAL's approach be generalized to models trained primarily on natural language rather than code?,"Yes, PAL can be generalized to models trained primarily on natural language, as long as those models have a sufficiently high coding ability. Experiments showed that while PAL performed worse with weaker models like text-davinci-001, it outperformed the chain-of-thought (COT) method with stronger models like text-davinci-002 and text-davinci-003. This indicates that PAL is not limited to code language models but can also work effectively with natural language models.","['PAL: Program-aided Language Models 9\n8. Conclusion\nWe introduce PAL, a new method for natural language rea-\nsoning, using programs as intermediate reasoning steps.\nDifferently from existing LM-based reasoning approaches,\nthe main idea is to ofﬂoad solving and calculating to an\nexternal Python interpreter, instead of using the LLM for\nboth understanding the problem andsolving. This results\nin a ﬁnal answer that is guaranteed to be accurate, given the\ncorrectly predicted programmatic steps. We demonstrate\nthis seamless synergy between an LLM and a Python in-\nterpreter across 13 tasks from BIG-Bench Hard and other\nbenchmarks. In all these benchmarks, PALoutperforms\nlarger LLMs such as PaLM-540 Bwhich use the popular\n“chain-of-thought” method and sets new state-of-the-art ac-\ncuracy on all of them. We believe that these results unlock\nexciting directions for future neuro-symbolic AI reasoners.\nReferences\nAhn, M., Brohan, A., Brown, N., Chebotar, Y ., Cortes, O.,\nDavid, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman,\nK., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B.,\nIrpan, A., Jang, E., Ruano, R. J., Jeffrey, K., Jesmonth,\nS., Joshi, N. J., Julian, R., Kalashnikov, D., Kuang, Y .,\nLee, K.-H., Levine, S., Lu, Y ., Luu, L., Parada, C., Pastor,\nP., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D.,\nSermanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke,\nV ., Xia, F., Xiao, T., Xu, P., Xu, S., Yan, M., and Zeng,\nA. Do as I Can, not as I Say: Grounding Language in\nRobotic Affordances. arXiv preprint arXiv:2204.01691 ,\n2022.\nAmini, A., Gabriel, S., Lin, S., Koncel-Kedziorski, R., Choi,\nY ., and Hajishirzi, H. MathQA: Towards Interpretable\nMath Word Problem Solving with Operation-Based For-\nmalisms. In ACL, 2019.\nAndor, D., He, L., Lee, K., and Pitler, E. Giving bert a cal-\nculator: Finding operations and arguments with reading\ncomprehension. arXiv preprint arXiv:1909.00109 , 2019.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,\nHenighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,\nJ., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,\nGray, S., Chess, B., Clark, J., Berner, C., McCandlish,\nS., Radford, A., Sutskever, I., and Amodei, D. Language\nModels are Few-Shot Learners. In NeurIPS , 2020.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y ., Joseph, N., Brockman,\nG., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf,\nH., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N.,\nPavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter,C., Tillet, P., Such, F. P., Cummings, D., Plappert, M.,\nChantzis, F., Barnes, E., Herbert-V oss, A., Guss, W. H.,\nNichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin,\nI., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr,\nA. N., Leike, J., Achiam, J., Misra, V ., Morikawa, E.,\nRadford, A., Knight, M., Brundage, M., Murati, M.,\nMayer, K., Welinder, P., McGrew, B., Amodei, D., Mc-\nCandlish, S., Sutskever, I., and Zaremba, W. Evaluating\nLarge Language Models Trained on Code. arXiv preprint\narXiv:2107.03374 , 2021a.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y ., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374 , 2021b.\nChen, W., Ma, X., Wang, X., and Cohen, W. W. Program\nof thoughts prompting: Disentangling computation from\nreasoning for numerical reasoning tasks. arXiv preprint\narXiv:2211.12588 , 2022.\nCheng, Z., Xie, T., Shi, P., Li, C., Nadkarni, R., Hu, Y .,\nXiong, C., Radev, D., Ostendorf, M., Zettlemoyer, L.,\nSmith, N. A., and Yu, T. Binding language models in\nsymbolic languages. arXiv preprint arXiv:2210.02875 ,\n2022.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton,\nC., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,'
 'model ( PAL): a novel method that uses an LLM to read\nnatural language problems and generate programs as rea-\nsoning steps, but ofﬂoads the solution step to a Python inter-\npreter, as illustrated in Figure 1. This ofﬂoading leverages an\nLLM that can decompose a natural language problem into\nprogrammatic steps, which is fortunately available using\ncontemporary state-of-the-art LLMs that are pre-trained on\nboth natural language and programming languages (Brown\net al., 2020; Chen et al., 2021a; Chowdhery et al., 2022).\nWhile natural language understanding and decompositionarXiv:2211.10435v2  [cs.CL]  27 Jan 2023'
 'PAL: Program-aided Language Models 14\nthat even with a prompt that matches the numbers, there are only modest gains in performance. These results show that the\ngains achieved by using code-based reasoning chains may not be achieved simply by using better few-shot examples for\nCOT.\nRegular Prompt Prompt with Larger Numbers\nCOT 23.3 23.8\nTable 5: GSM-hard results, when the prompts also had examples of larger numbers.\nSuccinct Code The programs used in few-shot examples by PALare multi-step, and show a step-by-step breakdown of\nthe reasoning process. Is this breakdown necessary? Alternatively, can we return a single line expression (see Figure 11b) to\ncalculate the result? Results in Table 6 ( 4throw) shows that is not the case. With single-line expressions, the performance of\nPAL falls to the level of direct prompting.\nGenerating the answer directly PALﬁrst generates a reasoning chain in the form of a Python program, and passes the\ngenerated program to a runtime to obtain an answer. Is PALbetter only because of the program-style intermediate reasoning\nchains, or are the improvements derived from ofﬂoading execution to the Python runtime? To investigate this, we experiment\nwith a variant that forces the LLM to generate the answer after generating the reasoning chain (Figure 11e). This setting\ncompels the LLM to condition on the generated code-based reasoning to generate an answer, simulating the runtime. The\nresults in Table 6 ( 5throw) show that the solve rate drops to near DIRECT levels. This reinforces our hypothesis that while\ncurrent LLMs can be excellent at specifying a high-level plan to solve a task—they are still incapable of executing them.\nAblation Solve Rate\nDIRECT (no intermediate reasoning) 19.7\nCOT 65.6\nPAL 72.0\nSuccinct Code 47.8\nLLM Simulating Runtime 23.2\nTable 6: Solve Rates for Ablations\nC. Effect of Using Language Models of Code\nIn our experiments, we focused on evaluating the performance of a language model for code. We aimed to investigate\nwhether the additional performance boost observed in our results was due to the use of models like Codex, or whether our\nformulation was useful even for text-based models. To this end, we conducted additional experiments using text-based\nlanguage models. Our ﬁndings indicate that the PAL approach is not restricted to working solely with Codex, but can also\nbe applied to natural language (NL) models, as long as the model is sufﬁciently strong. Speciﬁcally, our results showed that\nin the text-davinci-001 model, the use of the CoT approach resulted in better performance.\nModel CoT PaL\ntext-davinci-001 26.5 8.6\ntext-davinci-002 46.9 65.8\ntext-davinci-003 65.3 69.8\nD. Analyzing the Effect of Increasing Number of Samples on P AL\nIn Section 5.1, we show that PALoutperforms strong baselines both for a single sample and by drawing 40 samples and\nusing majority voting. Figure 12 illustrates the trends for cases when the number of samples drawn are between 1 and 40,\nand the interpolation estimates demonstrate that P AL remains competitive throughout the number of samples.'
 'PAL: Program-aided Language Models 7\ncode-cushman-001 code-davinci-001 code-davinci-002020406080\n21.731.872.0\n19.126.060.1\n13.6%22.3%19.8%Solve ratePAL\nCOT\nRelative Improvement\nFigure 7: PALwith different models on GSM8K: though\nthe absolute accuracies with code-cushman-001\nandcode-davinci-001 are lower than\ncode-davinci-002 , the relative improvement of\nPAL over C OT is consistent across models.text-davinci-001 text-davinci-002 text-davinci-003020406080\n26.546.965.3\n8.665.869.8 COT PAL\nFigure 8: PALwith NL LMs on GSM 8K: though\nCOToutperforms PALwithtext-davinci-001 , once\nthe base LM is sufﬁciently strong, PALis beneﬁcial\nwithtext-davinci-002 andtext-davinci-003\nas well. That is, P AL is not limited to code-LMs only.\n6. Analysis\nDoes P AL work with weaker LMs? In all our experi-\nments in Section 5, PALused thecode-davinci-002\nmodel; but can PALwork with weaker models of code? We\ncompared PALwith COTwhen both prompting approaches\nuse the same weaker base LMs code-cushman-001\nandcode-davinci-001 . As shown in Figure 7, even\nthough the absolute accuracies of code-cushman-001\nandcode-davinci-001 are lower, the relative improve-\nment of PALover COTremains consistent across models.\nThis shows that PALcan work with weaker models, while\nits beneﬁt scales elegantly to stronger models as well.\nDoes P AL work with LMs of natural language? We\nalso experimented with PALusing thetext-davinci\nseries. Figure 8 shows the following interesting re-\nsults: when the base LM’s “code modeling ability” is\nweak (using text-davinci-001 ),COTperforms better\nthan PAL. However, once the LM’s code modeling abil-\nity is sufﬁciently high (using text-davinci-002 and\ntext-davinci-003 ),PALoutperforms COT, and PAL\ntext-davinci-003 performs almost as PALcode-davinci-002 .\nThis shows that PALis not limited to LMs of code, but it\ncan work with LMs that were mainly trained for natural\nlanguage, if they have a sufﬁciently high coding ability.\nIs P AL better because of the Python prompt or because\nof the interpreter? We experimented with generating\nPython code, while requiring the neural LM to “execute” it\nas well, without using an interpreter, following Nye et al.\n(2021); Madaan et al. (2022). We created prompts that are\nsimilar to PAL’s, except that they do include the ﬁnal answer.\nThis resulted in a 23.2 solve rate on GSM 8K, much lower\nthan PAL(72.0), and only 4.5 points higher than DIRECT .\nThese results reinforce our hypothesis that the main beneﬁt\nofPALcomes from the synergy with the interpreter, andnot only from having a better prompt. Additional details\nare provided in Appendix B. For additional discussion on\nthe advantages of code-prompts over textual-prompts, see\nAppendix G.\nDo variable names matter? In all our experiments, we\nused meaningful variable names in the PALprompts, to ease\nthe model’s grounding of variables to the entities they rep-\nresent. For the Python interpreter, however, variable names\nare meaningless. To measure the importance of meaningful\nvariable names, we experimented with two prompts variants:\n1. P AL−comment – the PALprompt without intermediate\nNL comments.\n2. P AL−var\n−comment – the PALprompt without intermediate\nNL comments and with variable names substituted\nwith random characters.\nThe results are shown in Figure 9. In COLORED OBJECTED\nandDATE, removing intermediate NL comments but keep-\ning meaningful variable names ( PAL−comment ) – slightly re-\nduces the results compared to the full PALprompt, but it still\nachieves higher accuracy than the baselines COT. Remov-\ning variable names as well ( PAL−var\n−comment ) further decreases\naccuracy, and performs worse than COT. Since variable\nnames have an important part in code quality (Gellenbeck\n& Cook, 1991; Takang et al., 1996), meaningful variable\nnames are only expected to ease reasoning for Codex, which\nwas trained on mostly meaningful names, as was also found\nby Madaan et al. (2022).\n7. Related Work\nPrompting Few-shot prompting (Brown et al., 2020) has']","PAL is effective with models trained on code, but experiments indicate it can also enhance models primarily trained on natural language, provided they possess sufficient coding ability, demonstrating PAL's versatility.",0.999999999975,1.0,0.9813164654043917,0.6666666666666666,0.6121479831584133,0.9485919326336534
What contributions does CodeNet make towards the creation of AI models capable of understanding and generating code?,"CodeNet provides a large-scale, diverse, and high-quality dataset with over 14 million code samples in 55 programming languages, which is essential for training AI models. It includes extensive metadata and annotations that help benchmark various coding tasks, such as code similarity, classification, and translation. Additionally, CodeNet offers sample inputs and outputs for most code samples, facilitating the evaluation of code correctness and guiding reinforcement learning for code quality improvements.","['Given the success of non-AI tools for code, why should we consider AI to augment or possibly\nreplace them? Firstly, AI can help reﬁne and re-tune the heuristics used by traditional coding tools.\nSecondly, based on the training data from past experience, AI can help prioritize when there is more\nthan one sound answer [ 5]. Thirdly, an AI-based tool may handle incomplete or invalid code more\nrobustly, thus expanding its scope. Finally, AI can incorporate signals usually ignored by traditional\ntools for code, such as the natural language in identiﬁers or comments.\nIn the enterprise environment, developers often face code written by large teams over many years\nand geographies. Developers must manipulate such code to modernize it, ﬁx bugs, improve its\nperformance, evolve it when requirements change, make it more secure, and/or comply with regu-\nlations. These tasks are challenging, and it is crucial to provide tool support for developers to be\nmore productive at performing them. It is well known that the latest advancements in deep learning\nalgorithms rely on best-of-breed datasets, such as ImageNet, to create increasingly complex and\npowerful models. In this paper, we present ""CodeNet"", a ﬁrst-of-its-kind dataset in scale, diversity,\nand quality, to accelerate the algorithmic advances in AI for Code.\nTo promote widespread adoption of CodeNet, we will be launching contests involving use cases based\non the dataset. The ﬁrst contest [ 6] will focus on diversity, inclusion and spurring interest among\naspiring data scientists. We are partnering with the Global Women in Data Science organization (with\npresence in over 50 countries) founded by Stanford University [ 7] and targeting teams with at least\nﬁfty percent women. We are planning follow-up contests that target experienced AI practitioners.\nThe rest of the paper is organized as follows. Section 2 introduces the CodeNet dataset. Related\ndatasets are discussed in Section 3, and the differentiation of CodeNet with respect to these related\ndatasets is elaborated in Section 4. Section 5 describes how CodeNet was curated and Section 6\nenumerates the usability features of CodeNet with several pre-processing tools to transform source\ncodes into representations that can be readily used as inputs into machine learning models. Section 7\ndiscusses the upcoming CodeNet contest and Section 8 describes important baseline experiments\nwith the CodeNet dataset. Section 9 presents further uses of the CodeNet dataset and Section 10\nconcludes the paper.\n2 The CodeNet Dataset\nThe CodeNet dataset consists of a large collection of code samples with extensive metadata. It\nalso contains documented tools to transform code samples into intermediate representations and to\naccess the dataset and make tailored selections. Our goal is to provide the community with a large,\nhigh-quality curated dataset that can be used to advance AI techniques for source code.\nCodeNet is derived from the data available on two online judge websites: AIZU [ 8] and AtCoder [ 9].\nOnline judge websites pose programming problems in the form of courses and contests. The dataset\nconsists of submissions to these problems, which are judged by an automated review process for\ncorrectness. Problem descriptions, submission outcomes, and associated metadata are available via\nvarious REST APIs.\nScale and Statistics. CodeNet contains a total of 13,916,868 submissions, divided into 4053\nproblems. Among the submissions, 53.6% (7,460,588) are accepted (compilable and pass the\nprescribed tests), 29.5% are marked with wrong answer, and the remaining rejected due to their\nfailure to meet run time or memory requirements. To our knowledge, this is the largest dataset so\nfar among similar kinds. Submissions are in 55 different languages; 95% of them are coded in C++,\nPython, Java, C, Ruby, and C#. C++ is the most common language, with 8,008,527 submissions (57%'
 'CodeNet: A Large-Scale AI for Code Dataset for\nLearning a Diversity of Coding Tasks\nRuchir Puri1, David S. Kung1, Geert Janssen1, Wei Zhang1,\nGiacomo Domeniconi1,Vladimir Zolotov1,Julian Dolby1,Jie Chen2,1,\nMihir Choudhury1,Lindsey Decker1,Veronika Thost2,1,Luca Buratti1,\nSaurabh Pujar1,Shyam Ramji1,Ulrich Finkler1,Susan Malaika3,Frederick Reiss1\n1IBM Research\n2MIT-IBM Watson AI Lab\n3IBM Worldwide Ecosystems\nAbstract\nOver the last several decades, software has been woven into the fabric of every\naspect of our society. As software development surges and code infrastructure of\nenterprise applications ages, it is now more critical than ever to increase software\ndevelopment productivity and modernize legacy applications. Advances in deep\nlearning and machine learning algorithms have enabled breakthroughs in computer\nvision, speech recognition, natural language processing and beyond, motivating\nresearchers to leverage AI techniques to improve software development efﬁciency.\nThus, the fast-emerging research area of “AI for Code” has garnered new interest\nand gathered momentum. In this paper, we present a large-scale dataset CodeNet ,\nconsisting of over 14 million code samples and about 500 million lines of code\nin 55 different programming languages, which is aimed at teaching AI to code.\nIn addition to its large scale, CodeNet has a rich set of high-quality annotations\nto benchmark and help accelerate research in AI techniques for a variety of crit-\nical coding tasks, including code similarity and classiﬁcation, code translation\nbetween a large variety of programming languages, and code performance (runtime\nand memory) improvement techniques. Additionally, CodeNet provides sample\ninput and output test sets for 98.5% of the code samples, which can be used as\nan oracle for determining code correctness and potentially guide reinforcement\nlearning for code quality improvements. As a usability feature, we provide several\npre-processing tools in CodeNet to transform source code into representations that\ncan be readily used as inputs into machine learning models. Results of code classi-\nﬁcation and code similarity experiments using the CodeNet dataset are provided as\na reference. We hope that the scale, diversity and rich, high-quality annotations of\nCodeNet will offer unprecedented research opportunities at the intersection of AI\nand Software Engineering.\n1 Introduction\nThere is a growing trend towards leveraging AI for building tools that support software engineering\nand development [ 1,2]. AI can manipulate and generate computer code, but can it do so with\nhigh quality? Many researchers are fascinated by this possibility, encouraged by AI successes in\nother domains and tantalized by the vision of computers programming computers. Some recent\ndeep-learning models [ 3,4] for code have received a lot of publicity: trained on vast amounts of\ndata and using novel architectures with billions of parameters, they sometimes generate surprisingly\nplausible code.\nPreprint. Under review.arXiv:2105.12655v2  [cs.SE]  29 Aug 2021'
 '9 Further Uses of CodeNet\nThe rich metadata and language diversity open CodeNet to a plethora of use cases. The problem-\nsubmission relationship in CodeNet corresponds to type-4 similarity [ 43] and can be used for code\nsearch and clone detection. The code samples in CodeNet are labeled with their acceptance status\nso we can readily extract pairs of buggy and ﬁxed code for code repair [ 49,50]. A large number\nof code samples come with inputs so that we can execute the code to extract the CPU run time and\nmemory footprint, which can be used for regression studies and prediction.\nCodeNet may also be used for program translation, given its wealth of programs written in a multitude\nof languages. Translation between two programming languages is born out of a practical need to port\nlegacy codebases to modern languages in order to increase accessibility and lower maintenance costs.\nWith the help of neural networks, machine translation models developed for natural languages [ 51]\nwere adapted to programming languages, producing pivotal success [ 4]. One considerable challenge of\nneural machine translation is that model training depends on large, parallel corpora that are expensive\nto curate [ 52], especially for low-resource languages (e.g., legacy code). Recently, monolingual\napproaches [ 53,4] were developed to mitigate the reliance on parallel data, paving ways to build\nmodels for languages with little translation. Compared with current popular data sets (e.g., [ 4,54]),\nCodeNet covers a much richer set of languages with ample training instances.\n10 Conclusion\nArtiﬁcial intelligence has made great strides in understanding human language. Computer scientists\nhave been fascinated by the possibility and tantalized by the vision of computers (AI) programming\ncomputers. In this paper, we presented ""CodeNet"", a ﬁrst-of-its-kind very large-scale, diverse and\nhigh-quality dataset to accelerate the algorithmic advances in AI for Code. This dataset is not\nonly unique in its scale, but also in the diversity of coding tasks it can help benchmark: from code\nsimilarity and classiﬁcation for advances in code recommendation algorithms, and code translation\nbetween a large variety of programming languages, to advances in code performance improvement\ntechniques. We hope that the scale, diversity and rich, high-quality annotations of CodeNet will offer\nunprecedented research opportunities at the intersection of AI and Software Engineering.\n11 Acknowledgements\nWe would like to acknowledge AIZU and AtCoder for making the code submissions publicly available.\nWe would like to thank the IBM Data Asset eXchange team for providing a platform to host the\nCodeNet dataset. We would like to thank the Women in Data Science team at Stanford University\nand the IBM Call for Code team for their collaboration in launching the CodeNet challenge.\n12 Bibliography\n[1]Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. A survey of\nmachine learning for big code and naturalness. ACM Computing Surveys (CSUR) , 51(4):1–37,\n2018.\n[2]Yanming Yang, Xin Xia, David Lo, and John Grundy. A survey on deep learning for software\nengineering. arXiv preprint arXiv:2011.14597 , 2020.\n[3]Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul\nPuri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke\nChan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad\nBavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias\nPlappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-V oss, William Hebgen Guss, Alex\nNichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra,'
 ""[24] Miltiadis Allamanis. The adverse effects of code duplication in machine learning models of\ncode. In Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New\nParadigms, and Reﬂections on Programming and Software , Onward! 2019, page 143–153, New\nYork, NY , USA, 2019. Association for Computing Machinery.\n[25] Wikipedia. Jaccard index — Wikipedia, the free encyclopedia. https://en.wikipedia.\norg/wiki/Jaccard_index , 2020.\n[26] Terence Parr. The Deﬁnitive ANTLR 4 Reference . Pragmatic Bookshelf, 2nd edition, 2013.\n[27] Sifei Luan, Di Yang, Celeste Barnaby, Koushik Sen, and Satish Chandra. Aroma: code recom-\nmendation via structural code search. Proceedings of the ACM on Programming Languages ,\n3(OOPSLA):1–28, Oct 2019.\n[28] IBM T.J. Watson Research Center. Wala. https://github.com/wala/WALA , 2021.\n[29] Forbes on codenet. https://www.forbes.com/sites/moorinsights/2021/06/04/ibm-\ncodenet-artificial-intelligence-that-can-program-computers-and-solve-a-\n100-billion-legacy-code-problem/?sh=343813636cdc .\n[30] Venturebeat on codenet. https://venturebeat.com/2021/05/10/ibms-codenet-\ndataset-aims-to-train-ai-to-tackle-programming-challenges/ .\n[31] Zdnet on codenet. https://www.zdnet.com/article/ibm-launches-autosql-watson-\norchestrate-codenet-enterprise-ai-tools-at-think/ .\n[32] Project codenet repository. https://github.com/IBM/Project_CodeNet .\n[33] Luca Buratti, Saurabh Pujar, Mihaela Bornea, Scott McCarley, Yunhui Zheng, Gaetano Rossiello,\nAlessandro Morari, Jim Laredo, Veronika Thost, Yufan Zhuang, and Giacomo Domeniconi.\nExploring software naturalness through neural language models, 2020.\n[34] Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional\nnetworks. In ICLR , 2017.\n[35] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural\nnetworks? In ICLR , 2019.\n[36] Veronika Thost and Jie Chen. Directed acyclic graph neural networks. In ICLR , 2021.\n[37] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B.\nClement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou,\nMichele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng,\nShengyu Fu, and Shujie Liu. Codexglue: A machine learning benchmark dataset for code\nunderstanding and generation. CoRR , abs/2102.04664, 2021.\n[38] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks\nfor semi-supervised learning, 2018.\n[39] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele\nCatasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs.\narXiv preprint arXiv:2005.00687 , 2020.\n[40] Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric.\nInICLR Workshop on Representation Learning on Graphs and Manifolds , 2019.\n[41] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\nKopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-\nperformance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-\nBuc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32 ,\npages 8024–8035. Curran Associates, Inc., 2019.\n[42] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural\nnetworks?, 2019.\n[43] Hitesh Sajnani. Large-Scale Code Clone Detection . PhD thesis, University of California, Irvine,\n2016.\n[44] Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, and Pushmeet Kohli. Graph matching\nnetwork for learning the similarity of graph structured objects. In International Conference on\nMachine Learning (ICML) , 2019.\n21""]"," By providing a comprehensive dataset with diverse programming tasks and solutions, CodeNet accelerates the development of AI models for code generation and understanding.",0.999999999975,1.0,0.924231868818797,1.0,0.9051779973527507,0.9110345700561645
How does SuperGLUE facilitate the development and evaluation of models across its diverse set of tasks?,"SuperGLUE provides a robust evaluation metric for various language understanding tasks, allowing for straightforward comparison of model performance. It includes a diverse set of more challenging tasks compared to its predecessor, GLUE, which helps in pushing the boundaries of current state-of-the-art systems. Additionally, SuperGLUE offers a public leaderboard and a software toolkit to aid researchers in developing and testing their models effectively.","['Table 2: Development set examples from the tasks in SuperGLUE. Bold text represents part of the\nexample format for each task. Text in italics is part of the model input. Underlined text is specially\nmarked in the input. Text in a monospaced font represents the expected model output.BoolQPassage: Barq’s – Barq’s is an American soft drink. Its brand of root beer is notable for having caffeine.\nBarq’s, created by Edward Barq and bottled since the turn of the 20th century, is owned by the Barq\nfamily but bottled by the Coca-Cola Company. It was known as Barq’s Famous Olde Tyme Root Beer\nuntil 2012.\nQuestion: is barq’s root beer a pepsi product Answer: NoCBText: B: And yet, uh, I we-, I hope to see employer based, you know, helping out. You know, child, uh,\ncare centers at the place of employment and things like that, that will help out. A: Uh-huh. B: What do\nyou think, do you think we are, setting a trend?\nHypothesis: they are setting a trend Entailment: UnknownCOPAPremise: My body cast a shadow over the grass. Question: What’s the CAUSE for this?\nAlternative 1: The sun was rising. Alternative 2: The grass was cut.\nCorrect Alternative: 1MultiRCParagraph: Susan wanted to have a birthday party. She called all of her friends. She has ﬁve friends.\nHer mom said that Susan can invite them all to the party. Her ﬁrst friend could not go to the party\nbecause she was sick. Her second friend was going out of town. Her third friend was not so sure if her\nparents would let her. The fourth friend said maybe. The ﬁfth friend could go to the party for sure. Susan\nwas a little sad. On the day of the party, all ﬁve friends showed up. Each friend had a present for Susan.\nSusan was happy and sent each friend a thank you card the next week\nQuestion: Did Susan’s sick friend recover? Candidate answers: Yes, she recovered (T),No(F),Yes\n(T),No, she didn’t recover (F),Yes, she was at Susan’s party (T)ReCoRDParagraph: (CNN )Puerto Rico on Sunday overwhelmingly voted for statehood. But Congress, the only\nbody that can approve new states, will ultimately decide whether the status of the UScommonwealth\nchanges. Ninety-seven percent of the votes in the nonbinding referendum favored statehood, an increase\nover the results of a 2012 referendum, ofﬁcial results from the State Electorcal Commission show. It\nwas the ﬁfth such vote on statehood. ""Today, we the people of Puerto Rico are sending a strong and\nclear message to the US Congress ... and to the world ... claiming our equal rights as American citizens,\nPuerto Rico Gov. Ricardo Rossello said in a news release. @highlight Puerto Rico voted Sunday in\nfavor of US statehood\nQuery For one, they can truthfully say, “Don’t blame me, I didn’t vote for them, ” when discussing the\n<placeholder> presidency Correct Entities: USRTEText: Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44,\naccording to the Christopher Reeve Foundation.\nHypothesis: Christopher Reeve had an accident. Entailment: FalseWiCContext 1: Room and board .Context 2: He nailed boards across the windows.\nSense match: FalseWSCText: Mark told Pete many lies about himself, which Pete included in his book. Heshould have been\nmore truthful. Coreference: False\n3 SuperGLUE Overview\n3.1 Design Process\nThe goal of SuperGLUE is to provide a simple, robust evaluation metric of any method capable of\nbeing applied to a broad range of language understanding tasks. To that end, in designing SuperGLUE,\nwe identify the following desiderata of tasks in the benchmark:\nTask substance: Tasks should test a system’s ability to understand and reason about texts in English.\nTask difﬁculty: Tasks should be beyond the scope of current state-of-the-art systems, but solvable by\nmost college-educated English speakers. We exclude tasks that require domain-speciﬁc knowledge,\ne.g. medical notes or scientiﬁc papers.\nEvaluability: Tasks must have an automatic performance metric that corresponds well to human'
 'SuperGLUE: A Stickier Benchmark for\nGeneral-Purpose Language Understanding Systems\nAlex Wang∗\nNew York UniversityYada Pruksachatkun∗\nNew York UniversityNikita Nangia∗\nNew York University\nAmanpreet Singh∗\nFacebook AI ResearchJulian Michael\nUniversity of WashingtonFelix Hill\nDeepMindOmer Levy\nFacebook AI Research\nSamuel R. Bowman\nNew York University\nAbstract\nIn the last year, new models and methods for pretraining and transfer learning have\ndriven striking performance improvements across a range of language understand-\ning tasks. The GLUE benchmark, introduced a little over one year ago, offers\na single-number metric that summarizes progress on a diverse set of such tasks,\nbut performance on the benchmark has recently surpassed the level of non-expert\nhumans, suggesting limited headroom for further research. In this paper we present\nSuperGLUE, a new benchmark styled after GLUE with a new set of more difﬁ-\ncult language understanding tasks, a software toolkit, and a public leaderboard.\nSuperGLUE is available at super.gluebenchmark.com .\n1 Introduction\nRecently there has been notable progress across many natural language processing (NLP) tasks, led\nby methods such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT\n(Devlin et al., 2019). The unifying theme of these methods is that they couple self-supervised learning\nfrom massive unlabelled text corpora with effective adapting of the resulting model to target tasks.\nThe tasks that have proven amenable to this general approach include question answering, textual\nentailment, and parsing, among many others (Devlin et al., 2019; Kitaev et al., 2019, i.a.).\nIn this context, the GLUE benchmark (Wang et al., 2019a) has become a prominent evaluation\nframework for research towards general-purpose language understanding technologies. GLUE is\na collection of nine language understanding tasks built on existing public datasets, together with\nprivate test data, an evaluation server, a single-number target metric, and an accompanying expert-\nconstructed diagnostic set. GLUE was designed to provide a general-purpose evaluation of language\nunderstanding that covers a range of training data volumes, task genres, and task formulations. We\nbelieve it was these aspects that made GLUE particularly appropriate for exhibiting the transfer-\nlearning potential of approaches like OpenAI GPT and BERT.\nThe progress of the last twelve months has eroded headroom on the GLUE benchmark dramatically.\nWhile some tasks (Figure 1) and some linguistic phenomena (Figure 2 in Appendix B) measured\nin GLUE remain difﬁcult, the current state of the art GLUE Score as of early July 2019 (88.4 from\nYang et al., 2019) surpasses human performance (87.1 from Nangia and Bowman, 2019) by 1.3\npoints, and in fact exceeds this human performance estimate on four tasks. Consequently, while there\n∗Equal contribution. Correspondence: glue-benchmark-admin@googlegroups.com\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.arXiv:1905.00537v3  [cs.CL]  13 Feb 2020'
 'Table 1: The tasks included in SuperGLUE. WSD stands for word sense disambiguation, NLI is\nnatural language inference, coref. is coreference resolution, and QAis question answering. For\nMultiRC, we list the number of total answers for 456/83/166 train/dev/test questions.\nCorpus |Train | | Dev | | Test |Task Metrics Text Sources\nBoolQ 9427 3270 3245 QA acc. Google queries, Wikipedia\nCB 250 57 250 NLI acc./F1 various\nCOPA 400 100 500 QA acc. blogs, photography encyclopedia\nMultiRC 5100 953 1800 QA F1 a/EM various\nReCoRD 101k 10k 10k QA F1/EM news (CNN, Daily Mail)\nRTE 2500 278 300 NLI acc. news, Wikipedia\nWiC 6000 638 1400 WSD acc. WordNet, VerbNet, Wiktionary\nWSC 554 104 146 coref. acc. ﬁction books\n(Collobert and Weston, 2008; Dai and Le, 2015; Kiros et al., 2015; Hill et al., 2016; Conneau and\nKiela, 2018; McCann et al., 2017; Peters et al., 2018). GLUE was presented as a formal challenge\naffording straightforward comparison between such task-agnostic transfer learning techniques. Other\nsimilarly-motivated benchmarks include SentEval (Conneau and Kiela, 2018), which speciﬁcally\nevaluates ﬁxed-size sentence embeddings, and DecaNLP (McCann et al., 2018), which recasts a set\nof target tasks into a general question-answering format and prohibits task-speciﬁc parameters. In\ncontrast, GLUE provides a lightweight classiﬁcation API and no restrictions on model architecture or\nparameter sharing, which seems to have been well-suited to recent work in this area.\nSince its release, GLUE has been used as a testbed and showcase by the developers of several\ninﬂuential models, including GPT (Radford et al., 2018) and BERT (Devlin et al., 2019). As shown\nin Figure 1, progress on GLUE since its release has been striking. On GLUE, GPT and BERT\nachieved scores of 72.8 and 80.2 respectively, relative to 66.5 for an ELMo-based model (Peters\net al., 2018) and 63.7 for the strongest baseline with no multitask learning or pretraining above the\nword level. Recent models (Liu et al., 2019d; Yang et al., 2019) have clearly surpassed estimates of\nnon-expert human performance on GLUE (Nangia and Bowman, 2019). The success of these models\non GLUE has been driven by ever-increasing model capacity, compute power, and data quantity, as\nwell as innovations in model expressivity (from recurrent to bidirectional recurrent to multi-headed\ntransformer encoders) and degree of contextualization (from learning representation of words in\nisolation to using uni-directional contexts and ultimately to leveraging bidirectional contexts).\nIn parallel to work scaling up pretrained models, several studies have focused on complementary\nmethods for augmenting performance of pretrained models. Phang et al. (2018) show that BERT can\nbe improved using two-stage pretraining, i.e., ﬁne-tuning the pretrained model on an intermediate\ndata-rich supervised task before ﬁne-tuning it again on a data-poor target task. Liu et al. (2019d,c) and\nBach et al. (2018) get further improvements respectively via multi-task ﬁnetuning and using massive\namounts of weak supervision. Clark et al. (2019b) demonstrate that knowledge distillation (Hinton\net al., 2015; Furlanello et al., 2018) can lead to student networks that outperform their teachers.\nOverall, the quantity and quality of research contributions aimed at the challenges posed by GLUE\nunderline the utility of this style of benchmark for machine learning researchers looking to evaluate\nnew application-agnostic methods on language understanding.\nLimits to current approaches are also apparent via the GLUE suite. Performance on the GLUE\ndiagnostic entailment dataset, at 0.42 R3, falls far below the average human performance of 0.80\nR3reported in the original GLUE publication, with models performing near, or even below, chance\non some linguistic phenomena (Figure 2, Appendix B). While some initially difﬁcult categories\nsaw gains from advances on GLUE (e.g., double negation), others remain hard (restrictivity) or'
 'Table 4: Baseline performance on the SuperGLUE development.\nModel Avg BoolQ CB COPA MultiRC ReCoRD RTE WiC WSC\nMetrics Acc. Acc./F1 Acc. F1 a/EM F1/EM Acc. Acc. Acc.\nMost Frequent Class 47.7 62.2 50.0/22.2 55.0 59.9/ 0.8 32.4/31.5 52.7 50.0 63.5\nCBOW 47.7 62.4 71.4/49.6 63.0 20.3/ 0.3 14.4/13.8 54.2 55.3 61.5\nBERT 72.2 77.7 94.6/93.7 69.0 70.5/24.7 70.6/69.8 75.8 74.9 68.3\nBERT++ 74.6 80.1 96.4/95.0 78.0 70.5/24.7 70.6/69.8 82.3 74.9 68.3\nA Development Set Results\nIn Table 4, we present results of the baselines on the SuperGLUE tasks development sets.\nB Performance on GLUE Diagnostics\nFigure 2 shows the performance on the GLUE diagnostics dataset for systems submitted to the public\nleaderboard.\nDisjunction Downward Monotone Restrictivity Double Negation Prepositional Phrases60\n40\n20\n020406080\nChance\nBiLSTM+ELMo+Attn\nOpenAI GPT\nBERT + Single-task Adapters\nBERT (Large)\nBERT on STILTsBERT + BAM\nSemBERT\nSnorkel MeTaL\nALICE (Large)\nMT-DNN (ensemble)\nXLNet-Large (ensemble)\nFigure 2: Performance of GLUE submissions on selected diagnostic categories, reported using the\nR3metric scaled up by 100, as in Wang et al. (2019a, see paper for a description of the categories).\nSome initially difﬁcult categories, like double negation, saw gains from advances on GLUE, but\nothers remain hard (restrictivity) or even adversarial (disjunction, downward monotone).\nC Human Performance Estimation\nFor collecting data to establish human performance on the SuperGLUE tasks, we follow a two\nstep procedure where we ﬁrst provide some training to the crowd workers before they proceed to\nannotation. For both steps and all tasks, the average pay rate is $23.75/hr.7\nIn the training phase, workers are provided with instructions on the task, linked to an FAQ page, and\nare asked to annotate up to 30 examples from the development set. After answering each example,\nworkers are also asked to check their work against the provided ground truth label. After the training\nphase is complete, we provide the qualiﬁcation to work on the annotation phase to all workers\nwho annotated a minimum of ﬁve examples, i.e. completed ﬁve HITs during training and achieved\nperformance at, or above the median performance across all workers during training.\n7This estimate is taken from https://turkerview.com .\n16']","SuperGLUE provides a public leaderboard, a single-number performance metric for easy comparison, a software toolkit for model analysis, and detailed task instructions, supporting the development, evaluation, and comparison of models on its comprehensive suite of tasks.",0.999999999975,1.0,0.8977116818811343,1.0,0.5894031328951553,0.9458478256982685
 What tools and support does SuperGLUE offer to researchers working on language understanding models?,"SuperGLUE offers a modular toolkit for pretraining, multi-task learning, and transfer learning in NLP, built around standard tools like PyTorch and AllenNLP. It also includes a public leaderboard for performance tracking, comprehensive human baselines for benchmark tasks, and improved usage rules for fair competition. Additionally, the benchmark provides access to new, more challenging tasks and analysis tools for researchers.","['SuperGLUE: A Stickier Benchmark for\nGeneral-Purpose Language Understanding Systems\nAlex Wang∗\nNew York UniversityYada Pruksachatkun∗\nNew York UniversityNikita Nangia∗\nNew York University\nAmanpreet Singh∗\nFacebook AI ResearchJulian Michael\nUniversity of WashingtonFelix Hill\nDeepMindOmer Levy\nFacebook AI Research\nSamuel R. Bowman\nNew York University\nAbstract\nIn the last year, new models and methods for pretraining and transfer learning have\ndriven striking performance improvements across a range of language understand-\ning tasks. The GLUE benchmark, introduced a little over one year ago, offers\na single-number metric that summarizes progress on a diverse set of such tasks,\nbut performance on the benchmark has recently surpassed the level of non-expert\nhumans, suggesting limited headroom for further research. In this paper we present\nSuperGLUE, a new benchmark styled after GLUE with a new set of more difﬁ-\ncult language understanding tasks, a software toolkit, and a public leaderboard.\nSuperGLUE is available at super.gluebenchmark.com .\n1 Introduction\nRecently there has been notable progress across many natural language processing (NLP) tasks, led\nby methods such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT\n(Devlin et al., 2019). The unifying theme of these methods is that they couple self-supervised learning\nfrom massive unlabelled text corpora with effective adapting of the resulting model to target tasks.\nThe tasks that have proven amenable to this general approach include question answering, textual\nentailment, and parsing, among many others (Devlin et al., 2019; Kitaev et al., 2019, i.a.).\nIn this context, the GLUE benchmark (Wang et al., 2019a) has become a prominent evaluation\nframework for research towards general-purpose language understanding technologies. GLUE is\na collection of nine language understanding tasks built on existing public datasets, together with\nprivate test data, an evaluation server, a single-number target metric, and an accompanying expert-\nconstructed diagnostic set. GLUE was designed to provide a general-purpose evaluation of language\nunderstanding that covers a range of training data volumes, task genres, and task formulations. We\nbelieve it was these aspects that made GLUE particularly appropriate for exhibiting the transfer-\nlearning potential of approaches like OpenAI GPT and BERT.\nThe progress of the last twelve months has eroded headroom on the GLUE benchmark dramatically.\nWhile some tasks (Figure 1) and some linguistic phenomena (Figure 2 in Appendix B) measured\nin GLUE remain difﬁcult, the current state of the art GLUE Score as of early July 2019 (88.4 from\nYang et al., 2019) surpasses human performance (87.1 from Nangia and Bowman, 2019) by 1.3\npoints, and in fact exceeds this human performance estimate on four tasks. Consequently, while there\n∗Equal contribution. Correspondence: glue-benchmark-admin@googlegroups.com\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.arXiv:1905.00537v3  [cs.CL]  13 Feb 2020'
 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems .\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R. Bowman. 2019a. SuperGLUE:\nA stickier benchmark for general-purpose language\nunderstanding systems. arXiv preprint 1905.00537 .\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019b.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Inter-\nnational Conference on Learning Representations\n(ICLR) .\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2018. Neural network acceptability judg-\nments. arXiv preprint 1805.12471 .\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In North\nAmerican Association for Computational Linguis-\ntics (NAACL) .\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237 .\nYang You, Jing Li, Jonathan Hseu, Xiaodan Song,\nJames Demmel, and Cho-Jui Hsieh. 2019. Reduc-\ning bert pre-training time from 3 days to 76 minutes.\narXiv preprint arXiv:1904.00962 .\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. arXiv preprint arXiv:1905.12616 .\nYukun Zhu, Ryan Kiros, Richard Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watch-\ning movies and reading books. In arXiv preprint\narXiv:1506.06724 .\nAppendix for “RoBERTa: A Robustly\nOptimized BERT Pretraining Approach”\nA Full results on GLUE\nIn Table 8we present the full set of development\nset results for RoBERTa. We present results for\naLARGE conﬁguration that follows BERT LARGE ,\nas well as a BASE conﬁguration that follows\nBERT BASE.B Pretraining Hyperparameters\nTable 9describes the hyperparameters for pre-\ntraining of RoBERTa LARGE and RoBERTa BASE\nC Finetuning Hyperparameters\nFinetuning hyperparameters for RACE, SQuAD\nand GLUE are given in Table 10. We select the\nbest hyperparameter values based on the median\nof 5 random seeds for each task.'
 'BiLSTM+ELMo+Attn\nOpenAI GPT\nBERT + Single-task Adapters\nBERT (Large)\nBERT on STILTs\nBERT + BAM\nSemBERT\nSnorkel MeTaL\nALICE (Large)\nMT-DNN (ensemble)\nXLNet-Large (ensemble)0.50.60.70.80.91.01.11.2\nGLUE Score\nHuman Performance\nCoLA\nSST-2MRPC\nSTS-B\nQQP\nMNLIQNLI\nRTE\nWNLIFigure 1: GLUE benchmark performance for submitted systems, rescaled to set human performance\nto 1.0, shown as a single number score, and broken down into the nine constituent task performances.\nFor tasks with multiple metrics, we use an average of the metrics. More information on the tasks\nincluded in GLUE can be found in Wang et al. (2019a) and in Warstadt et al. (2019, CoLA), Socher\net al. (2013, SST-2), Dolan and Brockett (2005, MRPC), Cer et al. (2017, STS-B), and Williams et al.\n(2018, MNLI), and Rajpurkar et al. (2016, the original data source for QNLI).\nremains substantial scope for improvement towards GLUE’s high-level goals, the original version of\nthe benchmark is no longer a suitable metric for quantifying such progress.\nIn response, we introduce SuperGLUE, a new benchmark designed to pose a more rigorous test of\nlanguage understanding. SuperGLUE has the same high-level motivation as GLUE: to provide a\nsimple, hard-to-game measure of progress toward general-purpose language understanding technolo-\ngies for English. We anticipate that signiﬁcant progress on SuperGLUE should require substantive\ninnovations in a number of core areas of machine learning, including sample-efﬁcient, transfer,\nmultitask, and unsupervised or self-supervised learning.\nSuperGLUE follows the basic design of GLUE: It consists of a public leaderboard built around\neight language understanding tasks, drawing on existing data, accompanied by a single-number\nperformance metric, and an analysis toolkit. However, it improves upon GLUE in several ways:\nMore challenging tasks: SuperGLUE retains the two hardest tasks in GLUE. The remaining tasks\nwere identiﬁed from those submitted to an open call for task proposals and were selected based on\ndifﬁculty for current NLP approaches.\nMore diverse task formats: The task formats in GLUE are limited to sentence- and sentence-pair\nclassiﬁcation. We expand the set of task formats in SuperGLUE to include coreference resolution\nand question answering (QA).\nComprehensive human baselines: We include human performance estimates for all benchmark\ntasks, which verify that substantial headroom exists between a strong BERT-based baseline and\nhuman performance.\nImproved code support: SuperGLUE is distributed with a new, modular toolkit for work on\npretraining, multi-task learning, and transfer learning in NLP, built around standard tools including\nPyTorch (Paszke et al., 2017) and AllenNLP (Gardner et al., 2017).\nReﬁned usage rules: The conditions for inclusion on the SuperGLUE leaderboard have been\nrevamped to ensure fair competition, an informative leaderboard, and full credit assignment to data\nand task creators.\nThe SuperGLUE leaderboard, data, and software tools are available at super.gluebenchmark.com .\n2 Related Work\nMuch work prior to GLUE demonstrated that training neural models with large amounts of available\nsupervision can produce representations that effectively transfer to a broad range of NLP tasks\n2'
 'Table 1: The tasks included in SuperGLUE. WSD stands for word sense disambiguation, NLI is\nnatural language inference, coref. is coreference resolution, and QAis question answering. For\nMultiRC, we list the number of total answers for 456/83/166 train/dev/test questions.\nCorpus |Train | | Dev | | Test |Task Metrics Text Sources\nBoolQ 9427 3270 3245 QA acc. Google queries, Wikipedia\nCB 250 57 250 NLI acc./F1 various\nCOPA 400 100 500 QA acc. blogs, photography encyclopedia\nMultiRC 5100 953 1800 QA F1 a/EM various\nReCoRD 101k 10k 10k QA F1/EM news (CNN, Daily Mail)\nRTE 2500 278 300 NLI acc. news, Wikipedia\nWiC 6000 638 1400 WSD acc. WordNet, VerbNet, Wiktionary\nWSC 554 104 146 coref. acc. ﬁction books\n(Collobert and Weston, 2008; Dai and Le, 2015; Kiros et al., 2015; Hill et al., 2016; Conneau and\nKiela, 2018; McCann et al., 2017; Peters et al., 2018). GLUE was presented as a formal challenge\naffording straightforward comparison between such task-agnostic transfer learning techniques. Other\nsimilarly-motivated benchmarks include SentEval (Conneau and Kiela, 2018), which speciﬁcally\nevaluates ﬁxed-size sentence embeddings, and DecaNLP (McCann et al., 2018), which recasts a set\nof target tasks into a general question-answering format and prohibits task-speciﬁc parameters. In\ncontrast, GLUE provides a lightweight classiﬁcation API and no restrictions on model architecture or\nparameter sharing, which seems to have been well-suited to recent work in this area.\nSince its release, GLUE has been used as a testbed and showcase by the developers of several\ninﬂuential models, including GPT (Radford et al., 2018) and BERT (Devlin et al., 2019). As shown\nin Figure 1, progress on GLUE since its release has been striking. On GLUE, GPT and BERT\nachieved scores of 72.8 and 80.2 respectively, relative to 66.5 for an ELMo-based model (Peters\net al., 2018) and 63.7 for the strongest baseline with no multitask learning or pretraining above the\nword level. Recent models (Liu et al., 2019d; Yang et al., 2019) have clearly surpassed estimates of\nnon-expert human performance on GLUE (Nangia and Bowman, 2019). The success of these models\non GLUE has been driven by ever-increasing model capacity, compute power, and data quantity, as\nwell as innovations in model expressivity (from recurrent to bidirectional recurrent to multi-headed\ntransformer encoders) and degree of contextualization (from learning representation of words in\nisolation to using uni-directional contexts and ultimately to leveraging bidirectional contexts).\nIn parallel to work scaling up pretrained models, several studies have focused on complementary\nmethods for augmenting performance of pretrained models. Phang et al. (2018) show that BERT can\nbe improved using two-stage pretraining, i.e., ﬁne-tuning the pretrained model on an intermediate\ndata-rich supervised task before ﬁne-tuning it again on a data-poor target task. Liu et al. (2019d,c) and\nBach et al. (2018) get further improvements respectively via multi-task ﬁnetuning and using massive\namounts of weak supervision. Clark et al. (2019b) demonstrate that knowledge distillation (Hinton\net al., 2015; Furlanello et al., 2018) can lead to student networks that outperform their teachers.\nOverall, the quantity and quality of research contributions aimed at the challenges posed by GLUE\nunderline the utility of this style of benchmark for machine learning researchers looking to evaluate\nnew application-agnostic methods on language understanding.\nLimits to current approaches are also apparent via the GLUE suite. Performance on the GLUE\ndiagnostic entailment dataset, at 0.42 R3, falls far below the average human performance of 0.80\nR3reported in the original GLUE publication, with models performing near, or even below, chance\non some linguistic phenomena (Figure 2, Appendix B). While some initially difﬁcult categories\nsaw gains from advances on GLUE (e.g., double negation), others remain hard (restrictivity) or']","SuperGLUE offers jiant, a software toolkit for pretraining, multi-task learning, and transfer learning in NLP, built around standard tools and supporting popular pretrained models, facilitating research and experimentation on the benchmark's tasks.",0.8055555555287036,1.0,0.8760634370211485,1.0,0.8910870540974443,0.9556525642158638
In what ways does GLM-130B's bilingual capability extend its application compared to monolingual models?,"GLM-130B's bilingual capability allows it to effectively handle tasks in both English and Chinese, broadening its usability across diverse linguistic contexts. This enables the model to cater to a wider audience and tackle cross-lingual tasks, which monolingual models cannot perform. Additionally, its ability to understand and generate content in two languages enhances its performance on multilingual benchmarks and applications.","['ence. Altogether, it contributes to the high quality of GLM-130B in terms of both language perfor-\nmance on 112 tasks and ethical results on bias and toxicity benchmarks. Our experiences of both\nsuccess and failure are condensed into the lessons for training 100B-scale LLMs, attached in the\nAppendix B.10.\n9'
 'Published as a conference paper at ICLR 2023\nEPRSTMT OCNLI-FC BUSTM CHID-FC CLUEWSC-FC C3 WSC1.1 CMNLI DRCD OCNLI_50K AFQMC CMRC2018020406080Acc. or EM92.5\n73.877.590.1\n77.4 77.583.977.0 77.1 74.771.2\n55.788.8\n53.864.487.1\n53.5 54.981.1\n51.7\n29.544.669.0\n16.6GLM-130B ERNIE 3.0 Titan-260B\nFigure 8: GLM-130B and ERNIE Titan 3.0 260B evaluated on zero-shot CLUE and FewCLUE.\n7 CLUE and 5 FewCLUE datasets (Cf. Appendix C.7 for details). We compare GLM-130B to the\nlargest existing Chinese monolingual language model—the 260B ERNIE Titan 3.0 (Wang et al.,\n2021). We follow its setting to report zero-shot results on dev datasets. GLM-130B consistently\noutperforms ERNIE Titan 3.0 across 12 tasks (Cf. Figure 8). Interestingly, GLM-130B performs at\nleast 260% better than ERNIE on two abstractive MRC datasets (DRCD and CMRC2018), possibly\ndue to GLM-130B’s pre-training objective that naturally resonates to abstractive MRC’s form.\n6 R ELATED WORK\nIn this section, we review related work to GLM-130B on topics of pre-training, transferring, and\ninference of pre-trained LLMs (Qiu et al., 2020; Bommasani et al., 2021).\nPre-Training. Vanilla language modeling refers to decoder-only autoregressive models (e.g.,\nGPT (Radford et al., 2018)), but it also recognizes any forms of self-supervised objectives on texts.\nRecently, transformer-based (Vaswani et al., 2017) language models present a fascinating scaling\nlaw: new abilities (Wei et al., 2022b) arise as models scale up, from 1.5B (Radford et al., 2019),\n10B-scale language models (Raffel et al., 2020; Shoeybi et al., 2019; Black et al., 2022), to 100B-\nscale GPT-3 (Brown et al., 2020). Later, despite many 100B-scale LLMs (Lieber et al., 2021; Thop-\npilan et al., 2022; Rae et al., 2021; Smith et al., 2022; Chowdhery et al., 2022; Wu et al., 2021; Zeng\net al., 2021; Wang et al., 2021) in both English and Chinese, they are not available to public or only\naccessible via limited APIs. The closeness of LLMs severely stymies its development. GLM-130B’s\nefforts, along with recent ElutherAI, OPT-175B (Zhang et al., 2022), and BLOOM-176B (Scao et al.,\n2022), aim to offer high-quality open-sourced LLMs to our community.\nTransferring. Though fine-tuning has been a de facto way for transfer learning, the evaluation for\nLLMs has been focused on prompting and in-context learning due to their tremendous sizes (Brown\net al., 2020; Liu et al., 2021a). Nevertheless, some recent attempts has been on parameter-efficient\nlearning on language models (Houlsby et al., 2019) and prompt tuning (i.e., P-tuning, Li & Liang\n(2021); Liu et al. (2021b); Lester et al. (2021); Liu et al. (2022)). For now we do not focus on them\nand will leave the comprehensive testing of them on GLM-130B in future study.\nInference. Most public-accessible LLMs nowadays are providing their services via limited APIs.In\nthis work, an important part of our endeavor has been on LLMs’ efficient and fast inference. Related\nwork may include distillation (Sanh et al., 2019; Jiao et al., 2020; Wang et al., 2020), quantiza-\ntion (Zafrir et al., 2019; Shen et al., 2020; Tao et al., 2022), and pruning (Michel et al., 2019; Fan\net al., 2019). Very recent work (Dettmers et al., 2022) shows that LLMs such as OPT-175B and\nBLOOM-176B can be quantized to 8 bit due to special distribution of outlier dimensions. In this\nwork, we demonstrate GLM’s scaling law for INT4 weight quantization, which allows GLM-130B\nto inference on as few as 4 ×RTX 3090 (24G) GPUs or 8 ×RTX 2080 Ti (11G) GPUs.\n7 C ONCLUSION AND LESSONS\nWe introduce GLM-130B, a bilingual pre-trained language model that aims to facilitate open and\ninclusive LLM research. GLM-130B’s technical and engineering undertakings generate insight into\nLLMs’ architectures, pre-training objectives, training stability and efficiency, and affordable infer-\nence. Altogether, it contributes to the high quality of GLM-130B in terms of both language perfor-'
 'Published as a conference paper at ICLR 2023\nF A B RIEF HISTORY OF GLM-130B\nThe GLM-130B project16was conceived in Dec. 2021 in a brainstorming meeting at Tsinghua KEG.\nWe firmly believe that it is of value to pre-train a highly accurate language model, in particular for\nboth Chinese and English. Though GPT-3 (Brown et al., 2020) is the pioneer for this effort, it is not\navailable to most people in the world. In addition, it supports English only. We therefore decide to\ninitialize the project GLM-130B. Please note that the WuDao 1.75T model we built last year is a\nsparse model with 480 mixture-of-experts (MoE), rather than a dense one as GPT-3. Our goal then\nis to train a bilingual pre-trained dense model with high accuracy on downstream tasks, and to make\nit open to everyone in the world-anyone, anywhere can download it and use it on a single server with\nappropriate GPUs.\nThe ambitious project soon faced several important challenges:\n•Lack of computational resources : No organization is willing to sponsor such a big project and\nfreely make it public.\n•Lack of a robust pre-training algorithm : Despite GPT-3’s success on English corpus, it is\nunclear how to train a high-accurate bilingual model for both English and Chinese.\n•Lack of fast inference solutions : Since the goal is to have the model public to everyone, we need\nto design fast inference solutions with low resource requirements to run the model.\nFor the pre-training algorithm, we finally chose GLM (Du et al., 2022) due to its high performance\nin practice. We eventually decided to train a GLM model of 130 billion parameters after several\nrounds of discussions and exploration, because such a size makes it possible to run the inference on\na single A100 (40G * 8) server.\nOur first attempt at training the model was in January 2022, shortly after we received a small sponsor\nof GPUs for test running. However, we soon realized that we had significantly underestimated the\ntechnical difficulties of pre-training a model at such a scale (>100B). It seems that pre-training a\nhighly accurate 100B-scale model is quite different from training a 10B-scale one. Due to frequent\nrandom hardware failures, model gradients exploding, unexpected excessive memory usage in the\nalgorithm, debug for the 3D pipeline in the new Megatron and DeepSpeed frameworks, inability to\nrecover from optimizer states, blocked TCP responses between processes, and many many unex-\npected “bugs”, the project was delayed for many times. The Tsinghua PACMAN team gave us a\nhand at this difficult time and together we successfully fixed most of the “bugs”.\nBy March, we were still short on computational resources, but fortunately got a chance to try test\nruns on several other platforms, including Ascend 910, Hygon DCU, NVIDIA, and Sunway. The\nimmediate challenge was for us to adapt our training code to these different platforms, as the under-\nlying operators are quite different. Also, it introduced many new issues: the element-wise operators\nnot supporting fast computation for large-dimension vectors, various issues that hindered conver-\ngence—the large gradient norms of input embeddings, native Post-LN, Pre-LN, and Sandwich-LN,\ndataloader state seeds, and computation precision choices in Softmax and Attention — as well as\nnumerous mistakes we ourselves made. With tremendous help from all of our generous partners, we\nfinally succeeded in making our pre-training algorithms runnable across all the platforms—frankly,\na surprising achievement for this project. The timeline of GLM-130B in Figure 21 covers most of\nthe issues we have encountered and addressed as of this writing.\nOn April 26th, we received a generous computing sponsorship from Zhipu.AI — an AI startup that\naims to teach machines to think like humans. After another week of testing, we finally kicked off\nthe training of the GLM-130B model on its 96 A100 (40G * 8) servers on May 6th. Additionally,'
 'Published as a conference paper at ICLR 2023\nGPT-3 175BPaLM 540BBLOOM 176BOPT-175BGLM-130BLanguage Ability EvaluationBias & Toxicity Evaluation\nFigure 1: A summary of the performance evaluation and ethical studies.\nTable 1: A comparison between GLM-130B and other 100B-scale LLMs and PaLM 540B. (LN:\nlayer norm.; FPF: floating-point format; MIP: multi-task instruction pre-training; CN : Chinese)\nArchitecture & Data Training Inference\nModelOpen-\nsource Objective LN Major Lang. FPF Stabilization Quantization GPU Needed\nGPT-3 175B × English FP16 undisclosed undisclosed undisclosed\nOPT-175B ✓ English FP16 Manual Adjusting INT8 8 ×3090\nBLOOM-176B ✓GPT Pre-LN\nMulti-lingual BF16 Embedding Norm INT8 8 ×3090\nPaLM 540B × GPT Pre-LN English BF16 Manual Adjusting undisclosed undisclosed\nGLM-130B ✓GLM (Blank\nInfilling & MIP)Deep-\nNormBilingual\n(EN & CN)FP16Embedding\nGradient ShrinkINT44×3090 or\n8×1080 Ti\nIn this work, we introduce the pre-training of a 100B-scale model—GLM-130B, in terms of engi-\nneering efforts, model design choices, training strategies for efficiency and stability, and quantization\nfor affordable inference. As it has been widely realized that it is computationally unaffordable to\nempirically enumerate all possible designs for training 100B-scale LLMs, we present not only the\nsuccessful part for training GLM-130B but also many of the failed options and lessons learned.\nParticularly, the training stability is the decisive factor in the success of training models of such a\nscale. Different from practices such as manually adjusting learning rates in OPT-175B and using\nembedding norm in the sacrifice of performance in BLOOM-176B, we experiment with various op-\ntions and find the strategy of embedding gradient shrink can significantly stabilize the training of\nGLM-130B.\nSpecifically, GLM-130B is a bilingual (English and Chinese) bidirectional dense model with 130 bil-\nlion parameters, pre-trained over 400 billion tokens on a cluster of 96 NVIDIA DGX-A100 (8 ×40G)\nGPU nodes between May 6 and July 3, 2022. Instead of using the GPT-style architecture, we adopt\nthe General Language Model (GLM) algorithm (Du et al., 2022) to leverage its bidirectional at-\ntention advantage and autoregressive blank infilling objective. Table 1 summarizes the comparison\nbetween GLM-130B, GPT-3 and another two open-source efforts—OPT-175B and BLOOM-176B,\nas well as PaLM 540B (Chowdhery et al., 2022)—a 4 ×larger model—as a reference.\nAltogether, the conceptual uniqueness and engineering efforts enable GLM-130B to exhibit perfor-\nmance that surpasses the level of GPT-3 on a wide range of benchmarks (in total 112 tasks) and also\noutperforms PaLM 540B in many cases, while outperformance over GPT-3 has not been observed in\nOPT-175B and BLOOM-176B (Cf. Figure 1 left). For zero-shot performance, GLM-130B is better\nthan GPT-3 175B (+5.0%), OPT-175B (+6.5%), and BLOOM-176B (+13.0%) on LAMBADA (Pa-\nperno et al., 2016), and achieves 3 ×better performance than GPT-3 on Big-bench-lite (Srivastava\net al., 2022). For the 5-shot MMLU (Hendrycks et al., 2021) tasks, it is better than GPT-3 175B\n(+0.9%) and BLOOM-176B (+12.7%). As a bilingual LLM also in Chinese, it offers significantly\nbetter results than ERNIE TITAN 3.0 260B (Wang et al., 2021)—the largest Chinese LLM—on 7\nzero-shot CLUE (Xu et al., 2020) datasets (+24.26%) and 5 zero-shot FewCLUE (Xu et al., 2021)\nones (+12.75%). Importantly, as summarized in Figure 1 right, GLM-130B as an open model is\nassociated with significantly less bias and generation toxicity than its 100B-scale counterparts .\nFinally, we design GLM-130B to empower as many people as possible to conduct 100B-scale LLM\nstudies. First, instead of using 175B+ parameters as OPT and BLOOM, the 130B size is decided be-\ncause such a size supports inference on a single A100 (8 ×40G) server. Second, to further lower the\nGPU requirements, we quantize GLM-130B into INT4 precision without post training while OPT']","As a bilingual model in English and Chinese, GLM-130B significantly outperforms monolingual models in related benchmarks, showcasing its superior performance in cross-lingual understanding and applications.",0.6388888888675925,0.9,0.9254120784859028,0.5,0.8717198535380487,0.9605636246785106
What intrinsic model characteristic allows Megatron-LM to achieve efficient training with multi-billion parameter transformer models?,"Megatron-LM achieves efficient training with multi-billion parameter transformer models by utilizing intra-layer model parallelism, which exploits the inherent structure of transformer architectures. This approach allows the model to be partitioned across multiple GPUs without requiring custom compiler or library changes, facilitating efficient memory management and computation. Additionally, the careful placement of layer normalization in BERT-like models is critical for enhancing performance as model size increases.","['Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nContext : The 36th International Conference on Machine\nLearning (ICML 2019) will be held in Long Beach, CA,\nUSA from June 10th to June 15th, 2019. The conference\nwill consist of one day of tutorials (June 10), followed\nby three days of main conference sessions (June 11-13),\nfollowed by two days of workshops (June 14-15).\nMegatron-LM : With a broad scope, the conference ad-\ndresses the challenges and opportunities in machine learning\nfor practitioners and researchers. The conference program\nwill feature sessions that cover different machine learning\ntopics, from reinforcement learning to computational biol-\nogy and from AI to deep learning. It is the world’s premier\nforum for exploring and presenting advances in machine\nlearning.\nICML 2019 will offer an impressive range of sessions,\nwhich will be of interest to practitioners, researchers, and\nstudents. Some of the topics we will cover include:\nReinforcement learning\nSoft Computing\nComputational biology\nConvolutional networks\nDeep learning\nStatistical learning theory\nStatistical learning machine\nNatural language processing\nBiology\nAwards\nSince its ﬁrst edition, the conference has grown and evolved.\nToday, it is the premier forum for showcasing state-of-the-\nart research in machine learning. The conference is widely\nrecognized as the preeminent venue for showcasing the\nlatest research in machine learning, and a place where prac-\ntitioners, researchers, and students can exchange ideas and\ninsights.\nThe conference is a peer-reviewed venue that draws its pres-\ntige and inﬂuence from the excellence of the presentations\nand the quality of the invited speakers. With over 50% of all\ntalks selected based on their content by an expert committee,\nit is clear that the conference’s high standards of research\nand quality keep a very high bar.Context : Rescue workers on Tuesday found a ﬂight\nrecorder from the Russian plane that crashed into the Black\nSea over the weekend, the defense ministry said. All 92\npeople aboard the Russian military’s Tu-154 plane are be-\nlieved to have died Sunday morning when it crashed two\nminutes after taking off from the southern Russian city of\nSochi.\nMegatron-LM : A child holds up a portrait of a dead relative\nat a makeshift morgue in a building belonging to a local\ngovernment in the town of Petropavlovsk-Kamchatsky. A\nRussian Defense Ministry Tu-154 plane with 92 people\non board crashed into the Black Sea on Sunday morning\nshortly after taking off from the southern Russian city of\nSochi. (Yuri Kochetkov/EPA)\nAccording to the ministry, the plane was found at a depth\nof more than 1,000 feet and the recorder was damaged.\n”The defense ministry has decided to hand over the recorder\nto the Interstate Aviation Committee, which is the main\ninvestigative body in the air crash,” the statement said.\nThe search for the plane’s black boxes is being hampered by\nstrong winds and rough seas. Investigators have launched a\ncriminal probe into the crash, which ofﬁcials said was likely\ncaused by a technical fault.\nRussian President Vladimir Putin declared Wednesday a\nday of mourning for the victims.\nD. Further Scaling Analysis\nIn this section we study the effect of number of attention\nheads on the scaling results. We also present strong scaling\nresults for our 1.2 billion parameter model.\nD.1. Attention Heads and Scaling\nThis section studies the effect of attention heads on model\nparallel scaling. To this end, we consider the 8.3 billion\nparameter conﬁguration with 8-way model parallelism and\nvary the number of heads from 16 to 32. The results are\npresented in Table 7. As the number of attention heads\nincreases, some of the GEMMS inside the self-attention\nlayer become smaller and also the number of elements in\nthe self attention softmax increases. This results in a slight\ndecrease in scaling efﬁciency. Future research should be\nwary of this hyperparameter to design large transformer'
 'Megatron-LM: Training Multi-Billion Parameter Language Models Using\nModel Parallelism\nMohammad Shoeybi1 2Mostofa Patwary1 2Raul Puri1 2Patrick LeGresley2Jared Casper2\nBryan Catanzaro2\nAbstract\nRecent work in language modeling demonstrates\nthat training large transformer models advances\nthe state of the art in Natural Language Processing\napplications. However, very large models can be\nquite difﬁcult to train due to memory constraints.\nIn this work, we present our techniques for train-\ning very large transformer models and implement\na simple, efﬁcient intra-layer model parallel ap-\nproach that enables training transformer models\nwith billions of parameters. Our approach does\nnot require a new compiler or library changes, is\northogonal and complimentary to pipeline model\nparallelism, and can be fully implemented with\nthe insertion of a few communication operations\nin native PyTorch. We illustrate this approach\nby converging transformer based models up to\n8.3 billion parameters using 512 GPUs. We sus-\ntain 15.1 PetaFLOPs across the entire applica-\ntion with 76% scaling efﬁciency when compared\nto a strong single GPU baseline that sustains 39\nTeraFLOPs, which is 30% of peak FLOPs. To\ndemonstrate that large language models can fur-\nther advance the state of the art (SOTA), we train\nan 8.3 billion parameter transformer language\nmodel similar to GPT-2 and a 3.9 billion parame-\nter model similar to BERT. We show that careful\nattention to the placement of layer normalization\nin BERT-like models is critical to achieving in-\ncreased performance as the model size grows. Us-\ning the GPT-2 model we achieve SOTA results\non the WikiText103 (10.8 compared to SOTA per-\nplexity of 15.8) and LAMBADA (66.5% com-\npared to SOTA accuracy of 63.2%) datasets. Our\nBERT model achieves SOTA results on the RACE\ndataset (90.9% compared to SOTA accuracy of\n89.4%).\n1Equal contribution2NVIDIA. Correspondence to: Mohammad\nShoeybi <mshoeybi@nvidia.com >.1. Introduction\nNatural Language Processing (NLP) is advancing quickly in\npart due to an increase in available compute and dataset size.\nThe abundance of compute and data enables training increas-\ningly larger language models via unsupervised pretraining\n(Devlin et al., 2018; Radford et al., 2019). Empirical evi-\ndence indicates that larger language models are dramatically\nmore useful for NLP tasks such as article completion, ques-\ntion answering, and natural language inference (Lan et al.,\n2019; Raffel et al., 2019). By ﬁnetuning these pretrained\nlanguage models on downstream natural language tasks,\none can achieve state of the art results as shown in recent\nwork (Devlin et al., 2018; Peters et al., 2018; Howard &\nRuder, 2018; Radford et al., 2018; 2017; Ramachandran\net al., 2016; Liu et al., 2019b; Dai et al., 2019; Yang et al.,\n2019; Liu et al., 2019a; Lan et al., 2019).\nAs these models become larger, they exceed the memory\nlimit of modern processors, and require additional memory\nmanagement techniques such as activation checkpointing\n(Chen et al., 2016). Widely used optimization algorithms\nsuch as ADAM require additional memory per parameter to\nstore momentum and other optimizer state, which reduces\nthe size of models that can be effectively trained. Several\napproaches to model parallelism overcome this limit by\npartitioning the model such that the weights and their asso-\nciated optimizer state do not need to reside concurrently on\nthe processor. For example, GPipe (Huang et al., 2018) and\nMesh-Tensorﬂow (Shazeer et al., 2018) provide frameworks\nfor model parallelism of different kinds. However, they\nrequire rewriting the model, and rely on custom compilers\nand frameworks that are still under development.\nIn this work, we implement a simple and efﬁcient model\nparallel approach using intra-layer model-parallelism. We\nexploit the inherent structure in transformer based language\nmodels to make a simple model-parallel implementation that\ntrains efﬁciently in PyTorch, with no custom C++ code or'
 'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nFigure 2. Transformer Architecture. Purple blocks correspond to\nfully connected layers. Each blue block represents a single trans-\nformer layer that is replicated N times.\nand compute efﬁciency. The original transformer formula-\ntion was designed as a machine translation architecture that\ntransforms an input sequence into another output sequence\nusing two parts, an Encoder andDecoder . However, recent\nwork leveraging transformers for language modeling such as\nBERT (Devlin et al., 2018) and GPT-2 (Radford et al., 2019)\nuse only the Encoder orDecoder depending on their needs.\nThis work explores both a decoder architecture, GPT-2, and\nan encoder architecture, BERT.\nFigure 2 shows a schematic diagram of the model we used.\nWe refer the reader to prior work for a detailed descrip-\ntion of the model architecture (Vaswani et al., 2017; Devlin\net al., 2018; Radford et al., 2019). It is worthwhile to men-\ntion that both GPT-2 and BERT use GeLU (Hendrycks &\nGimpel, 2016) nonlinearities and layer normalization (Ba\net al., 2016) to the input of the multi-head attention and feed\nforward layers, whereas the original transformer (Vaswani\net al., 2017) uses ReLU nonlinearities and applies layer\nnormalization to outputs.\n2.3. Data and Model Parallelism in Deep Learning\nThere are two central paradigms for scaling out deep neu-\nral network training to numerous hardware accelerators:\ndata parallelism (Valiant, 1990) where a training minibatch\nis split across multiple workers, and model parallelism in\nwhich the memory usage and computation of a model is\ndistributed across multiple workers. By increasing the mini-\nbatch size proportionally to the number of available work-\ners (i.e. weak scaling ), one observes near linear scaling\nin training data throughput. However, large batch train-\ning introduces complications into the optimization process\nthat can result in reduced accuracy or longer time to conver-\ngence, offsetting the beneﬁt of increased training throughput\n(Keskar et al., 2017). Further research (Goyal et al., 2017;\nYou et al., 2017; 2019) has developed techniques to miti-gate these effects and drive down the training time of large\nneural networks. To scale out training even further, parallel\nwork (Chen et al., 2016) has combined data parallelism with\nactivation checkpointing: recomputing activations in the\nbackward pass without storing them in the forward pass to\nreduce memory requirements.\nHowever, these techniques have one fundamental limitation\nin the problem size they can tackle: the model must ﬁt\nentirely on one worker. With language models of increasing\nsize and complexity like BERT and GPT-2, neural networks\nhave approached the memory capacity of modern hardware\naccelerators. One solution to this problem is to employ\nparameter sharing to reduce the memory footprint of the\nmodel (Lan et al., 2019), but this limits the overall capacity\nof the model. Our approach is to utilize model parallelism\nto split the model across multiple accelerators. This not\nonly alleviates the memory pressure, but also increases the\namount of parallelism independently of the microbatch size.\nWithin model parallelism, there are two further paradigms:\nlayer-wise pipeline parallelism, and more general distributed\ntensor computation. In pipeline model parallelism, groups\nof operations are performed on one device before the outputs\nare passed to the next device in the pipeline where a differ-\nent group of operations are performed. Some approaches\n(Harlap et al., 2018; Chen et al., 2018) use a parameter\nserver (Li et al., 2014) in conjunction with pipeline par-\nallelism. However these suffer from inconsistency issues.\nThe GPipe framework for TensorFlow (Huang et al., 2018)\novercomes this inconsistency issue by using synchronous\ngradient decent. This approach requires additional logic to\nhandle the efﬁcient pipelining of these communication and'
 'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\ninvestigation that will further test existing deep learning\nhardware and software. To realize this, improvements in\nthe efﬁciency and memory footprint of optimizers will be\nneeded. In addition, training a model with more than 16\nbillion parameters will demand more memory than is avail-\nable within 16 GPUs of a DGX-2H box. For such models, a\nhybrid intra-layer and inter-layer model parallelism along\nwith inter-node model parallelism would be more suitable.\nThree other directions of investigation include (a) pretrain-\ning different model families (XLNet, T5), (b) evaluating per-\nformance of large models across more difﬁcult and diverse\ndownstream tasks (e.g. Generative Question Answering,\nSummarization, and Conversation), and (c) using knowl-\nedge distillation to train small student models from these\nlarge pretrained teacher models.\nReferences\nAbadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z.,\nCitro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M.,\nGhemawat, S., Goodfellow, I., Harp, A., Irving, G., Is-\nard, M., Jia, Y ., Jozefowicz, R., Kaiser, L., Kudlur, M.,\nLevenberg, J., Man ´e, D., Monga, R., Moore, S., Mur-\nray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B.,\nSutskever, I., Talwar, K., Tucker, P., Vanhoucke, V ., Va-\nsudevan, V ., Vi ´egas, F., Vinyals, O., Warden, P., Watten-\nberg, M., Wicke, M., Yu, Y ., and Zheng, X. TensorFlow:\nLarge-scale machine learning on heterogeneous systems,\n2015. URL http://tensorflow .org/ . Software\navailable from tensorﬂow.org.\nBa, J. L., Kiros, J. R., and Hinton, G. E. Layernorm. CoRR ,\nabs/1607.06450, 2016. URL http://arxiv .org/\nabs/1607.06450 .\nChen, C.-C., Yang, C.-L., and Cheng, H.-Y . Efﬁcient and\nrobust parallel dnn training through model parallelism on\nmulti-gpu platform. arXiv:1809.02839 , 2018.\nChen, T., Xu, B., Zhang, C., and Guestrin, C. Train-\ning deep nets with sublinear memory cost. CoRR ,\nabs/1604.06174, 2016. URL http://arxiv .org/\nabs/1604.06174 .\nDai, Z., Yang, Z., Yang, Y ., Carbonell, J. G., Le, Q. V .,\nand Salakhutdinov, R. Transformer-xl: Attentive lan-\nguage models beyond a ﬁxed-length context. CoRR ,\nabs/1901.02860, 2019. URL http://arxiv .org/\nabs/1901.02860 .\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding, 2018.\nGoyal, P., Doll ´ar, P., Girshick, R. B., Noordhuis, P.,\nWesolowski, L., Kyrola, A., Tulloch, A., Jia, Y ., andHe, K. Accurate, large minibatch SGD: training imagenet\nin 1 hour. CoRR , abs/1706.02677, 2017.\nHarlap, A., Narayanan, D., Phanishayee, A., Se-\nshadri, V ., Devanur, N., Ganger, G., and Gibbons, P.\nPipedream: Fast and efﬁcient pipeline parallel dnn train-\ning.arXiv:1806.03377 , 2018.\nHendrycks, D. and Gimpel, K. Bridging nonlinearities\nand stochastic regularizers with gaussian error linear\nunits. CoRR , abs/1606.08415, 2016. URL http:\n//arxiv.org/abs/1606 .08415 .\nHoward, J. and Ruder, S. Fine-tuned language models for\ntext classiﬁcation. CoRR , abs/1801.06146, 2018.\nHuang, Y ., Cheng, Y ., Chen, D., Lee, H., Ngiam, J., Le,\nQ. V ., and Chen, Z. Gpipe: Efﬁcient training of gi-\nant neural networks using pipeline parallelism. CoRR ,\nabs/1811.06965, 2018. URL http://arxiv .org/\nabs/1811.06965 .\nJia, Z., Zaharia, M., and Aiken, A. Beyond data and model\nparallelism for deep neural networks. arXiv:1807.05358 ,\n2018.\nJoshi, M., Chen, D., Liu, Y ., Weld, D. S., Zettlemoyer,\nL., and Levy, O. Spanbert: Improving pre-training by\nrepresenting and predicting spans. arXiv:1907.10529 ,\n2019.\nKeskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy,\nM., and Tang, P. T. P. On large- batch training for deep\nlearning: Generalization gap and sharp minima. ICLR ,\n2017.\nKhandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and\nLewis, M. Generalization through memorization: Nearest\nneighbor language models. arXiv:1911.00172 , 2019.']","Megatron-LM utilizes intra-layer model parallelism exploiting the inherent structure of transformer-based language models, enabling efficient training without custom C++ or compiler requirements.",0.6388888888675925,1.0,0.9531292605944061,1.0,0.5106455433125494,0.951673082341107
Describe Megatron-LM's approach to handling the output embedding weight matrix for model parallelism.,"Megatron-LM parallelizes the output embedding weight matrix by splitting it along the vocabulary dimension, resulting in a partitioned input embedding weight matrix. This allows for efficient GEMM operations, where each partition only contains a portion of the embedding table, minimizing communication overhead. Additionally, the output embedding is fused with the cross-entropy loss to further reduce the communication size, enhancing the model's parallel processing efficiency.","['Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nrequire any new compiler or code re-writing, and can be\nfully implemented by inserting a few simple primitives, as\ndescribed in the next section.\n3. Model Parallel Transformers\nWe take advantage of the structure of transformer networks\nto create a simple model parallel implementation by adding a\nfew synchronization primitives. A transformer layer consists\nof a self attention block followed by a two-layer, multi-layer\nperceptron (MLP) as shown in Figure 2. We introduce\nmodel parallelism in both of these blocks separately.\nWe start by detailing the MLP block. The ﬁrst part of the\nblock is a GEMM followed by a GeLU nonlinearity:\nY=GeLU (XA) (1)\nOne option to parallelize the GEMM is to split the weight\nmatrixAalong its rows and input Xalong its columns as:\nX= [X1,X2], A=[A1\nA2]\n. (2)\nThis partitioning will result in Y=GeLU (X1A1+\nX2A2). Since GeLU is a nonlinear function, GeLU (X1A1+\nX2A2)̸=GeLU (X1A1)+GeLU (X2A2)and this approach\nwill require a synchronization point before the GeLU func-\ntion.\nAnother option is to split Aalong its columns A= [A1,A2].\nThis partitioning allows the GeLU nonlinearity to be inde-\npendently applied to the output of each partitioned GEMM:\n[Y1,Y2] = [ GeLU (XA 1),GeLU (XA 2)] (3)\nThis is advantageous as it removes a synchronization point.\nHence, we partition the ﬁrst GEMM in this column parallel\nfashion and split the second GEMM along its rows so it takes\nthe output of the GeLU layer directly without requiring any\ncommunication as shown in Figure 3a. The output of the\nsecond GEMM is then reduced across the GPUs before\npassing the output to the dropout layer. This approach splits\nboth GEMMs in the MLP block across GPUs and requires\nonly a single all-reduce operation in the forward pass ( g\noperator) and a single all-reduce in the backward pass ( f\noperator). These two operators are conjugates of each other\nand can be implemented in PyTorch with only a few lines of\ncode. As an example, the implementation of the foperator\nis provided below:\nclass f(torch.autograd.Function):\ndef forward(ctx, x):\nreturn x\ndef backward(ctx, gradient):\nall_reduce(gradient)\nreturn gradient\nCode 1. Implementation of foperator. gis similar to fwith\nidentity in the backward and all-reduce in the forward\nfunctions.\n(a) MLP\n(b) Self-Attention\nFigure 3. Blocks of Transformer with Model Parallelism. fandg\nare conjugate. fis an identity operator in the forward pass and all\nreduce in the backward pass while gis an all reduce in the forward\npass and identity in the backward pass.\nAs shown in Figure 3b, for the self attention block we exploit\ninherent parallelism in the multihead attention operation,\npartitioning the GEMMs associated with key ( K), query\n(Q), and value ( V) in a column parallel fashion such that\nthe matrix multiply corresponding to each attention head is\ndone locally on one GPU. This allows us to split per atten-\ntion head parameters and workload across the GPUs, and\ndoesnt require any immediate communication to complete\nthe self-attention. The subsequent GEMM from the output\nlinear layer (after self attention) is parallelized along its\nrows and takes the output of the parallel attention layer di-\nrectly, without requiring communication between the GPUs.\nThis approach for both the MLP and self attention layer\nfuses groups of two GEMMs, eliminates a synchronization\npoint in between, and results in better scaling. This enables\nus to perform all GEMMs in a simple transformer layer\nusing only two all-reduces in the forward path and two in\nthe backward path (see Figure 4).\nThe transformer language model has an output embedding\nwith the dimension of hidden-size ( H) times vocabulary-\nsize (v). Since the vocabulary size is on the order of tens\nof thousands of tokens for modern language models (for\nexample, GPT-2 used a vocabulary size of 50,257), it is ben-\neﬁcial to parallelize the output embedding GEMM. How-'
 'of thousands of tokens for modern language models (for\nexample, GPT-2 used a vocabulary size of 50,257), it is ben-\neﬁcial to parallelize the output embedding GEMM. How-\never, in transformer language models, the output embed-\nding layer shares weights with the input embedding, requir-\ning modiﬁcations to both. We parallelize the input embed-\nding weight matrix EH×valong the vocabulary dimension\nE= [E1,E2](column-wise). Since each partition now only'
 'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nContext : The 36th International Conference on Machine\nLearning (ICML 2019) will be held in Long Beach, CA,\nUSA from June 10th to June 15th, 2019. The conference\nwill consist of one day of tutorials (June 10), followed\nby three days of main conference sessions (June 11-13),\nfollowed by two days of workshops (June 14-15).\nMegatron-LM : With a broad scope, the conference ad-\ndresses the challenges and opportunities in machine learning\nfor practitioners and researchers. The conference program\nwill feature sessions that cover different machine learning\ntopics, from reinforcement learning to computational biol-\nogy and from AI to deep learning. It is the world’s premier\nforum for exploring and presenting advances in machine\nlearning.\nICML 2019 will offer an impressive range of sessions,\nwhich will be of interest to practitioners, researchers, and\nstudents. Some of the topics we will cover include:\nReinforcement learning\nSoft Computing\nComputational biology\nConvolutional networks\nDeep learning\nStatistical learning theory\nStatistical learning machine\nNatural language processing\nBiology\nAwards\nSince its ﬁrst edition, the conference has grown and evolved.\nToday, it is the premier forum for showcasing state-of-the-\nart research in machine learning. The conference is widely\nrecognized as the preeminent venue for showcasing the\nlatest research in machine learning, and a place where prac-\ntitioners, researchers, and students can exchange ideas and\ninsights.\nThe conference is a peer-reviewed venue that draws its pres-\ntige and inﬂuence from the excellence of the presentations\nand the quality of the invited speakers. With over 50% of all\ntalks selected based on their content by an expert committee,\nit is clear that the conference’s high standards of research\nand quality keep a very high bar.Context : Rescue workers on Tuesday found a ﬂight\nrecorder from the Russian plane that crashed into the Black\nSea over the weekend, the defense ministry said. All 92\npeople aboard the Russian military’s Tu-154 plane are be-\nlieved to have died Sunday morning when it crashed two\nminutes after taking off from the southern Russian city of\nSochi.\nMegatron-LM : A child holds up a portrait of a dead relative\nat a makeshift morgue in a building belonging to a local\ngovernment in the town of Petropavlovsk-Kamchatsky. A\nRussian Defense Ministry Tu-154 plane with 92 people\non board crashed into the Black Sea on Sunday morning\nshortly after taking off from the southern Russian city of\nSochi. (Yuri Kochetkov/EPA)\nAccording to the ministry, the plane was found at a depth\nof more than 1,000 feet and the recorder was damaged.\n”The defense ministry has decided to hand over the recorder\nto the Interstate Aviation Committee, which is the main\ninvestigative body in the air crash,” the statement said.\nThe search for the plane’s black boxes is being hampered by\nstrong winds and rough seas. Investigators have launched a\ncriminal probe into the crash, which ofﬁcials said was likely\ncaused by a technical fault.\nRussian President Vladimir Putin declared Wednesday a\nday of mourning for the victims.\nD. Further Scaling Analysis\nIn this section we study the effect of number of attention\nheads on the scaling results. We also present strong scaling\nresults for our 1.2 billion parameter model.\nD.1. Attention Heads and Scaling\nThis section studies the effect of attention heads on model\nparallel scaling. To this end, we consider the 8.3 billion\nparameter conﬁguration with 8-way model parallelism and\nvary the number of heads from 16 to 32. The results are\npresented in Table 7. As the number of attention heads\nincreases, some of the GEMMS inside the self-attention\nlayer become smaller and also the number of elements in\nthe self attention softmax increases. This results in a slight\ndecrease in scaling efﬁciency. Future research should be\nwary of this hyperparameter to design large transformer'
 'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nFigure 4. Communication operations in a transformer layer. There\nare 4 total communication operations in the forward and backward\npass of a single model parallel transformer layer.\ncontains a portion of the embedding table, an all-reduce ( g\noperator) is required after the input embedding. For the\noutput embedding, one approach is to perform the parallel\nGEMM [Y1,Y2] = [XE 1,XE 2]to obtain the logits, add an\nall-gatherY=all-gather ([Y1,Y2]), and send the results to\nthe cross-entropy loss function. However, for this case, the\nall-gather will communicate b×s×velements (bis the\nbatch-size and sis the sequence length) which is huge due to\nvocabulary size being large. To reduce the communication\nsize, we fuse the output of the parallel GEMM [Y1,Y2]with\nthe cross entropy loss which reduces the dimension to b×s.\nCommunicating scalar losses instead of logits is a huge re-\nduction in communication that improves the efﬁciency of\nour model parallel approach.\nMuch of our model parallel approach can be characterized\nas techniques aimed at reducing communication and keep-\ning the GPUs compute bound. Rather than having one GPU\ncompute part of the dropout, layer normalization, or residual\nconnections and broadcast the results to other GPUs, we\nchoose to duplicate the computation across GPUs. Speciﬁ-\ncally, we maintain duplicate copies of layer normalization\nparameters on each GPU, and take the output of the model\nparallel region and run dropout and residual connection\non these tensors before feeding them as input to the next\nmodel parallel regions. To optimize the model we allow\neach model parallel worker to optimize its own set of pa-\nrameters. Since all values are either local to or duplicated\non a GPU, there is no need for communicating updated\nparameter values in this formulation.\nWe present further details about the hybrid model and data\nparallelism and handling random number generation in Ap-\npendix B for reference. In summary, our approach as de-\nscribed above is simple to implement, requiring only a few\nextra all-reduce operations added to the forward and back-\nward pass. It does not require a compiler, and is orthogonal\nand complementary to the pipeline model parallelism advo-\ncated by approaches such as (Huang et al., 2018).4. Setup\nPretrained language understanding models are central tasks\nin natural language processing and language understanding.\nThere are several formulations of language modeling. In\nthis work we focus on GPT-2 (Radford et al., 2019), a left-\nto-right generative transformer based language model, and\nBERT (Devlin et al., 2018), a bi-directional transformer\nmodel based on language model masking. We explain our\nconﬁgurations for these models in the following section and\nrefer to the original papers for more details.\n4.1. Training Dataset\nTo collect a large diverse training set with longterm de-\npendencies we aggregate several of the largest language\nmodeling datasets. We create an aggregate dataset consist-\ning of Wikipedia (Devlin et al., 2018), CC-Stories (Trinh &\nLe, 2018), RealNews (Zellers et al., 2019), and OpenWeb-\ntext (Radford et al., 2019). To avoid training set leakage\ninto our downstream tasks we remove the Wikipedia articles\npresent in the WikiText103 test set (Merity et al., 2016).\nWe also remove unnecessary newlines from the CC-Stories\ncorpus introduced by preprocessing artifacts. For BERT\nmodels we include BooksCorpus (Zhu et al., 2015) in the\ntraining dataset, however, this dataset is excluded for GPT-2\ntrainings as it overlaps with LAMBADA task.\nWe combined all the datasets and then ﬁltered out all the\ndocuments with content length less than 128 tokens from\nthe aggregated dataset. Since similar content might appear\nmultiple times in the aggregated datasets, we used locality-\nsensitive hashing (LSH) to deduplicate content with a jac-\ncard similarity greater than 0.7. The resulting aggregate']","The output embedding weight matrix is parallelized along the vocabulary dimension, requiring minimal communication and maximizing compute efficiency during training.",0.9166666666361111,1.0,0.8629380359286647,1.0,0.47507462082454316,0.9002984832981726
How does the addition of a Python interpreter in PAL's framework influence the accuracy of solutions?,"The addition of a Python interpreter in PAL's framework significantly enhances the accuracy of solutions by offloading calculations and some reasoning to the interpreter, which ensures correctness given the right programmatic steps. This approach leads to improved performance compared to traditional methods, as demonstrated by PAL outperforming larger language models like PaLM-540B. The integration allows for accurate execution of generated code, which is critical for tasks requiring precise arithmetic and logic.","['domain-speciﬁc languages, making Python code a much\nmore preferable representation. Andor et al. (2019) generate\ntask-speciﬁc arithmetic operations for reading comprehen-\nsion tasks; Gupta et al. (2019) design neural modules such\nascount to deal with arithmetic operations. PALgener-\nalizes these works by generating general Python programs,\nwithout the need for deﬁning specialized modules. The clos-\nest work to ours technically may be Binder (Cheng et al.,\n2022), but it addressed mostly answering questions about\ntables using SQL and SQL-like Python.'
 'PAL: Program-aided Language Models 14\nthat even with a prompt that matches the numbers, there are only modest gains in performance. These results show that the\ngains achieved by using code-based reasoning chains may not be achieved simply by using better few-shot examples for\nCOT.\nRegular Prompt Prompt with Larger Numbers\nCOT 23.3 23.8\nTable 5: GSM-hard results, when the prompts also had examples of larger numbers.\nSuccinct Code The programs used in few-shot examples by PALare multi-step, and show a step-by-step breakdown of\nthe reasoning process. Is this breakdown necessary? Alternatively, can we return a single line expression (see Figure 11b) to\ncalculate the result? Results in Table 6 ( 4throw) shows that is not the case. With single-line expressions, the performance of\nPAL falls to the level of direct prompting.\nGenerating the answer directly PALﬁrst generates a reasoning chain in the form of a Python program, and passes the\ngenerated program to a runtime to obtain an answer. Is PALbetter only because of the program-style intermediate reasoning\nchains, or are the improvements derived from ofﬂoading execution to the Python runtime? To investigate this, we experiment\nwith a variant that forces the LLM to generate the answer after generating the reasoning chain (Figure 11e). This setting\ncompels the LLM to condition on the generated code-based reasoning to generate an answer, simulating the runtime. The\nresults in Table 6 ( 5throw) show that the solve rate drops to near DIRECT levels. This reinforces our hypothesis that while\ncurrent LLMs can be excellent at specifying a high-level plan to solve a task—they are still incapable of executing them.\nAblation Solve Rate\nDIRECT (no intermediate reasoning) 19.7\nCOT 65.6\nPAL 72.0\nSuccinct Code 47.8\nLLM Simulating Runtime 23.2\nTable 6: Solve Rates for Ablations\nC. Effect of Using Language Models of Code\nIn our experiments, we focused on evaluating the performance of a language model for code. We aimed to investigate\nwhether the additional performance boost observed in our results was due to the use of models like Codex, or whether our\nformulation was useful even for text-based models. To this end, we conducted additional experiments using text-based\nlanguage models. Our ﬁndings indicate that the PAL approach is not restricted to working solely with Codex, but can also\nbe applied to natural language (NL) models, as long as the model is sufﬁciently strong. Speciﬁcally, our results showed that\nin the text-davinci-001 model, the use of the CoT approach resulted in better performance.\nModel CoT PaL\ntext-davinci-001 26.5 8.6\ntext-davinci-002 46.9 65.8\ntext-davinci-003 65.3 69.8\nD. Analyzing the Effect of Increasing Number of Samples on P AL\nIn Section 5.1, we show that PALoutperforms strong baselines both for a single sample and by drawing 40 samples and\nusing majority voting. Figure 12 illustrates the trends for cases when the number of samples drawn are between 1 and 40,\nand the interpolation estimates demonstrate that P AL remains competitive throughout the number of samples.'
 'PAL: Program-aided Language Models 9\n8. Conclusion\nWe introduce PAL, a new method for natural language rea-\nsoning, using programs as intermediate reasoning steps.\nDifferently from existing LM-based reasoning approaches,\nthe main idea is to ofﬂoad solving and calculating to an\nexternal Python interpreter, instead of using the LLM for\nboth understanding the problem andsolving. This results\nin a ﬁnal answer that is guaranteed to be accurate, given the\ncorrectly predicted programmatic steps. We demonstrate\nthis seamless synergy between an LLM and a Python in-\nterpreter across 13 tasks from BIG-Bench Hard and other\nbenchmarks. In all these benchmarks, PALoutperforms\nlarger LLMs such as PaLM-540 Bwhich use the popular\n“chain-of-thought” method and sets new state-of-the-art ac-\ncuracy on all of them. We believe that these results unlock\nexciting directions for future neuro-symbolic AI reasoners.\nReferences\nAhn, M., Brohan, A., Brown, N., Chebotar, Y ., Cortes, O.,\nDavid, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman,\nK., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B.,\nIrpan, A., Jang, E., Ruano, R. J., Jeffrey, K., Jesmonth,\nS., Joshi, N. J., Julian, R., Kalashnikov, D., Kuang, Y .,\nLee, K.-H., Levine, S., Lu, Y ., Luu, L., Parada, C., Pastor,\nP., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D.,\nSermanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke,\nV ., Xia, F., Xiao, T., Xu, P., Xu, S., Yan, M., and Zeng,\nA. Do as I Can, not as I Say: Grounding Language in\nRobotic Affordances. arXiv preprint arXiv:2204.01691 ,\n2022.\nAmini, A., Gabriel, S., Lin, S., Koncel-Kedziorski, R., Choi,\nY ., and Hajishirzi, H. MathQA: Towards Interpretable\nMath Word Problem Solving with Operation-Based For-\nmalisms. In ACL, 2019.\nAndor, D., He, L., Lee, K., and Pitler, E. Giving bert a cal-\nculator: Finding operations and arguments with reading\ncomprehension. arXiv preprint arXiv:1909.00109 , 2019.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,\nHenighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,\nJ., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,\nGray, S., Chess, B., Clark, J., Berner, C., McCandlish,\nS., Radford, A., Sutskever, I., and Amodei, D. Language\nModels are Few-Shot Learners. In NeurIPS , 2020.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y ., Joseph, N., Brockman,\nG., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf,\nH., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N.,\nPavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter,C., Tillet, P., Such, F. P., Cummings, D., Plappert, M.,\nChantzis, F., Barnes, E., Herbert-V oss, A., Guss, W. H.,\nNichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin,\nI., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr,\nA. N., Leike, J., Achiam, J., Misra, V ., Morikawa, E.,\nRadford, A., Knight, M., Brundage, M., Murati, M.,\nMayer, K., Welinder, P., McGrew, B., Amodei, D., Mc-\nCandlish, S., Sutskever, I., and Zaremba, W. Evaluating\nLarge Language Models Trained on Code. arXiv preprint\narXiv:2107.03374 , 2021a.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y ., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374 , 2021b.\nChen, W., Ma, X., Wang, X., and Cohen, W. W. Program\nof thoughts prompting: Disentangling computation from\nreasoning for numerical reasoning tasks. arXiv preprint\narXiv:2211.12588 , 2022.\nCheng, Z., Xie, T., Shi, P., Li, C., Nadkarni, R., Hu, Y .,\nXiong, C., Radev, D., Ostendorf, M., Zettlemoyer, L.,\nSmith, N. A., and Yu, T. Binding language models in\nsymbolic languages. arXiv preprint arXiv:2210.02875 ,\n2022.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton,\nC., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,'
 'PAL: Program-aided Language Models 8\nColored Objects Date Penguins60708090100\n84.4\n64.879.295.2\n76.293.391.1\n69.191.3\n79.9\n63.491.9COT PAL PAL−comment PAL−var\n−comment\nFigure 9: Ablation study of PALprompt formats. We consider the original PALprompt, it with natural language comments\nremoved ( PAL−comment ), and further variable names replaced with random character ( PAL−var\n−comment ). As a reference, we also\nshow the C OT performance (blue).\n2021; Reif et al., 2021; Wei et al., 2021; Sanh et al., 2021)\nto code-generation (Chen et al., 2021b). Methods such as\nchain-of-thought prompting ( COT) have further unlocked a\nvariety of reasoning tasks, boosting the performance of mod-\nels on a variety of benchmarks. Nevertheless, all previous\napproaches suffer from inaccuracy in arithmetic calculation\nand incorrect reasoning (Lewkowycz et al., 2022; Hendrycks\net al., 2021; Madaan & Yazdanbakhsh, 2022). PALavoids\nthese problems by ofﬂoading the calculation and some of\nthe reasoning to a Python interpreter, which is correct by\nconstruction, given the right program. Further, not only\nthatPALcan improve the standard chain-of-thought, it can\nimprove least-to-most prompting (Zhou et al., 2022) as well,\nas we show in Appendix I.\nLMs with external tools Several prior works have\nequipped neural models with specialized modules. For ex-\nample, Cobbe et al. (2021) employ a calculator for arith-\nmetic operations as a post hoc processing, and Demeter\n& Downey (2020) add specialized modules for generating\ncities and dates. Unlike these works, PALgenerates code\nfor a Python interpreter, which is general enough to handle\nboth arithmetic calculations and dates, without specialized\nmodules and ad-hoc ﬁxes. Chowdhery et al. (2022) and Wei\net al. (2022) have also experimented with external calcula-\ntors; however, the calculator had improved Codex by only\n2.3% (absolute) on GSM 8Kand improved PaLM-540 Bby\n1.7%, while PALimproves Codex by 6.4% on the same\nbenchmark (Section 5.1). Similarly to our work, Chowd-\nhery et al. (2022) have also experimented with generating\nPython code for solving the GSM 8Kbenchmark, but their\nexperiments resulted in lower accuracy than the standard\nPaLM-540 Bthat uses chain-of-thought. Pi et al. (2022)\npretrain the model on execution results of random expres-\nsions on a calculator, instead of using the solver at test time\nas well. While their model can hypothetically perform arith-\nmetic better than other pretrained LMs, their results on the\nSV AMP benchmark are much lower: 57.4% using a T5-11Bmodel, while PAL achieves 79.4% on the same benchmark\nwithout any specialized pretraining.\nShortly after a preprint of our work was submitted to arXiv,\nanother related work on “program of thought prompting”\n(Chen et al., 2022) was also submitted to arXiv. Their\nmethod is conceptually similar to ours, but PoT (1) only\ndemonstrates efﬁcacy on mathematical problems, whereas\nwe demonstrate gains on symbolic and algorithmic bench-\nmarks as well, and (2) chose benchmark-speciﬁc prompt\nexamples, while we used the same prompt examples as pre-\nvious work, to disentangled the beneﬁt of our approach from\nthe beneﬁt of the choice of examples.\nSemantic parsing Our work can also be seen as a very\ngeneral form of semantic parsing, where instead of parsing\ninto strict domain-speciﬁc languages, the model generates\nfree-form Python code. Some works constrain the decoder\nusing a Context-Free Grammar (CFG) to generate a domain-\nspeciﬁc meaning representation (Shin & Van Durme, 2021)\nor a canonical utterance, which can be converted to a Lisp-\nlike meaning representation (Shin et al., 2021). In contrast,\nPALdoes not require any constraining or domain-speciﬁc\nrepresentations other than Python code. Further, LMs that\nwere pretrained on Python are abundant compared to other\ndomain-speciﬁc languages, making Python code a much\nmore preferable representation. Andor et al. (2019) generate\ntask-speciﬁc arithmetic operations for reading comprehen-']","Incorporating a Python interpreter allows PAL to execute generated programs accurately, bridging the gap in LLMs' arithmetic and logical capabilities, thus significantly improving solution accuracy compared to relying solely on LLMs.",0.999999999975,0.8571428571428571,0.9834779295636764,1.0,0.7391816704702241,0.9567266818808962
How does CodeNet's dataset size and diversity support advanced AI for code research compared to previous datasets?,"CodeNet's dataset is the largest in its class, containing approximately 13.9 million code samples across 55 different programming languages, which is about ten times more than similar datasets like GCJ. This extensive size and diversity enable researchers to expose AI models to a realistic and complex landscape of coding tasks, enhancing their learning and performance. Additionally, CodeNet includes rich annotations and metadata, allowing for a variety of applications such as code classification, translation, and performance improvement.","['Given the success of non-AI tools for code, why should we consider AI to augment or possibly\nreplace them? Firstly, AI can help reﬁne and re-tune the heuristics used by traditional coding tools.\nSecondly, based on the training data from past experience, AI can help prioritize when there is more\nthan one sound answer [ 5]. Thirdly, an AI-based tool may handle incomplete or invalid code more\nrobustly, thus expanding its scope. Finally, AI can incorporate signals usually ignored by traditional\ntools for code, such as the natural language in identiﬁers or comments.\nIn the enterprise environment, developers often face code written by large teams over many years\nand geographies. Developers must manipulate such code to modernize it, ﬁx bugs, improve its\nperformance, evolve it when requirements change, make it more secure, and/or comply with regu-\nlations. These tasks are challenging, and it is crucial to provide tool support for developers to be\nmore productive at performing them. It is well known that the latest advancements in deep learning\nalgorithms rely on best-of-breed datasets, such as ImageNet, to create increasingly complex and\npowerful models. In this paper, we present ""CodeNet"", a ﬁrst-of-its-kind dataset in scale, diversity,\nand quality, to accelerate the algorithmic advances in AI for Code.\nTo promote widespread adoption of CodeNet, we will be launching contests involving use cases based\non the dataset. The ﬁrst contest [ 6] will focus on diversity, inclusion and spurring interest among\naspiring data scientists. We are partnering with the Global Women in Data Science organization (with\npresence in over 50 countries) founded by Stanford University [ 7] and targeting teams with at least\nﬁfty percent women. We are planning follow-up contests that target experienced AI practitioners.\nThe rest of the paper is organized as follows. Section 2 introduces the CodeNet dataset. Related\ndatasets are discussed in Section 3, and the differentiation of CodeNet with respect to these related\ndatasets is elaborated in Section 4. Section 5 describes how CodeNet was curated and Section 6\nenumerates the usability features of CodeNet with several pre-processing tools to transform source\ncodes into representations that can be readily used as inputs into machine learning models. Section 7\ndiscusses the upcoming CodeNet contest and Section 8 describes important baseline experiments\nwith the CodeNet dataset. Section 9 presents further uses of the CodeNet dataset and Section 10\nconcludes the paper.\n2 The CodeNet Dataset\nThe CodeNet dataset consists of a large collection of code samples with extensive metadata. It\nalso contains documented tools to transform code samples into intermediate representations and to\naccess the dataset and make tailored selections. Our goal is to provide the community with a large,\nhigh-quality curated dataset that can be used to advance AI techniques for source code.\nCodeNet is derived from the data available on two online judge websites: AIZU [ 8] and AtCoder [ 9].\nOnline judge websites pose programming problems in the form of courses and contests. The dataset\nconsists of submissions to these problems, which are judged by an automated review process for\ncorrectness. Problem descriptions, submission outcomes, and associated metadata are available via\nvarious REST APIs.\nScale and Statistics. CodeNet contains a total of 13,916,868 submissions, divided into 4053\nproblems. Among the submissions, 53.6% (7,460,588) are accepted (compilable and pass the\nprescribed tests), 29.5% are marked with wrong answer, and the remaining rejected due to their\nfailure to meet run time or memory requirements. To our knowledge, this is the largest dataset so\nfar among similar kinds. Submissions are in 55 different languages; 95% of them are coded in C++,\nPython, Java, C, Ruby, and C#. C++ is the most common language, with 8,008,527 submissions (57%'
 'CodeNet: A Large-Scale AI for Code Dataset for\nLearning a Diversity of Coding Tasks\nRuchir Puri1, David S. Kung1, Geert Janssen1, Wei Zhang1,\nGiacomo Domeniconi1,Vladimir Zolotov1,Julian Dolby1,Jie Chen2,1,\nMihir Choudhury1,Lindsey Decker1,Veronika Thost2,1,Luca Buratti1,\nSaurabh Pujar1,Shyam Ramji1,Ulrich Finkler1,Susan Malaika3,Frederick Reiss1\n1IBM Research\n2MIT-IBM Watson AI Lab\n3IBM Worldwide Ecosystems\nAbstract\nOver the last several decades, software has been woven into the fabric of every\naspect of our society. As software development surges and code infrastructure of\nenterprise applications ages, it is now more critical than ever to increase software\ndevelopment productivity and modernize legacy applications. Advances in deep\nlearning and machine learning algorithms have enabled breakthroughs in computer\nvision, speech recognition, natural language processing and beyond, motivating\nresearchers to leverage AI techniques to improve software development efﬁciency.\nThus, the fast-emerging research area of “AI for Code” has garnered new interest\nand gathered momentum. In this paper, we present a large-scale dataset CodeNet ,\nconsisting of over 14 million code samples and about 500 million lines of code\nin 55 different programming languages, which is aimed at teaching AI to code.\nIn addition to its large scale, CodeNet has a rich set of high-quality annotations\nto benchmark and help accelerate research in AI techniques for a variety of crit-\nical coding tasks, including code similarity and classiﬁcation, code translation\nbetween a large variety of programming languages, and code performance (runtime\nand memory) improvement techniques. Additionally, CodeNet provides sample\ninput and output test sets for 98.5% of the code samples, which can be used as\nan oracle for determining code correctness and potentially guide reinforcement\nlearning for code quality improvements. As a usability feature, we provide several\npre-processing tools in CodeNet to transform source code into representations that\ncan be readily used as inputs into machine learning models. Results of code classi-\nﬁcation and code similarity experiments using the CodeNet dataset are provided as\na reference. We hope that the scale, diversity and rich, high-quality annotations of\nCodeNet will offer unprecedented research opportunities at the intersection of AI\nand Software Engineering.\n1 Introduction\nThere is a growing trend towards leveraging AI for building tools that support software engineering\nand development [ 1,2]. AI can manipulate and generate computer code, but can it do so with\nhigh quality? Many researchers are fascinated by this possibility, encouraged by AI successes in\nother domains and tantalized by the vision of computers programming computers. Some recent\ndeep-learning models [ 3,4] for code have received a lot of publicity: trained on vast amounts of\ndata and using novel architectures with billions of parameters, they sometimes generate surprisingly\nplausible code.\nPreprint. Under review.arXiv:2105.12655v2  [cs.SE]  29 Aug 2021'
 '9 Further Uses of CodeNet\nThe rich metadata and language diversity open CodeNet to a plethora of use cases. The problem-\nsubmission relationship in CodeNet corresponds to type-4 similarity [ 43] and can be used for code\nsearch and clone detection. The code samples in CodeNet are labeled with their acceptance status\nso we can readily extract pairs of buggy and ﬁxed code for code repair [ 49,50]. A large number\nof code samples come with inputs so that we can execute the code to extract the CPU run time and\nmemory footprint, which can be used for regression studies and prediction.\nCodeNet may also be used for program translation, given its wealth of programs written in a multitude\nof languages. Translation between two programming languages is born out of a practical need to port\nlegacy codebases to modern languages in order to increase accessibility and lower maintenance costs.\nWith the help of neural networks, machine translation models developed for natural languages [ 51]\nwere adapted to programming languages, producing pivotal success [ 4]. One considerable challenge of\nneural machine translation is that model training depends on large, parallel corpora that are expensive\nto curate [ 52], especially for low-resource languages (e.g., legacy code). Recently, monolingual\napproaches [ 53,4] were developed to mitigate the reliance on parallel data, paving ways to build\nmodels for languages with little translation. Compared with current popular data sets (e.g., [ 4,54]),\nCodeNet covers a much richer set of languages with ample training instances.\n10 Conclusion\nArtiﬁcial intelligence has made great strides in understanding human language. Computer scientists\nhave been fascinated by the possibility and tantalized by the vision of computers (AI) programming\ncomputers. In this paper, we presented ""CodeNet"", a ﬁrst-of-its-kind very large-scale, diverse and\nhigh-quality dataset to accelerate the algorithmic advances in AI for Code. This dataset is not\nonly unique in its scale, but also in the diversity of coding tasks it can help benchmark: from code\nsimilarity and classiﬁcation for advances in code recommendation algorithms, and code translation\nbetween a large variety of programming languages, to advances in code performance improvement\ntechniques. We hope that the scale, diversity and rich, high-quality annotations of CodeNet will offer\nunprecedented research opportunities at the intersection of AI and Software Engineering.\n11 Acknowledgements\nWe would like to acknowledge AIZU and AtCoder for making the code submissions publicly available.\nWe would like to thank the IBM Data Asset eXchange team for providing a platform to host the\nCodeNet dataset. We would like to thank the Women in Data Science team at Stanford University\nand the IBM Call for Code team for their collaboration in launching the CodeNet challenge.\n12 Bibliography\n[1]Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. A survey of\nmachine learning for big code and naturalness. ACM Computing Surveys (CSUR) , 51(4):1–37,\n2018.\n[2]Yanming Yang, Xin Xia, David Lo, and John Grundy. A survey on deep learning for software\nengineering. arXiv preprint arXiv:2011.14597 , 2020.\n[3]Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul\nPuri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke\nChan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad\nBavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias\nPlappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-V oss, William Hebgen Guss, Alex\nNichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra,'
 'GCJ is advantageous over POJ-104 in size and language diversity, but we believe that an even\nlarger dataset such as CodeNet can better serve the community. GCJ contains neither metadata nor\ninformation on identical problems and near duplicates.\n4 CodeNet Differentiation\nTable 5: Related datasets comparison\nCodeNet GCJ POJ\nTotal number of problems 4053 332 104\nNumber of programming languages 55 20 2\nTotal number of code samples 13,916,828 2,430,000 52,000\nC++/C subset data size (code samples) 8,008,527 280,000 52,000\nPercentage of problems with test data 51% 0% 0%\nTask: Memory Consumption Prediction Yes No No\nTask: Runtime Performance Comparison Yes No No\nTask: Error Prediction Yes No No\nTask: Near duplicate prediction Yes No No\nA high quality code dataset has certain desired properties. We constructed CodeNet according to\nthese requirements. In the following, we discuss how CodeNet differentiates itself from the existing\ndatasets along these lines. Table 5 is a comparison with related datasets.\nLarge scale. A useful dataset should contain a large number and variety of data samples to expose\nthe realistic and complex landscape of data distributions one meets in practice. CodeNet is the\nlargest dataset in its class - it has approximately 10 times more code samples than GCJ and its C++\nbenchmark is approximately 10 times larger than POJ-104.\nRich annotation. For the dataset class in question, it is important to include information beyond\nwhich problem a code sample solves to enable a wide range of applications and use cases. It is useful\nto know whether a code sample solves the problem correctly, and if not, the error category (e.g.,\ncompilation error, runtime error, and out-of-memory error). Since the source code is supposed to\nsolve a programming problem, it is advantageous to know the problem statement and have a sample\ninput for execution and a sample output for validation. All such extra information is part of CodeNet\nbut absent in GCJ and POJ-104.\nClean samples. For effective machine learning, the data samples are expected to be independent\nand identically distributed (iid); otherwise, the resulting performance metric could be signiﬁcantly\ninﬂated [ 24]. The existence of duplicate and/or near duplicate code samples makes the iid assumption\ndubious. Hence, it is crucial to identify the near duplicates. The presence of identical problems in the\ndataset poses an even bigger issue. In CodeNet, we analyzed the code samples for (near) duplication\nand used clustering to ﬁnd identical problems. This information is made available as part of the\ndataset release but it is absent in GCJ and POJ-104.\n5 Construction of CodeNet\n5.1 Collection of Code Samples\nThe CodeNet dataset contains problems, submissions, and metadata, scraped from the AIZU and\nAtCoder online judging systems. For AIZU, we used the provided REST APIs to download all the\nmetadata. For AtCoder, due to the absence of a REST API, we scraped the problems, submissions,\nand metadata directly from the web pages. We considered only public and non-empty submissions\nthat did not contain errors or inconsistencies in the metadata. We manually merged the information\nfrom the two sources and adopted a uniﬁed format to create a single dataset.\n6']","CodeNet surpasses previous datasets with over 14 million code samples and around 500 million lines of code in 55 languages, providing unparalleled scale and diversity for AI for code research.",0.999999999975,0.9166666666666666,0.8936565445287323,1.0,0.5496600678114125,0.9354823765088081
What methodologies were employed in SuperGLUE to establish robust human baseline performances across its tasks?,"To establish robust human baseline performances in SuperGLUE, a two-step procedure was employed where crowd workers received training and were then asked to annotate examples from the development set. Workers were provided with instructions and an FAQ page during the training phase, and only those who annotated a minimum of five examples and achieved median performance were qualified for the annotation phase. This method ensured a systematic approach to obtaining accurate human performance estimates across all benchmark tasks.","['Table 4: Baseline performance on the SuperGLUE development.\nModel Avg BoolQ CB COPA MultiRC ReCoRD RTE WiC WSC\nMetrics Acc. Acc./F1 Acc. F1 a/EM F1/EM Acc. Acc. Acc.\nMost Frequent Class 47.7 62.2 50.0/22.2 55.0 59.9/ 0.8 32.4/31.5 52.7 50.0 63.5\nCBOW 47.7 62.4 71.4/49.6 63.0 20.3/ 0.3 14.4/13.8 54.2 55.3 61.5\nBERT 72.2 77.7 94.6/93.7 69.0 70.5/24.7 70.6/69.8 75.8 74.9 68.3\nBERT++ 74.6 80.1 96.4/95.0 78.0 70.5/24.7 70.6/69.8 82.3 74.9 68.3\nA Development Set Results\nIn Table 4, we present results of the baselines on the SuperGLUE tasks development sets.\nB Performance on GLUE Diagnostics\nFigure 2 shows the performance on the GLUE diagnostics dataset for systems submitted to the public\nleaderboard.\nDisjunction Downward Monotone Restrictivity Double Negation Prepositional Phrases60\n40\n20\n020406080\nChance\nBiLSTM+ELMo+Attn\nOpenAI GPT\nBERT + Single-task Adapters\nBERT (Large)\nBERT on STILTsBERT + BAM\nSemBERT\nSnorkel MeTaL\nALICE (Large)\nMT-DNN (ensemble)\nXLNet-Large (ensemble)\nFigure 2: Performance of GLUE submissions on selected diagnostic categories, reported using the\nR3metric scaled up by 100, as in Wang et al. (2019a, see paper for a description of the categories).\nSome initially difﬁcult categories, like double negation, saw gains from advances on GLUE, but\nothers remain hard (restrictivity) or even adversarial (disjunction, downward monotone).\nC Human Performance Estimation\nFor collecting data to establish human performance on the SuperGLUE tasks, we follow a two\nstep procedure where we ﬁrst provide some training to the crowd workers before they proceed to\nannotation. For both steps and all tasks, the average pay rate is $23.75/hr.7\nIn the training phase, workers are provided with instructions on the task, linked to an FAQ page, and\nare asked to annotate up to 30 examples from the development set. After answering each example,\nworkers are also asked to check their work against the provided ground truth label. After the training\nphase is complete, we provide the qualiﬁcation to work on the annotation phase to all workers\nwho annotated a minimum of ﬁve examples, i.e. completed ﬁve HITs during training and achieved\nperformance at, or above the median performance across all workers during training.\n7This estimate is taken from https://turkerview.com .\n16'
 'BiLSTM+ELMo+Attn\nOpenAI GPT\nBERT + Single-task Adapters\nBERT (Large)\nBERT on STILTs\nBERT + BAM\nSemBERT\nSnorkel MeTaL\nALICE (Large)\nMT-DNN (ensemble)\nXLNet-Large (ensemble)0.50.60.70.80.91.01.11.2\nGLUE Score\nHuman Performance\nCoLA\nSST-2MRPC\nSTS-B\nQQP\nMNLIQNLI\nRTE\nWNLIFigure 1: GLUE benchmark performance for submitted systems, rescaled to set human performance\nto 1.0, shown as a single number score, and broken down into the nine constituent task performances.\nFor tasks with multiple metrics, we use an average of the metrics. More information on the tasks\nincluded in GLUE can be found in Wang et al. (2019a) and in Warstadt et al. (2019, CoLA), Socher\net al. (2013, SST-2), Dolan and Brockett (2005, MRPC), Cer et al. (2017, STS-B), and Williams et al.\n(2018, MNLI), and Rajpurkar et al. (2016, the original data source for QNLI).\nremains substantial scope for improvement towards GLUE’s high-level goals, the original version of\nthe benchmark is no longer a suitable metric for quantifying such progress.\nIn response, we introduce SuperGLUE, a new benchmark designed to pose a more rigorous test of\nlanguage understanding. SuperGLUE has the same high-level motivation as GLUE: to provide a\nsimple, hard-to-game measure of progress toward general-purpose language understanding technolo-\ngies for English. We anticipate that signiﬁcant progress on SuperGLUE should require substantive\ninnovations in a number of core areas of machine learning, including sample-efﬁcient, transfer,\nmultitask, and unsupervised or self-supervised learning.\nSuperGLUE follows the basic design of GLUE: It consists of a public leaderboard built around\neight language understanding tasks, drawing on existing data, accompanied by a single-number\nperformance metric, and an analysis toolkit. However, it improves upon GLUE in several ways:\nMore challenging tasks: SuperGLUE retains the two hardest tasks in GLUE. The remaining tasks\nwere identiﬁed from those submitted to an open call for task proposals and were selected based on\ndifﬁculty for current NLP approaches.\nMore diverse task formats: The task formats in GLUE are limited to sentence- and sentence-pair\nclassiﬁcation. We expand the set of task formats in SuperGLUE to include coreference resolution\nand question answering (QA).\nComprehensive human baselines: We include human performance estimates for all benchmark\ntasks, which verify that substantial headroom exists between a strong BERT-based baseline and\nhuman performance.\nImproved code support: SuperGLUE is distributed with a new, modular toolkit for work on\npretraining, multi-task learning, and transfer learning in NLP, built around standard tools including\nPyTorch (Paszke et al., 2017) and AllenNLP (Gardner et al., 2017).\nReﬁned usage rules: The conditions for inclusion on the SuperGLUE leaderboard have been\nrevamped to ensure fair competition, an informative leaderboard, and full credit assignment to data\nand task creators.\nThe SuperGLUE leaderboard, data, and software tools are available at super.gluebenchmark.com .\n2 Related Work\nMuch work prior to GLUE demonstrated that training neural models with large amounts of available\nsupervision can produce representations that effectively transfer to a broad range of NLP tasks\n2'
 'the best known result (WiC). The outside results for COPA, MultiRC, and RTE are from Sap et al.\n(2019), Trivedi et al. (2019), and Liu et al. (2019d) respectively.\nHuman Performance Pilehvar and Camacho-Collados (2019), Khashabi et al. (2018), Nangia and\nBowman (2019), and Zhang et al. (2018) respectively provide estimates for human performance\non WiC, MultiRC, RTE, and ReCoRD. For the remaining tasks, including the diagnostic set, we\nestimate human performance by hiring crowdworker annotators through Amazon’s Mechanical Turk\nplatform to reannotate a sample of each test set. We follow a two step procedure where a crowd\nworker completes a short training phase before proceeding to the annotation phase, modeled after the\nmethod used by Nangia and Bowman (2019) for GLUE. See Appendix C for details.\n5.2 Results\nTable 3 shows results for all baselines. The most frequent class and CBOW baselines do not perform\nwell overall, achieving near chance performance for several of the tasks. Using BERT increases\nthe average SuperGLUE score by 25 points, attaining signiﬁcant gains on all of the benchmark\ntasks, particularly MultiRC, ReCoRD, and RTE. On WSC, BERT actually performs worse than\nthe simple baselines, likely due to the small size of the dataset and the lack of data augmentation.\nUsing MultiNLI as an additional source of supervision for BoolQ, CB, and RTE leads to a 2-5 point\nimprovement on all tasks. Using SWAG as a transfer task for COPA sees an 8 point improvement.\nOur best baselines still lag substantially behind human performance. On average, there is a nearly 20\npoint gap between BERT++ and human performance. The largest gap is on WSC, with a 35 point\ndifference between the best model and human performance. The smallest margins are on BoolQ,\nCB, RTE, and WiC, with gaps of around 10 points on each of these. We believe these gaps will be\nchallenging to close: On WSC and COPA, human performance is perfect. On three other tasks, it is\nin the mid-to-high 90s. On the diagnostics, all models continue to lag signiﬁcantly behind humans.\nThough all models obtain near perfect gender parity scores on Winogender, this is due to the fact that\nthey are obtaining accuracy near that of random guessing.\n6 Conclusion\nWe present SuperGLUE, a new benchmark for evaluating general-purpose language understanding\nsystems. SuperGLUE updates the GLUE benchmark by identifying a new set of challenging NLU\ntasks, as measured by the difference between human and machine baselines. The set of eight tasks in\nour benchmark emphasizes diverse task formats and low-data training data tasks, with nearly half the\ntasks having fewer than 1k examples and all but one of the tasks having fewer than 10k examples.\nWe evaluate BERT-based baselines and ﬁnd that they still lag behind humans by nearly 20 points.\nGiven the difﬁculty of SuperGLUE for BERT, we expect that further progress in multi-task, transfer,\nand unsupervised/self-supervised learning techniques will be necessary to approach human-level per-\nformance on the benchmark. Overall, we argue that SuperGLUE offers a rich and challenging testbed\nfor work developing new general-purpose machine learning methods for language understanding.\n7 Acknowledgments\nWe thank the original authors of the included datasets in SuperGLUE for their cooperation in the\ncreation of the benchmark, as well as those who proposed tasks and datasets that we ultimately\ncould not include. This work was made possible in part by a donation to NYU from Eric and Wendy\nSchmidt made by recommendation of the Schmidt Futures program. We gratefully acknowledge\nthe support of the NVIDIA Corporation with the donation of a Titan V GPU used at NYU for this\nresearch, and funding from DeepMind for the hosting of the benchmark platform. AW is supported\nby the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE\n1342536. Any opinions, ﬁndings, and conclusions or recommendations expressed in this material are'
 'SuperGLUE: A Stickier Benchmark for\nGeneral-Purpose Language Understanding Systems\nAlex Wang∗\nNew York UniversityYada Pruksachatkun∗\nNew York UniversityNikita Nangia∗\nNew York University\nAmanpreet Singh∗\nFacebook AI ResearchJulian Michael\nUniversity of WashingtonFelix Hill\nDeepMindOmer Levy\nFacebook AI Research\nSamuel R. Bowman\nNew York University\nAbstract\nIn the last year, new models and methods for pretraining and transfer learning have\ndriven striking performance improvements across a range of language understand-\ning tasks. The GLUE benchmark, introduced a little over one year ago, offers\na single-number metric that summarizes progress on a diverse set of such tasks,\nbut performance on the benchmark has recently surpassed the level of non-expert\nhumans, suggesting limited headroom for further research. In this paper we present\nSuperGLUE, a new benchmark styled after GLUE with a new set of more difﬁ-\ncult language understanding tasks, a software toolkit, and a public leaderboard.\nSuperGLUE is available at super.gluebenchmark.com .\n1 Introduction\nRecently there has been notable progress across many natural language processing (NLP) tasks, led\nby methods such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT\n(Devlin et al., 2019). The unifying theme of these methods is that they couple self-supervised learning\nfrom massive unlabelled text corpora with effective adapting of the resulting model to target tasks.\nThe tasks that have proven amenable to this general approach include question answering, textual\nentailment, and parsing, among many others (Devlin et al., 2019; Kitaev et al., 2019, i.a.).\nIn this context, the GLUE benchmark (Wang et al., 2019a) has become a prominent evaluation\nframework for research towards general-purpose language understanding technologies. GLUE is\na collection of nine language understanding tasks built on existing public datasets, together with\nprivate test data, an evaluation server, a single-number target metric, and an accompanying expert-\nconstructed diagnostic set. GLUE was designed to provide a general-purpose evaluation of language\nunderstanding that covers a range of training data volumes, task genres, and task formulations. We\nbelieve it was these aspects that made GLUE particularly appropriate for exhibiting the transfer-\nlearning potential of approaches like OpenAI GPT and BERT.\nThe progress of the last twelve months has eroded headroom on the GLUE benchmark dramatically.\nWhile some tasks (Figure 1) and some linguistic phenomena (Figure 2 in Appendix B) measured\nin GLUE remain difﬁcult, the current state of the art GLUE Score as of early July 2019 (88.4 from\nYang et al., 2019) surpasses human performance (87.1 from Nangia and Bowman, 2019) by 1.3\npoints, and in fact exceeds this human performance estimate on four tasks. Consequently, while there\n∗Equal contribution. Correspondence: glue-benchmark-admin@googlegroups.com\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.arXiv:1905.00537v3  [cs.CL]  13 Feb 2020']","SuperGLUE's human baseline performances were established through careful collection of non-expert annotations for a sample of each test set, following structured annotation procedures that included a short training phase for annotators, ensuring reliable estimates of human performance.",0.9999999999666667,0.8888888888888888,0.9610510333810846,1.0,0.6115358844232428,0.9461349344823577
How do the selected tasks in SuperGLUE reflect the benchmark's goals for advancing language understanding technologies?,"The tasks in SuperGLUE are designed to test a system's ability to understand and reason about texts in English, reflecting its goal of advancing language understanding technologies. They are intentionally chosen to be more challenging than those in GLUE, aiming to exceed the capabilities of current state-of-the-art systems while still being solvable by college-educated English speakers. This ensures that SuperGLUE serves as a rigorous benchmark that encourages significant innovations in machine learning methodologies.","['SuperGLUE: A Stickier Benchmark for\nGeneral-Purpose Language Understanding Systems\nAlex Wang∗\nNew York UniversityYada Pruksachatkun∗\nNew York UniversityNikita Nangia∗\nNew York University\nAmanpreet Singh∗\nFacebook AI ResearchJulian Michael\nUniversity of WashingtonFelix Hill\nDeepMindOmer Levy\nFacebook AI Research\nSamuel R. Bowman\nNew York University\nAbstract\nIn the last year, new models and methods for pretraining and transfer learning have\ndriven striking performance improvements across a range of language understand-\ning tasks. The GLUE benchmark, introduced a little over one year ago, offers\na single-number metric that summarizes progress on a diverse set of such tasks,\nbut performance on the benchmark has recently surpassed the level of non-expert\nhumans, suggesting limited headroom for further research. In this paper we present\nSuperGLUE, a new benchmark styled after GLUE with a new set of more difﬁ-\ncult language understanding tasks, a software toolkit, and a public leaderboard.\nSuperGLUE is available at super.gluebenchmark.com .\n1 Introduction\nRecently there has been notable progress across many natural language processing (NLP) tasks, led\nby methods such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT\n(Devlin et al., 2019). The unifying theme of these methods is that they couple self-supervised learning\nfrom massive unlabelled text corpora with effective adapting of the resulting model to target tasks.\nThe tasks that have proven amenable to this general approach include question answering, textual\nentailment, and parsing, among many others (Devlin et al., 2019; Kitaev et al., 2019, i.a.).\nIn this context, the GLUE benchmark (Wang et al., 2019a) has become a prominent evaluation\nframework for research towards general-purpose language understanding technologies. GLUE is\na collection of nine language understanding tasks built on existing public datasets, together with\nprivate test data, an evaluation server, a single-number target metric, and an accompanying expert-\nconstructed diagnostic set. GLUE was designed to provide a general-purpose evaluation of language\nunderstanding that covers a range of training data volumes, task genres, and task formulations. We\nbelieve it was these aspects that made GLUE particularly appropriate for exhibiting the transfer-\nlearning potential of approaches like OpenAI GPT and BERT.\nThe progress of the last twelve months has eroded headroom on the GLUE benchmark dramatically.\nWhile some tasks (Figure 1) and some linguistic phenomena (Figure 2 in Appendix B) measured\nin GLUE remain difﬁcult, the current state of the art GLUE Score as of early July 2019 (88.4 from\nYang et al., 2019) surpasses human performance (87.1 from Nangia and Bowman, 2019) by 1.3\npoints, and in fact exceeds this human performance estimate on four tasks. Consequently, while there\n∗Equal contribution. Correspondence: glue-benchmark-admin@googlegroups.com\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.arXiv:1905.00537v3  [cs.CL]  13 Feb 2020'
 'Table 2: Development set examples from the tasks in SuperGLUE. Bold text represents part of the\nexample format for each task. Text in italics is part of the model input. Underlined text is specially\nmarked in the input. Text in a monospaced font represents the expected model output.BoolQPassage: Barq’s – Barq’s is an American soft drink. Its brand of root beer is notable for having caffeine.\nBarq’s, created by Edward Barq and bottled since the turn of the 20th century, is owned by the Barq\nfamily but bottled by the Coca-Cola Company. It was known as Barq’s Famous Olde Tyme Root Beer\nuntil 2012.\nQuestion: is barq’s root beer a pepsi product Answer: NoCBText: B: And yet, uh, I we-, I hope to see employer based, you know, helping out. You know, child, uh,\ncare centers at the place of employment and things like that, that will help out. A: Uh-huh. B: What do\nyou think, do you think we are, setting a trend?\nHypothesis: they are setting a trend Entailment: UnknownCOPAPremise: My body cast a shadow over the grass. Question: What’s the CAUSE for this?\nAlternative 1: The sun was rising. Alternative 2: The grass was cut.\nCorrect Alternative: 1MultiRCParagraph: Susan wanted to have a birthday party. She called all of her friends. She has ﬁve friends.\nHer mom said that Susan can invite them all to the party. Her ﬁrst friend could not go to the party\nbecause she was sick. Her second friend was going out of town. Her third friend was not so sure if her\nparents would let her. The fourth friend said maybe. The ﬁfth friend could go to the party for sure. Susan\nwas a little sad. On the day of the party, all ﬁve friends showed up. Each friend had a present for Susan.\nSusan was happy and sent each friend a thank you card the next week\nQuestion: Did Susan’s sick friend recover? Candidate answers: Yes, she recovered (T),No(F),Yes\n(T),No, she didn’t recover (F),Yes, she was at Susan’s party (T)ReCoRDParagraph: (CNN )Puerto Rico on Sunday overwhelmingly voted for statehood. But Congress, the only\nbody that can approve new states, will ultimately decide whether the status of the UScommonwealth\nchanges. Ninety-seven percent of the votes in the nonbinding referendum favored statehood, an increase\nover the results of a 2012 referendum, ofﬁcial results from the State Electorcal Commission show. It\nwas the ﬁfth such vote on statehood. ""Today, we the people of Puerto Rico are sending a strong and\nclear message to the US Congress ... and to the world ... claiming our equal rights as American citizens,\nPuerto Rico Gov. Ricardo Rossello said in a news release. @highlight Puerto Rico voted Sunday in\nfavor of US statehood\nQuery For one, they can truthfully say, “Don’t blame me, I didn’t vote for them, ” when discussing the\n<placeholder> presidency Correct Entities: USRTEText: Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44,\naccording to the Christopher Reeve Foundation.\nHypothesis: Christopher Reeve had an accident. Entailment: FalseWiCContext 1: Room and board .Context 2: He nailed boards across the windows.\nSense match: FalseWSCText: Mark told Pete many lies about himself, which Pete included in his book. Heshould have been\nmore truthful. Coreference: False\n3 SuperGLUE Overview\n3.1 Design Process\nThe goal of SuperGLUE is to provide a simple, robust evaluation metric of any method capable of\nbeing applied to a broad range of language understanding tasks. To that end, in designing SuperGLUE,\nwe identify the following desiderata of tasks in the benchmark:\nTask substance: Tasks should test a system’s ability to understand and reason about texts in English.\nTask difﬁculty: Tasks should be beyond the scope of current state-of-the-art systems, but solvable by\nmost college-educated English speakers. We exclude tasks that require domain-speciﬁc knowledge,\ne.g. medical notes or scientiﬁc papers.\nEvaluability: Tasks must have an automatic performance metric that corresponds well to human'
 'BiLSTM+ELMo+Attn\nOpenAI GPT\nBERT + Single-task Adapters\nBERT (Large)\nBERT on STILTs\nBERT + BAM\nSemBERT\nSnorkel MeTaL\nALICE (Large)\nMT-DNN (ensemble)\nXLNet-Large (ensemble)0.50.60.70.80.91.01.11.2\nGLUE Score\nHuman Performance\nCoLA\nSST-2MRPC\nSTS-B\nQQP\nMNLIQNLI\nRTE\nWNLIFigure 1: GLUE benchmark performance for submitted systems, rescaled to set human performance\nto 1.0, shown as a single number score, and broken down into the nine constituent task performances.\nFor tasks with multiple metrics, we use an average of the metrics. More information on the tasks\nincluded in GLUE can be found in Wang et al. (2019a) and in Warstadt et al. (2019, CoLA), Socher\net al. (2013, SST-2), Dolan and Brockett (2005, MRPC), Cer et al. (2017, STS-B), and Williams et al.\n(2018, MNLI), and Rajpurkar et al. (2016, the original data source for QNLI).\nremains substantial scope for improvement towards GLUE’s high-level goals, the original version of\nthe benchmark is no longer a suitable metric for quantifying such progress.\nIn response, we introduce SuperGLUE, a new benchmark designed to pose a more rigorous test of\nlanguage understanding. SuperGLUE has the same high-level motivation as GLUE: to provide a\nsimple, hard-to-game measure of progress toward general-purpose language understanding technolo-\ngies for English. We anticipate that signiﬁcant progress on SuperGLUE should require substantive\ninnovations in a number of core areas of machine learning, including sample-efﬁcient, transfer,\nmultitask, and unsupervised or self-supervised learning.\nSuperGLUE follows the basic design of GLUE: It consists of a public leaderboard built around\neight language understanding tasks, drawing on existing data, accompanied by a single-number\nperformance metric, and an analysis toolkit. However, it improves upon GLUE in several ways:\nMore challenging tasks: SuperGLUE retains the two hardest tasks in GLUE. The remaining tasks\nwere identiﬁed from those submitted to an open call for task proposals and were selected based on\ndifﬁculty for current NLP approaches.\nMore diverse task formats: The task formats in GLUE are limited to sentence- and sentence-pair\nclassiﬁcation. We expand the set of task formats in SuperGLUE to include coreference resolution\nand question answering (QA).\nComprehensive human baselines: We include human performance estimates for all benchmark\ntasks, which verify that substantial headroom exists between a strong BERT-based baseline and\nhuman performance.\nImproved code support: SuperGLUE is distributed with a new, modular toolkit for work on\npretraining, multi-task learning, and transfer learning in NLP, built around standard tools including\nPyTorch (Paszke et al., 2017) and AllenNLP (Gardner et al., 2017).\nReﬁned usage rules: The conditions for inclusion on the SuperGLUE leaderboard have been\nrevamped to ensure fair competition, an informative leaderboard, and full credit assignment to data\nand task creators.\nThe SuperGLUE leaderboard, data, and software tools are available at super.gluebenchmark.com .\n2 Related Work\nMuch work prior to GLUE demonstrated that training neural models with large amounts of available\nsupervision can produce representations that effectively transfer to a broad range of NLP tasks\n2'
 'Table 1: The tasks included in SuperGLUE. WSD stands for word sense disambiguation, NLI is\nnatural language inference, coref. is coreference resolution, and QAis question answering. For\nMultiRC, we list the number of total answers for 456/83/166 train/dev/test questions.\nCorpus |Train | | Dev | | Test |Task Metrics Text Sources\nBoolQ 9427 3270 3245 QA acc. Google queries, Wikipedia\nCB 250 57 250 NLI acc./F1 various\nCOPA 400 100 500 QA acc. blogs, photography encyclopedia\nMultiRC 5100 953 1800 QA F1 a/EM various\nReCoRD 101k 10k 10k QA F1/EM news (CNN, Daily Mail)\nRTE 2500 278 300 NLI acc. news, Wikipedia\nWiC 6000 638 1400 WSD acc. WordNet, VerbNet, Wiktionary\nWSC 554 104 146 coref. acc. ﬁction books\n(Collobert and Weston, 2008; Dai and Le, 2015; Kiros et al., 2015; Hill et al., 2016; Conneau and\nKiela, 2018; McCann et al., 2017; Peters et al., 2018). GLUE was presented as a formal challenge\naffording straightforward comparison between such task-agnostic transfer learning techniques. Other\nsimilarly-motivated benchmarks include SentEval (Conneau and Kiela, 2018), which speciﬁcally\nevaluates ﬁxed-size sentence embeddings, and DecaNLP (McCann et al., 2018), which recasts a set\nof target tasks into a general question-answering format and prohibits task-speciﬁc parameters. In\ncontrast, GLUE provides a lightweight classiﬁcation API and no restrictions on model architecture or\nparameter sharing, which seems to have been well-suited to recent work in this area.\nSince its release, GLUE has been used as a testbed and showcase by the developers of several\ninﬂuential models, including GPT (Radford et al., 2018) and BERT (Devlin et al., 2019). As shown\nin Figure 1, progress on GLUE since its release has been striking. On GLUE, GPT and BERT\nachieved scores of 72.8 and 80.2 respectively, relative to 66.5 for an ELMo-based model (Peters\net al., 2018) and 63.7 for the strongest baseline with no multitask learning or pretraining above the\nword level. Recent models (Liu et al., 2019d; Yang et al., 2019) have clearly surpassed estimates of\nnon-expert human performance on GLUE (Nangia and Bowman, 2019). The success of these models\non GLUE has been driven by ever-increasing model capacity, compute power, and data quantity, as\nwell as innovations in model expressivity (from recurrent to bidirectional recurrent to multi-headed\ntransformer encoders) and degree of contextualization (from learning representation of words in\nisolation to using uni-directional contexts and ultimately to leveraging bidirectional contexts).\nIn parallel to work scaling up pretrained models, several studies have focused on complementary\nmethods for augmenting performance of pretrained models. Phang et al. (2018) show that BERT can\nbe improved using two-stage pretraining, i.e., ﬁne-tuning the pretrained model on an intermediate\ndata-rich supervised task before ﬁne-tuning it again on a data-poor target task. Liu et al. (2019d,c) and\nBach et al. (2018) get further improvements respectively via multi-task ﬁnetuning and using massive\namounts of weak supervision. Clark et al. (2019b) demonstrate that knowledge distillation (Hinton\net al., 2015; Furlanello et al., 2018) can lead to student networks that outperform their teachers.\nOverall, the quantity and quality of research contributions aimed at the challenges posed by GLUE\nunderline the utility of this style of benchmark for machine learning researchers looking to evaluate\nnew application-agnostic methods on language understanding.\nLimits to current approaches are also apparent via the GLUE suite. Performance on the GLUE\ndiagnostic entailment dataset, at 0.42 R3, falls far below the average human performance of 0.80\nR3reported in the original GLUE publication, with models performing near, or even below, chance\non some linguistic phenomena (Figure 2, Appendix B). While some initially difﬁcult categories\nsaw gains from advances on GLUE (e.g., double negation), others remain hard (restrictivity) or']","The selected tasks, chosen for their difficulty and diversity, reflect SuperGLUE's goals by requiring substantive innovations in machine learning to improve sample-efficient transfer, multitask, and unsupervised learning, driving forward the capabilities of language understanding systems.",0.999999999975,1.0,0.8870559367266515,1.0,0.7609441231650749,0.9437764926602997
"In the context of PAL, what role do meaningful variable names play in the generated program's effectiveness?","Meaningful variable names in PAL prompts enhance the model's ability to ground variables to the entities they represent, improving reasoning and comprehension. Removing meaningful variable names leads to a significant drop in performance, indicating that they are crucial for effective code quality and understanding. Thus, while the Python interpreter does not require them, meaningful names aid the model's reasoning process significantly.","['PAL: Program-aided Language Models 13\nA. Alternative Prompts without Meaningful Variable Names\na = 23\nb = 5\nc = 3\nd = b*c\ne = a - d\nprint(e)\n(a) Structured explanation with uninformative variable names (P AL -var)\n# Olivia has $23\na = 23\n# number of bagels bought\nb = 5\n# price of each bagel\nc = 3\n# total price of bagels\nd = b*c\n# money left\ne = a - d\nprint(e)\n(b) Structured explanation with uninformative variable names, but useful comments (P AL -var +comms )\nmoneyinitial = 23\nbagels = 5\nbagelcost = 3\nmoneyspent = bagels *bagelcost\nmoneyleft = money initial - money spent\nresult = money left\nprint(result)\n(c) P AL prompts\nFigure 10: Role of text in P AL: three different reasoning steps for the question Olivia has $23. She bought ﬁve bagels for\n$3 each. How much money does she have left? Uninformative variable names (left), Uninformative variable names with\nuseful comments (left), and P AL. Including text description\nSetting C OT P AL -var PAL -var +comms PAL\nSolve Rate 63.1 59.0 69.0 71.8\nTable 4: Role of text: including text either as informative variable names ( PAL) or comments is important ( PAL-var +\ncomms ). Uninformative variable names PAL-var cause a drastic drop in performance, indicating that just structure is not\nsufﬁcient. The corresponding prompts are shown in Figure 10.\nFor mathematical problems, since our standard prompts do not use much comment, we start by creating alternative prompts\nwhere the informative variable names are replaced with single-letters (Figure 10). The results in Table 4 shows a considerable\nperformance drop: from an average of 71.8% to 59%. Note that the ablation where structured outputs are completely\nremoved in favor of purely text explanations is precisely the COTsetting, which achieves a solve rate of 63%. These results\nunderscore the importance of text but more importantly show that combining both text and procedural statements leads to\nhigher performance gains—either is sub-optimal.\nB. Additional analysis on Arithmetic Reasoning\nGSM-hard with hard prompts The GSM-HARD experiments used prompts that were sampled from the GSM8Ktraining\nset. Will COTbe helped by using larger numbers in the prompts as well? To investigate this, we create prompts where the\nnumbers are changed to larger numbers, matching the distribution of numbers in GSM -HARD . The results in Table 5 shows'
 'PAL: Program-aided Language Models 12\nPart I\nAppendix\nTable of Contents\nA Alternative Prompts without Meaningful Variable Names 13\nB Additional analysis on Arithmetic Reasoning 13\nC Effect of Using Language Models of Code 14\nD Analyzing the Effect of Increasing Number of Samples on P AL 14\nE Standard Deviations Across Multiple Order of Prompts 17\nF P AL Beyond Benchmarks 17\nG Closer Look into Token-level Behaviors of Different Mechanisms 20\nH Datasets 20\nH.1 Creating GSM -HARD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nH.2 GSM -HARD Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nI Generalization of PAL to Least-to-Most Prompting 24\nJ Prompts 26\nJ.1 Reasoning about Colored Objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\nJ.2 Penguins in a Table . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\nJ.3 Date Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\nJ.4 Math . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\nJ.5 Object Counting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\nJ.6 Repeat Copy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\nK Success and Failure Modes in Symbolic Tasks 33\nK.1 Colored Objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\nK.2 Penguins in a Table . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\nK.3 Date Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34'
 'PAL: Program-aided Language Models 14\nthat even with a prompt that matches the numbers, there are only modest gains in performance. These results show that the\ngains achieved by using code-based reasoning chains may not be achieved simply by using better few-shot examples for\nCOT.\nRegular Prompt Prompt with Larger Numbers\nCOT 23.3 23.8\nTable 5: GSM-hard results, when the prompts also had examples of larger numbers.\nSuccinct Code The programs used in few-shot examples by PALare multi-step, and show a step-by-step breakdown of\nthe reasoning process. Is this breakdown necessary? Alternatively, can we return a single line expression (see Figure 11b) to\ncalculate the result? Results in Table 6 ( 4throw) shows that is not the case. With single-line expressions, the performance of\nPAL falls to the level of direct prompting.\nGenerating the answer directly PALﬁrst generates a reasoning chain in the form of a Python program, and passes the\ngenerated program to a runtime to obtain an answer. Is PALbetter only because of the program-style intermediate reasoning\nchains, or are the improvements derived from ofﬂoading execution to the Python runtime? To investigate this, we experiment\nwith a variant that forces the LLM to generate the answer after generating the reasoning chain (Figure 11e). This setting\ncompels the LLM to condition on the generated code-based reasoning to generate an answer, simulating the runtime. The\nresults in Table 6 ( 5throw) show that the solve rate drops to near DIRECT levels. This reinforces our hypothesis that while\ncurrent LLMs can be excellent at specifying a high-level plan to solve a task—they are still incapable of executing them.\nAblation Solve Rate\nDIRECT (no intermediate reasoning) 19.7\nCOT 65.6\nPAL 72.0\nSuccinct Code 47.8\nLLM Simulating Runtime 23.2\nTable 6: Solve Rates for Ablations\nC. Effect of Using Language Models of Code\nIn our experiments, we focused on evaluating the performance of a language model for code. We aimed to investigate\nwhether the additional performance boost observed in our results was due to the use of models like Codex, or whether our\nformulation was useful even for text-based models. To this end, we conducted additional experiments using text-based\nlanguage models. Our ﬁndings indicate that the PAL approach is not restricted to working solely with Codex, but can also\nbe applied to natural language (NL) models, as long as the model is sufﬁciently strong. Speciﬁcally, our results showed that\nin the text-davinci-001 model, the use of the CoT approach resulted in better performance.\nModel CoT PaL\ntext-davinci-001 26.5 8.6\ntext-davinci-002 46.9 65.8\ntext-davinci-003 65.3 69.8\nD. Analyzing the Effect of Increasing Number of Samples on P AL\nIn Section 5.1, we show that PALoutperforms strong baselines both for a single sample and by drawing 40 samples and\nusing majority voting. Figure 12 illustrates the trends for cases when the number of samples drawn are between 1 and 40,\nand the interpolation estimates demonstrate that P AL remains competitive throughout the number of samples.'
 'PAL: Program-aided Language Models 7\ncode-cushman-001 code-davinci-001 code-davinci-002020406080\n21.731.872.0\n19.126.060.1\n13.6%22.3%19.8%Solve ratePAL\nCOT\nRelative Improvement\nFigure 7: PALwith different models on GSM8K: though\nthe absolute accuracies with code-cushman-001\nandcode-davinci-001 are lower than\ncode-davinci-002 , the relative improvement of\nPAL over C OT is consistent across models.text-davinci-001 text-davinci-002 text-davinci-003020406080\n26.546.965.3\n8.665.869.8 COT PAL\nFigure 8: PALwith NL LMs on GSM 8K: though\nCOToutperforms PALwithtext-davinci-001 , once\nthe base LM is sufﬁciently strong, PALis beneﬁcial\nwithtext-davinci-002 andtext-davinci-003\nas well. That is, P AL is not limited to code-LMs only.\n6. Analysis\nDoes P AL work with weaker LMs? In all our experi-\nments in Section 5, PALused thecode-davinci-002\nmodel; but can PALwork with weaker models of code? We\ncompared PALwith COTwhen both prompting approaches\nuse the same weaker base LMs code-cushman-001\nandcode-davinci-001 . As shown in Figure 7, even\nthough the absolute accuracies of code-cushman-001\nandcode-davinci-001 are lower, the relative improve-\nment of PALover COTremains consistent across models.\nThis shows that PALcan work with weaker models, while\nits beneﬁt scales elegantly to stronger models as well.\nDoes P AL work with LMs of natural language? We\nalso experimented with PALusing thetext-davinci\nseries. Figure 8 shows the following interesting re-\nsults: when the base LM’s “code modeling ability” is\nweak (using text-davinci-001 ),COTperforms better\nthan PAL. However, once the LM’s code modeling abil-\nity is sufﬁciently high (using text-davinci-002 and\ntext-davinci-003 ),PALoutperforms COT, and PAL\ntext-davinci-003 performs almost as PALcode-davinci-002 .\nThis shows that PALis not limited to LMs of code, but it\ncan work with LMs that were mainly trained for natural\nlanguage, if they have a sufﬁciently high coding ability.\nIs P AL better because of the Python prompt or because\nof the interpreter? We experimented with generating\nPython code, while requiring the neural LM to “execute” it\nas well, without using an interpreter, following Nye et al.\n(2021); Madaan et al. (2022). We created prompts that are\nsimilar to PAL’s, except that they do include the ﬁnal answer.\nThis resulted in a 23.2 solve rate on GSM 8K, much lower\nthan PAL(72.0), and only 4.5 points higher than DIRECT .\nThese results reinforce our hypothesis that the main beneﬁt\nofPALcomes from the synergy with the interpreter, andnot only from having a better prompt. Additional details\nare provided in Appendix B. For additional discussion on\nthe advantages of code-prompts over textual-prompts, see\nAppendix G.\nDo variable names matter? In all our experiments, we\nused meaningful variable names in the PALprompts, to ease\nthe model’s grounding of variables to the entities they rep-\nresent. For the Python interpreter, however, variable names\nare meaningless. To measure the importance of meaningful\nvariable names, we experimented with two prompts variants:\n1. P AL−comment – the PALprompt without intermediate\nNL comments.\n2. P AL−var\n−comment – the PALprompt without intermediate\nNL comments and with variable names substituted\nwith random characters.\nThe results are shown in Figure 9. In COLORED OBJECTED\nandDATE, removing intermediate NL comments but keep-\ning meaningful variable names ( PAL−comment ) – slightly re-\nduces the results compared to the full PALprompt, but it still\nachieves higher accuracy than the baselines COT. Remov-\ning variable names as well ( PAL−var\n−comment ) further decreases\naccuracy, and performs worse than COT. Since variable\nnames have an important part in code quality (Gellenbeck\n& Cook, 1991; Takang et al., 1996), meaningful variable\nnames are only expected to ease reasoning for Codex, which\nwas trained on mostly meaningful names, as was also found\nby Madaan et al. (2022).\n7. Related Work\nPrompting Few-shot prompting (Brown et al., 2020) has']","Meaningful variable names in PAL's generated code are crucial for linking programmatic steps to natural language problem entities, enhancing readability and maintainability of the code, and improving the model's reasoning and execution accuracy.",0.8055555555287036,1.0,0.934987236278006,1.0,0.6864426615535654,0.9457706462142618
How does PAL address the execution of complex computations in natural language processing tasks?,"PAL addresses complex computations by offloading the solution step to a Python interpreter, while using a large language model (LLM) to decompose natural language problems into programmatic steps. This approach ensures that calculations and some reasoning are handled correctly, as the interpreter executes the generated Python code. As a result, PAL achieves higher accuracy in arithmetic and reasoning tasks compared to existing LLM-based methods.","['PAL: Program-aided Language Models 9\n8. Conclusion\nWe introduce PAL, a new method for natural language rea-\nsoning, using programs as intermediate reasoning steps.\nDifferently from existing LM-based reasoning approaches,\nthe main idea is to ofﬂoad solving and calculating to an\nexternal Python interpreter, instead of using the LLM for\nboth understanding the problem andsolving. This results\nin a ﬁnal answer that is guaranteed to be accurate, given the\ncorrectly predicted programmatic steps. We demonstrate\nthis seamless synergy between an LLM and a Python in-\nterpreter across 13 tasks from BIG-Bench Hard and other\nbenchmarks. In all these benchmarks, PALoutperforms\nlarger LLMs such as PaLM-540 Bwhich use the popular\n“chain-of-thought” method and sets new state-of-the-art ac-\ncuracy on all of them. We believe that these results unlock\nexciting directions for future neuro-symbolic AI reasoners.\nReferences\nAhn, M., Brohan, A., Brown, N., Chebotar, Y ., Cortes, O.,\nDavid, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman,\nK., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B.,\nIrpan, A., Jang, E., Ruano, R. J., Jeffrey, K., Jesmonth,\nS., Joshi, N. J., Julian, R., Kalashnikov, D., Kuang, Y .,\nLee, K.-H., Levine, S., Lu, Y ., Luu, L., Parada, C., Pastor,\nP., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D.,\nSermanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke,\nV ., Xia, F., Xiao, T., Xu, P., Xu, S., Yan, M., and Zeng,\nA. Do as I Can, not as I Say: Grounding Language in\nRobotic Affordances. arXiv preprint arXiv:2204.01691 ,\n2022.\nAmini, A., Gabriel, S., Lin, S., Koncel-Kedziorski, R., Choi,\nY ., and Hajishirzi, H. MathQA: Towards Interpretable\nMath Word Problem Solving with Operation-Based For-\nmalisms. In ACL, 2019.\nAndor, D., He, L., Lee, K., and Pitler, E. Giving bert a cal-\nculator: Finding operations and arguments with reading\ncomprehension. arXiv preprint arXiv:1909.00109 , 2019.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,\nHenighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,\nJ., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,\nGray, S., Chess, B., Clark, J., Berner, C., McCandlish,\nS., Radford, A., Sutskever, I., and Amodei, D. Language\nModels are Few-Shot Learners. In NeurIPS , 2020.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y ., Joseph, N., Brockman,\nG., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf,\nH., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N.,\nPavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter,C., Tillet, P., Such, F. P., Cummings, D., Plappert, M.,\nChantzis, F., Barnes, E., Herbert-V oss, A., Guss, W. H.,\nNichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin,\nI., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr,\nA. N., Leike, J., Achiam, J., Misra, V ., Morikawa, E.,\nRadford, A., Knight, M., Brundage, M., Murati, M.,\nMayer, K., Welinder, P., McGrew, B., Amodei, D., Mc-\nCandlish, S., Sutskever, I., and Zaremba, W. Evaluating\nLarge Language Models Trained on Code. arXiv preprint\narXiv:2107.03374 , 2021a.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y ., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374 , 2021b.\nChen, W., Ma, X., Wang, X., and Cohen, W. W. Program\nof thoughts prompting: Disentangling computation from\nreasoning for numerical reasoning tasks. arXiv preprint\narXiv:2211.12588 , 2022.\nCheng, Z., Xie, T., Shi, P., Li, C., Nadkarni, R., Hu, Y .,\nXiong, C., Radev, D., Ostendorf, M., Zettlemoyer, L.,\nSmith, N. A., and Yu, T. Binding language models in\nsymbolic languages. arXiv preprint arXiv:2210.02875 ,\n2022.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton,\nC., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,'
 'PAL: Program-aided Language Models 8\nColored Objects Date Penguins60708090100\n84.4\n64.879.295.2\n76.293.391.1\n69.191.3\n79.9\n63.491.9COT PAL PAL−comment PAL−var\n−comment\nFigure 9: Ablation study of PALprompt formats. We consider the original PALprompt, it with natural language comments\nremoved ( PAL−comment ), and further variable names replaced with random character ( PAL−var\n−comment ). As a reference, we also\nshow the C OT performance (blue).\n2021; Reif et al., 2021; Wei et al., 2021; Sanh et al., 2021)\nto code-generation (Chen et al., 2021b). Methods such as\nchain-of-thought prompting ( COT) have further unlocked a\nvariety of reasoning tasks, boosting the performance of mod-\nels on a variety of benchmarks. Nevertheless, all previous\napproaches suffer from inaccuracy in arithmetic calculation\nand incorrect reasoning (Lewkowycz et al., 2022; Hendrycks\net al., 2021; Madaan & Yazdanbakhsh, 2022). PALavoids\nthese problems by ofﬂoading the calculation and some of\nthe reasoning to a Python interpreter, which is correct by\nconstruction, given the right program. Further, not only\nthatPALcan improve the standard chain-of-thought, it can\nimprove least-to-most prompting (Zhou et al., 2022) as well,\nas we show in Appendix I.\nLMs with external tools Several prior works have\nequipped neural models with specialized modules. For ex-\nample, Cobbe et al. (2021) employ a calculator for arith-\nmetic operations as a post hoc processing, and Demeter\n& Downey (2020) add specialized modules for generating\ncities and dates. Unlike these works, PALgenerates code\nfor a Python interpreter, which is general enough to handle\nboth arithmetic calculations and dates, without specialized\nmodules and ad-hoc ﬁxes. Chowdhery et al. (2022) and Wei\net al. (2022) have also experimented with external calcula-\ntors; however, the calculator had improved Codex by only\n2.3% (absolute) on GSM 8Kand improved PaLM-540 Bby\n1.7%, while PALimproves Codex by 6.4% on the same\nbenchmark (Section 5.1). Similarly to our work, Chowd-\nhery et al. (2022) have also experimented with generating\nPython code for solving the GSM 8Kbenchmark, but their\nexperiments resulted in lower accuracy than the standard\nPaLM-540 Bthat uses chain-of-thought. Pi et al. (2022)\npretrain the model on execution results of random expres-\nsions on a calculator, instead of using the solver at test time\nas well. While their model can hypothetically perform arith-\nmetic better than other pretrained LMs, their results on the\nSV AMP benchmark are much lower: 57.4% using a T5-11Bmodel, while PAL achieves 79.4% on the same benchmark\nwithout any specialized pretraining.\nShortly after a preprint of our work was submitted to arXiv,\nanother related work on “program of thought prompting”\n(Chen et al., 2022) was also submitted to arXiv. Their\nmethod is conceptually similar to ours, but PoT (1) only\ndemonstrates efﬁcacy on mathematical problems, whereas\nwe demonstrate gains on symbolic and algorithmic bench-\nmarks as well, and (2) chose benchmark-speciﬁc prompt\nexamples, while we used the same prompt examples as pre-\nvious work, to disentangled the beneﬁt of our approach from\nthe beneﬁt of the choice of examples.\nSemantic parsing Our work can also be seen as a very\ngeneral form of semantic parsing, where instead of parsing\ninto strict domain-speciﬁc languages, the model generates\nfree-form Python code. Some works constrain the decoder\nusing a Context-Free Grammar (CFG) to generate a domain-\nspeciﬁc meaning representation (Shin & Van Durme, 2021)\nor a canonical utterance, which can be converted to a Lisp-\nlike meaning representation (Shin et al., 2021). In contrast,\nPALdoes not require any constraining or domain-speciﬁc\nrepresentations other than Python code. Further, LMs that\nwere pretrained on Python are abundant compared to other\ndomain-speciﬁc languages, making Python code a much\nmore preferable representation. Andor et al. (2019) generate\ntask-speciﬁc arithmetic operations for reading comprehen-'
 'model ( PAL): a novel method that uses an LLM to read\nnatural language problems and generate programs as rea-\nsoning steps, but ofﬂoads the solution step to a Python inter-\npreter, as illustrated in Figure 1. This ofﬂoading leverages an\nLLM that can decompose a natural language problem into\nprogrammatic steps, which is fortunately available using\ncontemporary state-of-the-art LLMs that are pre-trained on\nboth natural language and programming languages (Brown\net al., 2020; Chen et al., 2021a; Chowdhery et al., 2022).\nWhile natural language understanding and decompositionarXiv:2211.10435v2  [cs.CL]  27 Jan 2023'
 'PAL: Program-aided Language Models\nLuyu Gao* 1Aman Madaan* 1Shuyan Zhou* 1Uri Alon1Pengfei Liu1 2Yiming Yang1Jamie Callan1\nGraham Neubig1 2\n{luyug,amadaan,shuyanzh,ualon,pliu3,yiming,callan,gneubig }@cs.cmu.edu\nAbstract\nLarge language models (LLMs) have recently\ndemonstrated an impressive ability to perform\narithmetic and symbolic reasoning tasks, when\nprovided with a few examples at test time (“few-\nshot prompting”). Much of this success can be\nattributed to prompting methods such as “chain-\nof-thought”, which employ LLMs for both under-\nstanding the problem description by decomposing\nit into steps, as well as solving each step of the\nproblem. While LLMs seem to be adept at this\nsort of step-by-step decomposition, LLMs often\nmake logical and arithmetic mistakes in the solu-\ntion part, even when the problem is decomposed\ncorrectly. In this paper, we present Program-\nAided Language models ( PAL): a novel approach\nthat uses the LLM to read natural language prob-\nlems and generate programs as the intermediate\nreasoning steps, but ofﬂoads the solution step to a\nruntime such as a Python interpreter. With PAL,\ndecomposing the natural language problem into\nrunnable steps remains the only learning task for\nthe LLM, while solving is delegated to the inter-\npreter. We demonstrate this synergy between a\nneural LLM and a symbolic interpreter across 13\nmathematical, symbolic, and algorithmic reason-\ning tasks from BIG-Bench Hard and other bench-\nmarks. In all these natural language reasoning\ntasks, generating code using an LLM and rea-\nsoning using a Python interpreter leads to more\naccurate results than much larger models. For ex-\nample, PALusing CODEX achieves state-of-the-\nart few-shot accuracy on the GSM 8Kbenchmark\nof math word problems, surpassing PaLM-540 B\nwhich uses chain-of-thought by absolute 15% top-\n1. Our code and data are publicly available at\nhttp://reasonwithpal.com .\n*The ﬁrst three authors contributed equally.1Language Tech-\nnologies Institute, Carnegie Mellon University, USA2Inspired\nCognition, USA.1. Introduction\nUntil as recently as two years ago, reasoning was considered\nto be one of the most signiﬁcant challenges that large lan-\nguage models (LLMs) had not yet overcome (Marcus, 2018;\n2020; Garcez & Lamb, 2020). Recently, LLMs have shown\nimpressive success on a wide range of tasks, including com-\nmonsense (Wei et al., 2021; Sanh et al., 2021; Madaan\net al., 2022), mathematical (Lewkowycz et al., 2022; Wu\net al., 2022; Mishra et al., 2022), and symbolic reason-\ning (Yao et al., 2022; Ahn et al., 2022), using few-shot\nprompting (Brown et al., 2020).\nThis process has been accelerated by methods that require\nLLMs to generate their explicit reasoning steps, such as\n“chain-of-thought” (Wei et al., 2022), “scratchpads” (Nye\net al., 2021), and “least-to-most” (Zhou et al., 2022) prompt-\ning. In particular, the widely used chain-of-thought ( COT)\nmethod presents the model with the explicit intermediate\nsteps that are required to reach the ﬁnal answer. Then, the\nmodel is expected to apply a similar decomposition to the ac-\ntual test example, and consecutively reach an accurate ﬁnal\nanswer (Ling et al., 2017; Amini et al., 2019). Nevertheless,\nwhile LLMs can decompose natural language problems into\nsteps and perform simple arithmetic operations, their perfor-\nmance falls dramatically when dealing with complex arith-\nmetic (Hendrycks et al., 2021; Madaan & Yazdanbakhsh,\n2022) or large numbers (Nogueira et al., 2021; Qian et al.,\n2022). In fact, even when ﬁne-tuning a PaLM-based model\non 164B tokens of explicit mathematical content, its two\nmost common failures are reportedly “incorrect reasoning”\nand “incorrect calculation” (Lewkowycz et al., 2022).\nIn this paper, we propose Program- Aided Language\nmodel ( PAL): a novel method that uses an LLM to read\nnatural language problems and generate programs as rea-\nsoning steps, but ofﬂoads the solution step to a Python inter-']"," PAL addresses complex computations by generating programmatic reasoning steps for LLMs and executing them via an interpreter, ensuring accurate and efficient problem-solving acros various domains.",0.999999999975,0.8571428571428571,0.8890980157446973,1.0,0.8527655462914057,0.9110621851656229
How does SuperGLUE's design ensure it presents a more challenging benchmark than its predecessor GLUE for evaluating language understanding models?,"SuperGLUE presents a more challenging benchmark than GLUE by incorporating more difficult tasks and expanding the range of task formats to include coreference resolution and question answering. It retains the hardest tasks from GLUE while introducing new ones identified from an open call for proposals based on their difficulty for current NLP approaches. Additionally, SuperGLUE includes comprehensive human performance baselines to verify that substantial headroom exists for improvement compared to model performance.","['SuperGLUE: A Stickier Benchmark for\nGeneral-Purpose Language Understanding Systems\nAlex Wang∗\nNew York UniversityYada Pruksachatkun∗\nNew York UniversityNikita Nangia∗\nNew York University\nAmanpreet Singh∗\nFacebook AI ResearchJulian Michael\nUniversity of WashingtonFelix Hill\nDeepMindOmer Levy\nFacebook AI Research\nSamuel R. Bowman\nNew York University\nAbstract\nIn the last year, new models and methods for pretraining and transfer learning have\ndriven striking performance improvements across a range of language understand-\ning tasks. The GLUE benchmark, introduced a little over one year ago, offers\na single-number metric that summarizes progress on a diverse set of such tasks,\nbut performance on the benchmark has recently surpassed the level of non-expert\nhumans, suggesting limited headroom for further research. In this paper we present\nSuperGLUE, a new benchmark styled after GLUE with a new set of more difﬁ-\ncult language understanding tasks, a software toolkit, and a public leaderboard.\nSuperGLUE is available at super.gluebenchmark.com .\n1 Introduction\nRecently there has been notable progress across many natural language processing (NLP) tasks, led\nby methods such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT\n(Devlin et al., 2019). The unifying theme of these methods is that they couple self-supervised learning\nfrom massive unlabelled text corpora with effective adapting of the resulting model to target tasks.\nThe tasks that have proven amenable to this general approach include question answering, textual\nentailment, and parsing, among many others (Devlin et al., 2019; Kitaev et al., 2019, i.a.).\nIn this context, the GLUE benchmark (Wang et al., 2019a) has become a prominent evaluation\nframework for research towards general-purpose language understanding technologies. GLUE is\na collection of nine language understanding tasks built on existing public datasets, together with\nprivate test data, an evaluation server, a single-number target metric, and an accompanying expert-\nconstructed diagnostic set. GLUE was designed to provide a general-purpose evaluation of language\nunderstanding that covers a range of training data volumes, task genres, and task formulations. We\nbelieve it was these aspects that made GLUE particularly appropriate for exhibiting the transfer-\nlearning potential of approaches like OpenAI GPT and BERT.\nThe progress of the last twelve months has eroded headroom on the GLUE benchmark dramatically.\nWhile some tasks (Figure 1) and some linguistic phenomena (Figure 2 in Appendix B) measured\nin GLUE remain difﬁcult, the current state of the art GLUE Score as of early July 2019 (88.4 from\nYang et al., 2019) surpasses human performance (87.1 from Nangia and Bowman, 2019) by 1.3\npoints, and in fact exceeds this human performance estimate on four tasks. Consequently, while there\n∗Equal contribution. Correspondence: glue-benchmark-admin@googlegroups.com\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.arXiv:1905.00537v3  [cs.CL]  13 Feb 2020'
 'BiLSTM+ELMo+Attn\nOpenAI GPT\nBERT + Single-task Adapters\nBERT (Large)\nBERT on STILTs\nBERT + BAM\nSemBERT\nSnorkel MeTaL\nALICE (Large)\nMT-DNN (ensemble)\nXLNet-Large (ensemble)0.50.60.70.80.91.01.11.2\nGLUE Score\nHuman Performance\nCoLA\nSST-2MRPC\nSTS-B\nQQP\nMNLIQNLI\nRTE\nWNLIFigure 1: GLUE benchmark performance for submitted systems, rescaled to set human performance\nto 1.0, shown as a single number score, and broken down into the nine constituent task performances.\nFor tasks with multiple metrics, we use an average of the metrics. More information on the tasks\nincluded in GLUE can be found in Wang et al. (2019a) and in Warstadt et al. (2019, CoLA), Socher\net al. (2013, SST-2), Dolan and Brockett (2005, MRPC), Cer et al. (2017, STS-B), and Williams et al.\n(2018, MNLI), and Rajpurkar et al. (2016, the original data source for QNLI).\nremains substantial scope for improvement towards GLUE’s high-level goals, the original version of\nthe benchmark is no longer a suitable metric for quantifying such progress.\nIn response, we introduce SuperGLUE, a new benchmark designed to pose a more rigorous test of\nlanguage understanding. SuperGLUE has the same high-level motivation as GLUE: to provide a\nsimple, hard-to-game measure of progress toward general-purpose language understanding technolo-\ngies for English. We anticipate that signiﬁcant progress on SuperGLUE should require substantive\ninnovations in a number of core areas of machine learning, including sample-efﬁcient, transfer,\nmultitask, and unsupervised or self-supervised learning.\nSuperGLUE follows the basic design of GLUE: It consists of a public leaderboard built around\neight language understanding tasks, drawing on existing data, accompanied by a single-number\nperformance metric, and an analysis toolkit. However, it improves upon GLUE in several ways:\nMore challenging tasks: SuperGLUE retains the two hardest tasks in GLUE. The remaining tasks\nwere identiﬁed from those submitted to an open call for task proposals and were selected based on\ndifﬁculty for current NLP approaches.\nMore diverse task formats: The task formats in GLUE are limited to sentence- and sentence-pair\nclassiﬁcation. We expand the set of task formats in SuperGLUE to include coreference resolution\nand question answering (QA).\nComprehensive human baselines: We include human performance estimates for all benchmark\ntasks, which verify that substantial headroom exists between a strong BERT-based baseline and\nhuman performance.\nImproved code support: SuperGLUE is distributed with a new, modular toolkit for work on\npretraining, multi-task learning, and transfer learning in NLP, built around standard tools including\nPyTorch (Paszke et al., 2017) and AllenNLP (Gardner et al., 2017).\nReﬁned usage rules: The conditions for inclusion on the SuperGLUE leaderboard have been\nrevamped to ensure fair competition, an informative leaderboard, and full credit assignment to data\nand task creators.\nThe SuperGLUE leaderboard, data, and software tools are available at super.gluebenchmark.com .\n2 Related Work\nMuch work prior to GLUE demonstrated that training neural models with large amounts of available\nsupervision can produce representations that effectively transfer to a broad range of NLP tasks\n2'
 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems .\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R. Bowman. 2019a. SuperGLUE:\nA stickier benchmark for general-purpose language\nunderstanding systems. arXiv preprint 1905.00537 .\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019b.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Inter-\nnational Conference on Learning Representations\n(ICLR) .\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2018. Neural network acceptability judg-\nments. arXiv preprint 1805.12471 .\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In North\nAmerican Association for Computational Linguis-\ntics (NAACL) .\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237 .\nYang You, Jing Li, Jonathan Hseu, Xiaodan Song,\nJames Demmel, and Cho-Jui Hsieh. 2019. Reduc-\ning bert pre-training time from 3 days to 76 minutes.\narXiv preprint arXiv:1904.00962 .\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. arXiv preprint arXiv:1905.12616 .\nYukun Zhu, Ryan Kiros, Richard Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watch-\ning movies and reading books. In arXiv preprint\narXiv:1506.06724 .\nAppendix for “RoBERTa: A Robustly\nOptimized BERT Pretraining Approach”\nA Full results on GLUE\nIn Table 8we present the full set of development\nset results for RoBERTa. We present results for\naLARGE conﬁguration that follows BERT LARGE ,\nas well as a BASE conﬁguration that follows\nBERT BASE.B Pretraining Hyperparameters\nTable 9describes the hyperparameters for pre-\ntraining of RoBERTa LARGE and RoBERTa BASE\nC Finetuning Hyperparameters\nFinetuning hyperparameters for RACE, SQuAD\nand GLUE are given in Table 10. We select the\nbest hyperparameter values based on the median\nof 5 random seeds for each task.'
 'Table 2: Development set examples from the tasks in SuperGLUE. Bold text represents part of the\nexample format for each task. Text in italics is part of the model input. Underlined text is specially\nmarked in the input. Text in a monospaced font represents the expected model output.BoolQPassage: Barq’s – Barq’s is an American soft drink. Its brand of root beer is notable for having caffeine.\nBarq’s, created by Edward Barq and bottled since the turn of the 20th century, is owned by the Barq\nfamily but bottled by the Coca-Cola Company. It was known as Barq’s Famous Olde Tyme Root Beer\nuntil 2012.\nQuestion: is barq’s root beer a pepsi product Answer: NoCBText: B: And yet, uh, I we-, I hope to see employer based, you know, helping out. You know, child, uh,\ncare centers at the place of employment and things like that, that will help out. A: Uh-huh. B: What do\nyou think, do you think we are, setting a trend?\nHypothesis: they are setting a trend Entailment: UnknownCOPAPremise: My body cast a shadow over the grass. Question: What’s the CAUSE for this?\nAlternative 1: The sun was rising. Alternative 2: The grass was cut.\nCorrect Alternative: 1MultiRCParagraph: Susan wanted to have a birthday party. She called all of her friends. She has ﬁve friends.\nHer mom said that Susan can invite them all to the party. Her ﬁrst friend could not go to the party\nbecause she was sick. Her second friend was going out of town. Her third friend was not so sure if her\nparents would let her. The fourth friend said maybe. The ﬁfth friend could go to the party for sure. Susan\nwas a little sad. On the day of the party, all ﬁve friends showed up. Each friend had a present for Susan.\nSusan was happy and sent each friend a thank you card the next week\nQuestion: Did Susan’s sick friend recover? Candidate answers: Yes, she recovered (T),No(F),Yes\n(T),No, she didn’t recover (F),Yes, she was at Susan’s party (T)ReCoRDParagraph: (CNN )Puerto Rico on Sunday overwhelmingly voted for statehood. But Congress, the only\nbody that can approve new states, will ultimately decide whether the status of the UScommonwealth\nchanges. Ninety-seven percent of the votes in the nonbinding referendum favored statehood, an increase\nover the results of a 2012 referendum, ofﬁcial results from the State Electorcal Commission show. It\nwas the ﬁfth such vote on statehood. ""Today, we the people of Puerto Rico are sending a strong and\nclear message to the US Congress ... and to the world ... claiming our equal rights as American citizens,\nPuerto Rico Gov. Ricardo Rossello said in a news release. @highlight Puerto Rico voted Sunday in\nfavor of US statehood\nQuery For one, they can truthfully say, “Don’t blame me, I didn’t vote for them, ” when discussing the\n<placeholder> presidency Correct Entities: USRTEText: Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44,\naccording to the Christopher Reeve Foundation.\nHypothesis: Christopher Reeve had an accident. Entailment: FalseWiCContext 1: Room and board .Context 2: He nailed boards across the windows.\nSense match: FalseWSCText: Mark told Pete many lies about himself, which Pete included in his book. Heshould have been\nmore truthful. Coreference: False\n3 SuperGLUE Overview\n3.1 Design Process\nThe goal of SuperGLUE is to provide a simple, robust evaluation metric of any method capable of\nbeing applied to a broad range of language understanding tasks. To that end, in designing SuperGLUE,\nwe identify the following desiderata of tasks in the benchmark:\nTask substance: Tasks should test a system’s ability to understand and reason about texts in English.\nTask difﬁculty: Tasks should be beyond the scope of current state-of-the-art systems, but solvable by\nmost college-educated English speakers. We exclude tasks that require domain-speciﬁc knowledge,\ne.g. medical notes or scientiﬁc papers.\nEvaluability: Tasks must have an automatic performance metric that corresponds well to human']","SuperGLUE introduces more challenging tasks retained from GLUE, diversifies task formats beyond sentence- and sentence-pair classification to include coreference resolution and question answering, and features comprehensive human baseline performances to ensure significant headroom for model improvement.",0.9166666666361111,1.0,0.8741637729882883,1.0,0.8861830819587909,0.9733037564065926
" In what ways does SuperGLUE expand beyond GLUE's task formats, and why is this expansion significant?","SuperGLUE expands beyond GLUE's task formats by including a more diverse set of tasks, such as coreference resolution and question answering, in addition to the sentence- and sentence-pair classification tasks present in GLUE. This expansion is significant because it allows for a more comprehensive assessment of language understanding capabilities, pushing the boundaries of what existing models can achieve and challenging them with more complex reasoning tasks. Ultimately, this broader scope aims to foster further advancements in natural language processing research.","['SuperGLUE: A Stickier Benchmark for\nGeneral-Purpose Language Understanding Systems\nAlex Wang∗\nNew York UniversityYada Pruksachatkun∗\nNew York UniversityNikita Nangia∗\nNew York University\nAmanpreet Singh∗\nFacebook AI ResearchJulian Michael\nUniversity of WashingtonFelix Hill\nDeepMindOmer Levy\nFacebook AI Research\nSamuel R. Bowman\nNew York University\nAbstract\nIn the last year, new models and methods for pretraining and transfer learning have\ndriven striking performance improvements across a range of language understand-\ning tasks. The GLUE benchmark, introduced a little over one year ago, offers\na single-number metric that summarizes progress on a diverse set of such tasks,\nbut performance on the benchmark has recently surpassed the level of non-expert\nhumans, suggesting limited headroom for further research. In this paper we present\nSuperGLUE, a new benchmark styled after GLUE with a new set of more difﬁ-\ncult language understanding tasks, a software toolkit, and a public leaderboard.\nSuperGLUE is available at super.gluebenchmark.com .\n1 Introduction\nRecently there has been notable progress across many natural language processing (NLP) tasks, led\nby methods such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT\n(Devlin et al., 2019). The unifying theme of these methods is that they couple self-supervised learning\nfrom massive unlabelled text corpora with effective adapting of the resulting model to target tasks.\nThe tasks that have proven amenable to this general approach include question answering, textual\nentailment, and parsing, among many others (Devlin et al., 2019; Kitaev et al., 2019, i.a.).\nIn this context, the GLUE benchmark (Wang et al., 2019a) has become a prominent evaluation\nframework for research towards general-purpose language understanding technologies. GLUE is\na collection of nine language understanding tasks built on existing public datasets, together with\nprivate test data, an evaluation server, a single-number target metric, and an accompanying expert-\nconstructed diagnostic set. GLUE was designed to provide a general-purpose evaluation of language\nunderstanding that covers a range of training data volumes, task genres, and task formulations. We\nbelieve it was these aspects that made GLUE particularly appropriate for exhibiting the transfer-\nlearning potential of approaches like OpenAI GPT and BERT.\nThe progress of the last twelve months has eroded headroom on the GLUE benchmark dramatically.\nWhile some tasks (Figure 1) and some linguistic phenomena (Figure 2 in Appendix B) measured\nin GLUE remain difﬁcult, the current state of the art GLUE Score as of early July 2019 (88.4 from\nYang et al., 2019) surpasses human performance (87.1 from Nangia and Bowman, 2019) by 1.3\npoints, and in fact exceeds this human performance estimate on four tasks. Consequently, while there\n∗Equal contribution. Correspondence: glue-benchmark-admin@googlegroups.com\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.arXiv:1905.00537v3  [cs.CL]  13 Feb 2020'
 'Table 2: Development set examples from the tasks in SuperGLUE. Bold text represents part of the\nexample format for each task. Text in italics is part of the model input. Underlined text is specially\nmarked in the input. Text in a monospaced font represents the expected model output.BoolQPassage: Barq’s – Barq’s is an American soft drink. Its brand of root beer is notable for having caffeine.\nBarq’s, created by Edward Barq and bottled since the turn of the 20th century, is owned by the Barq\nfamily but bottled by the Coca-Cola Company. It was known as Barq’s Famous Olde Tyme Root Beer\nuntil 2012.\nQuestion: is barq’s root beer a pepsi product Answer: NoCBText: B: And yet, uh, I we-, I hope to see employer based, you know, helping out. You know, child, uh,\ncare centers at the place of employment and things like that, that will help out. A: Uh-huh. B: What do\nyou think, do you think we are, setting a trend?\nHypothesis: they are setting a trend Entailment: UnknownCOPAPremise: My body cast a shadow over the grass. Question: What’s the CAUSE for this?\nAlternative 1: The sun was rising. Alternative 2: The grass was cut.\nCorrect Alternative: 1MultiRCParagraph: Susan wanted to have a birthday party. She called all of her friends. She has ﬁve friends.\nHer mom said that Susan can invite them all to the party. Her ﬁrst friend could not go to the party\nbecause she was sick. Her second friend was going out of town. Her third friend was not so sure if her\nparents would let her. The fourth friend said maybe. The ﬁfth friend could go to the party for sure. Susan\nwas a little sad. On the day of the party, all ﬁve friends showed up. Each friend had a present for Susan.\nSusan was happy and sent each friend a thank you card the next week\nQuestion: Did Susan’s sick friend recover? Candidate answers: Yes, she recovered (T),No(F),Yes\n(T),No, she didn’t recover (F),Yes, she was at Susan’s party (T)ReCoRDParagraph: (CNN )Puerto Rico on Sunday overwhelmingly voted for statehood. But Congress, the only\nbody that can approve new states, will ultimately decide whether the status of the UScommonwealth\nchanges. Ninety-seven percent of the votes in the nonbinding referendum favored statehood, an increase\nover the results of a 2012 referendum, ofﬁcial results from the State Electorcal Commission show. It\nwas the ﬁfth such vote on statehood. ""Today, we the people of Puerto Rico are sending a strong and\nclear message to the US Congress ... and to the world ... claiming our equal rights as American citizens,\nPuerto Rico Gov. Ricardo Rossello said in a news release. @highlight Puerto Rico voted Sunday in\nfavor of US statehood\nQuery For one, they can truthfully say, “Don’t blame me, I didn’t vote for them, ” when discussing the\n<placeholder> presidency Correct Entities: USRTEText: Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44,\naccording to the Christopher Reeve Foundation.\nHypothesis: Christopher Reeve had an accident. Entailment: FalseWiCContext 1: Room and board .Context 2: He nailed boards across the windows.\nSense match: FalseWSCText: Mark told Pete many lies about himself, which Pete included in his book. Heshould have been\nmore truthful. Coreference: False\n3 SuperGLUE Overview\n3.1 Design Process\nThe goal of SuperGLUE is to provide a simple, robust evaluation metric of any method capable of\nbeing applied to a broad range of language understanding tasks. To that end, in designing SuperGLUE,\nwe identify the following desiderata of tasks in the benchmark:\nTask substance: Tasks should test a system’s ability to understand and reason about texts in English.\nTask difﬁculty: Tasks should be beyond the scope of current state-of-the-art systems, but solvable by\nmost college-educated English speakers. We exclude tasks that require domain-speciﬁc knowledge,\ne.g. medical notes or scientiﬁc papers.\nEvaluability: Tasks must have an automatic performance metric that corresponds well to human'
 'Table 1: The tasks included in SuperGLUE. WSD stands for word sense disambiguation, NLI is\nnatural language inference, coref. is coreference resolution, and QAis question answering. For\nMultiRC, we list the number of total answers for 456/83/166 train/dev/test questions.\nCorpus |Train | | Dev | | Test |Task Metrics Text Sources\nBoolQ 9427 3270 3245 QA acc. Google queries, Wikipedia\nCB 250 57 250 NLI acc./F1 various\nCOPA 400 100 500 QA acc. blogs, photography encyclopedia\nMultiRC 5100 953 1800 QA F1 a/EM various\nReCoRD 101k 10k 10k QA F1/EM news (CNN, Daily Mail)\nRTE 2500 278 300 NLI acc. news, Wikipedia\nWiC 6000 638 1400 WSD acc. WordNet, VerbNet, Wiktionary\nWSC 554 104 146 coref. acc. ﬁction books\n(Collobert and Weston, 2008; Dai and Le, 2015; Kiros et al., 2015; Hill et al., 2016; Conneau and\nKiela, 2018; McCann et al., 2017; Peters et al., 2018). GLUE was presented as a formal challenge\naffording straightforward comparison between such task-agnostic transfer learning techniques. Other\nsimilarly-motivated benchmarks include SentEval (Conneau and Kiela, 2018), which speciﬁcally\nevaluates ﬁxed-size sentence embeddings, and DecaNLP (McCann et al., 2018), which recasts a set\nof target tasks into a general question-answering format and prohibits task-speciﬁc parameters. In\ncontrast, GLUE provides a lightweight classiﬁcation API and no restrictions on model architecture or\nparameter sharing, which seems to have been well-suited to recent work in this area.\nSince its release, GLUE has been used as a testbed and showcase by the developers of several\ninﬂuential models, including GPT (Radford et al., 2018) and BERT (Devlin et al., 2019). As shown\nin Figure 1, progress on GLUE since its release has been striking. On GLUE, GPT and BERT\nachieved scores of 72.8 and 80.2 respectively, relative to 66.5 for an ELMo-based model (Peters\net al., 2018) and 63.7 for the strongest baseline with no multitask learning or pretraining above the\nword level. Recent models (Liu et al., 2019d; Yang et al., 2019) have clearly surpassed estimates of\nnon-expert human performance on GLUE (Nangia and Bowman, 2019). The success of these models\non GLUE has been driven by ever-increasing model capacity, compute power, and data quantity, as\nwell as innovations in model expressivity (from recurrent to bidirectional recurrent to multi-headed\ntransformer encoders) and degree of contextualization (from learning representation of words in\nisolation to using uni-directional contexts and ultimately to leveraging bidirectional contexts).\nIn parallel to work scaling up pretrained models, several studies have focused on complementary\nmethods for augmenting performance of pretrained models. Phang et al. (2018) show that BERT can\nbe improved using two-stage pretraining, i.e., ﬁne-tuning the pretrained model on an intermediate\ndata-rich supervised task before ﬁne-tuning it again on a data-poor target task. Liu et al. (2019d,c) and\nBach et al. (2018) get further improvements respectively via multi-task ﬁnetuning and using massive\namounts of weak supervision. Clark et al. (2019b) demonstrate that knowledge distillation (Hinton\net al., 2015; Furlanello et al., 2018) can lead to student networks that outperform their teachers.\nOverall, the quantity and quality of research contributions aimed at the challenges posed by GLUE\nunderline the utility of this style of benchmark for machine learning researchers looking to evaluate\nnew application-agnostic methods on language understanding.\nLimits to current approaches are also apparent via the GLUE suite. Performance on the GLUE\ndiagnostic entailment dataset, at 0.42 R3, falls far below the average human performance of 0.80\nR3reported in the original GLUE publication, with models performing near, or even below, chance\non some linguistic phenomena (Figure 2, Appendix B). While some initially difﬁcult categories\nsaw gains from advances on GLUE (e.g., double negation), others remain hard (restrictivity) or'
 'BiLSTM+ELMo+Attn\nOpenAI GPT\nBERT + Single-task Adapters\nBERT (Large)\nBERT on STILTs\nBERT + BAM\nSemBERT\nSnorkel MeTaL\nALICE (Large)\nMT-DNN (ensemble)\nXLNet-Large (ensemble)0.50.60.70.80.91.01.11.2\nGLUE Score\nHuman Performance\nCoLA\nSST-2MRPC\nSTS-B\nQQP\nMNLIQNLI\nRTE\nWNLIFigure 1: GLUE benchmark performance for submitted systems, rescaled to set human performance\nto 1.0, shown as a single number score, and broken down into the nine constituent task performances.\nFor tasks with multiple metrics, we use an average of the metrics. More information on the tasks\nincluded in GLUE can be found in Wang et al. (2019a) and in Warstadt et al. (2019, CoLA), Socher\net al. (2013, SST-2), Dolan and Brockett (2005, MRPC), Cer et al. (2017, STS-B), and Williams et al.\n(2018, MNLI), and Rajpurkar et al. (2016, the original data source for QNLI).\nremains substantial scope for improvement towards GLUE’s high-level goals, the original version of\nthe benchmark is no longer a suitable metric for quantifying such progress.\nIn response, we introduce SuperGLUE, a new benchmark designed to pose a more rigorous test of\nlanguage understanding. SuperGLUE has the same high-level motivation as GLUE: to provide a\nsimple, hard-to-game measure of progress toward general-purpose language understanding technolo-\ngies for English. We anticipate that signiﬁcant progress on SuperGLUE should require substantive\ninnovations in a number of core areas of machine learning, including sample-efﬁcient, transfer,\nmultitask, and unsupervised or self-supervised learning.\nSuperGLUE follows the basic design of GLUE: It consists of a public leaderboard built around\neight language understanding tasks, drawing on existing data, accompanied by a single-number\nperformance metric, and an analysis toolkit. However, it improves upon GLUE in several ways:\nMore challenging tasks: SuperGLUE retains the two hardest tasks in GLUE. The remaining tasks\nwere identiﬁed from those submitted to an open call for task proposals and were selected based on\ndifﬁculty for current NLP approaches.\nMore diverse task formats: The task formats in GLUE are limited to sentence- and sentence-pair\nclassiﬁcation. We expand the set of task formats in SuperGLUE to include coreference resolution\nand question answering (QA).\nComprehensive human baselines: We include human performance estimates for all benchmark\ntasks, which verify that substantial headroom exists between a strong BERT-based baseline and\nhuman performance.\nImproved code support: SuperGLUE is distributed with a new, modular toolkit for work on\npretraining, multi-task learning, and transfer learning in NLP, built around standard tools including\nPyTorch (Paszke et al., 2017) and AllenNLP (Gardner et al., 2017).\nReﬁned usage rules: The conditions for inclusion on the SuperGLUE leaderboard have been\nrevamped to ensure fair competition, an informative leaderboard, and full credit assignment to data\nand task creators.\nThe SuperGLUE leaderboard, data, and software tools are available at super.gluebenchmark.com .\n2 Related Work\nMuch work prior to GLUE demonstrated that training neural models with large amounts of available\nsupervision can produce representations that effectively transfer to a broad range of NLP tasks\n2']","By including coreference resolution and question answering formats, SuperGLUE expands the scope of evaluated linguistic phenomena, testing models' abilities in more varied contexts and reflecting a broader range of natural language understanding challenges.",0.999999999975,1.0,0.9497275651779272,1.0,0.9870624195264681,0.9482496781058722
